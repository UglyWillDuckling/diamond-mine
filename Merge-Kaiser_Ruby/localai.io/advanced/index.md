-   [![](https://github.com/go-skynet/LocalAI/assets/2420543/0966aa2a-166e-4f99-a3e5-6c915fc997dd){style="width:calc(65%);height:calc(65%)"}](../index.html){aria-label="HomePage"
    alt="HomePage"}

    LocalAI

-   [![](https://img.shields.io/github/release/go-skynet/LocalAI?&label=Latest&style=for-the-badge)](https://github.com/go-skynet/LocalAI/releases)

-   [![](https://img.shields.io/badge/dockerhub-images-important.svg?logo=Docker)](https://hub.docker.com/r/localai/localai){target="_blank"}
    [![](https://img.shields.io/badge/quay.io-images-important.svg?)](https://quay.io/repository/go-skynet/local-ai?tab=tags&tag=latest){target="_blank"}

-   [*info* Overview](../index.html){.sidebar-root-link}
-   *rocket_launch* Getting started

    -   [Quickstart](../basics/getting_started/index.html){.sidebar-nested-link}
    -   [Install and Run
        Models](https://localai.io/docs/getting-started/models/){.sidebar-nested-link}
    -   [Try it out](../basics/try/index.html){.sidebar-nested-link}
    -   [Customizing the
        Model](https://localai.io/docs/getting-started/customize-model/){.sidebar-nested-link}
    -   [Build LocalAI from
        source](../basics/build/index.html){.sidebar-nested-link}
    -   [Run with container
        images](../basics/container/index.html){.sidebar-nested-link}
    -   [Run with
        Kubernetes](../basics/kubernetes/index.html){.sidebar-nested-link}
-   [*newspaper* News](../basics/news/index.html){.sidebar-root-link}
-   *feature_search* Features

    -   [‚ö° GPU
        acceleration](../features/gpu-acceleration/index.html){.sidebar-nested-link}
    -   [üìñ Text generation
        (GPT)](../features/text-generation/index.html){.sidebar-nested-link}
    -   [üìà
        Reranker](../features/reranker/index.html){.sidebar-nested-link}
    -   [üó£ Text to audio
        (TTS)](../features/text-to-audio/index.html){.sidebar-nested-link}
    -   [üé® Image
        generation](../features/image-generation/index.html){.sidebar-nested-link}
    -   [üß†
        Embeddings](../features/embeddings/index.html){.sidebar-nested-link}
    -   [ü•Ω GPT
        Vision](../features/gpt-vision/index.html){.sidebar-nested-link}
    -   [‚úçÔ∏è Constrained
        Grammars](../features/constrained_grammars/index.html){.sidebar-nested-link}
    -   [üÜïüñß Distributed
        Inference](../features/distribute/index.html){.sidebar-nested-link}
    -   [üîà Audio to
        text](../features/audio-to-text/index.html){.sidebar-nested-link}
    -   [üî• OpenAI functions and
        tools](../features/openai-functions/index.html){.sidebar-nested-link}
    -   [üíæ Stores](../stores/index.html){.sidebar-nested-link}
    -   [üñºÔ∏è Model gallery](../models/index.html){.sidebar-nested-link}
-   [*sync*
    Integrations](https://localai.io/docs/integrations/){.sidebar-root-link}
-   *settings* Advanced

    -   [Advanced usage](index.html){.sidebar-nested-link}
    -   [Fine-tuning LLMs for text
        generation](https://localai.io/docs/advanced/fine-tuning/){.sidebar-nested-link}
    -   [Installer
        options](https://localai.io/docs/advanced/installer/){.sidebar-nested-link}
-   *menu_book* References

    -   [Model compatibility
        table](../model-compatibility/index.html){.sidebar-nested-link}
    -   [Architecture](https://localai.io/docs/reference/architecture/){.sidebar-nested-link}
    -   [LocalAI
        binaries](https://localai.io/docs/reference/binaries/){.sidebar-nested-link}
    -   [Running on Nvidia
        ARM64](https://localai.io/docs/reference/nvidia-l4t/){.sidebar-nested-link}
-   [*quiz* FAQ](../faq/index.html){.sidebar-root-link}

[](../index.html){.logo-icon .me-3 aria-label="HomePage" alt="HomePage"}

![](data:image/svg+xml;base64,PHN2ZyBpZD0iTGF5ZXJfMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Ym94PSIwIDAgMjUwIDI1MCI+PHBhdGggZD0ibTE0MyAzOS41Yy0xOCAwLTE4IDE4LTE4IDE4czAtMTgtMTgtMThIMjJjLTIuNzYuMC01IDIuMjQtNSA1djE0M2MwIDIuNzYgMi4yNCA1IDUgNWg3NmM3LjIuMCA4LjY0IDExLjUyIDguOTMgMTYuMTMuMDcgMS4wNS45NSAxLjg3IDIgMS44N2gzMi4xNGMxLjA2LjAgMS45NC0uODIgMi0xLjg3LjI5LTQuNjEgMS43My0xNi4xMyA4LjkzLTE2LjEzaDc2YzIuNzYuMCA1LTIuMjQgNS01VjQ0LjVjMC0yLjc2LTIuMjQtNS01LTVoLTg1ek0yMDYgMTYzYzAgMS4zOC0xLjEyIDIuNS0yLjUgMi41SDE0M2MtMTggMC0xOCAxOC0xOCAxOHMwLTE4LTE4LTE4SDQ2LjVjLTEuMzguMC0yLjUtMS4xMi0yLjUtMi41VjY5YzAtMS4zOCAxLjEyLTIuNSAyLjUtMi41SDk4YzcuMi4wIDguNjQgMTEuNTIgOC45MyAxNi4xMy4wNyAxLjA1Ljk1IDEuODcgMiAxLjg3aDMyLjE0YzEuMDYuMCAxLjk0LS44MiAyLTEuODcuMjktNC42MSAxLjczLTE2LjEzIDguOTMtMTYuMTNoNTEuNWMxLjM4LjAgMi41IDEuMTIgMi41IDIuNXY5NHoiIHN0eWxlPSJmaWxsOiMwNmYiIC8+PC9zdmc+){#Layer_1}

![](data:image/svg+xml;base64,PHN2ZyBpZD0iTGF5ZXJfMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Ym94PSIwIDAgMjUwIDI1MCI+PHBhdGggZD0ibTE0MyAzOS41Yy0xOCAwLTE4IDE4LTE4IDE4czAtMTgtMTgtMThIMjJjLTIuNzYuMC01IDIuMjQtNSA1djE0M2MwIDIuNzYgMi4yNCA1IDUgNWg3NmM3LjIuMCA4LjY0IDExLjUyIDguOTMgMTYuMTMuMDcgMS4wNS45NSAxLjg3IDIgMS44N2gzMi4xNGMxLjA2LjAgMS45NC0uODIgMi0xLjg3LjI5LTQuNjEgMS43My0xNi4xMyA4LjkzLTE2LjEzaDc2YzIuNzYuMCA1LTIuMjQgNS01VjQ0LjVjMC0yLjc2LTIuMjQtNS01LTVoLTg1ek0yMDYgMTYzYzAgMS4zOC0xLjEyIDIuNS0yLjUgMi41SDE0M2MtMTggMC0xOCAxOC0xOCAxOHMwLTE4LTE4LTE4SDQ2LjVjLTEuMzguMC0yLjUtMS4xMi0yLjUtMi41VjY5YzAtMS4zOCAxLjEyLTIuNSAyLjUtMi41SDk4YzcuMi4wIDguNjQgMTEuNTIgOC45MyAxNi4xMy4wNyAxLjA1Ljk1IDEuODcgMiAxLjg3aDMyLjE0YzEuMDYuMCAxLjk0LS44MiAyLTEuODcuMjktNC42MSAxLjczLTE2LjEzIDguOTMtMTYuMTNoNTEuNWMxLjM4LjAgMi41IDEuMTIgMi41IDIuNXY5NHoiIHN0eWxlPSJmaWxsOiMwNmYiIC8+PC9zdmc+){#Layer_1}

[menu]{.material-icons .size-20 .menu-icon .align-middle}

[search]{.material-icons .size-20 .menu-icon .align-middle}
[Search]{.flexsearch-button-placeholder .ms-1 .me-2 .d-none .d-sm-block}

[[![](data:image/svg+xml;base64,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)]{.kbd
.flexsearch-button-cmd-key}[![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTUiIGhlaWdodD0iMTUiPjxwYXRoIGQ9Ik01LjkyNiAxMi4yNzlINC40MUw5LjA3MyAyLjcyMUgxMC41OXoiIGZpbGw9ImN1cnJlbnRjb2xvciIgLz48L3N2Zz4=)]{.kbd
.flexsearch-button-key}]{.flexsearch-button-keys}

##### Star us on GitHub !¬†

[Star](https://github.com/mudler/LocalAI){.github-button
color-scheme="no-preference: light; light: light; dark: dark;"
icon="octicon-star" data-size="large" show-count="true"
aria-label="Star mudler/LocalAI on GitHub"}

-   [](https://github.com/mudler/LocalAI){alt="github"
    rel="noopener noreferrer" target="_blank"}

    ![](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld2JveD0iMCAwIDI0IDI0IiBmaWxsPSJub25lIiBzdHJva2U9ImN1cnJlbnRjb2xvciIgc3Ryb2tlLXdpZHRoPSIyIiBzdHJva2UtbGluZWNhcD0icm91bmQiIHN0cm9rZS1saW5lam9pbj0icm91bmQiPjx0aXRsZT5HaXRIdWI8L3RpdGxlPjxwYXRoIGQ9Ik05IDE5Yy01IDEuNS01LTIuNS03LTNtMTQgNnYtMy44N2EzLjM3IDMuMzcuMCAwMC0uOTQtMi42MWMzLjE0LS4zNSA2LjQ0LTEuNTQgNi40NC03QTUuNDQgNS40NC4wIDAwMjAgNC43NyA1LjA3IDUuMDcuMCAwMDE5LjkxIDFTMTguNzMuNjUgMTYgMi40OGExMy4zOCAxMy4zOC4wIDAwLTcgMEM2LjI3LjY1IDUuMDkgMSA1LjA5IDFBNS4wNyA1LjA3LjAgMDA1IDQuNzcgNS40NCA1LjQ0LjAgMDAzLjUgOC41NWMwIDUuNDIgMy4zIDYuNjEgNi40NCA3QTMuMzcgMy4zNy4wIDAwOSAxOC4xM1YyMiIgLz48L3N2Zz4=)
-   [](https://twitter.com/LocalAI_API){alt="twitter"
    rel="noopener noreferrer" target="_blank"}

    ![](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld2JveD0iMCAwIDI0IDI0Ij48dGl0bGU+VHdpdHRlciAvIFg8L3RpdGxlPjxwYXRoIGQ9Ik0uMDg4Ljc2OGw5LjI2NiAxMi4zOUwuMDI5IDIzLjIzMWgyLjFsOC4xNjMtOC44MTkgNi42IDguODE5aDcuMTQyTDE0LjI0MiAxMC4xNDUgMjIuOTIxLjc2OGgtMi4xTDEzLjMgOC44OTEgNy4yMjkuNzY4ek0zLjE3NCAyLjMxNEg2LjQ1NUwyMC45NDIgMjEuNjg1aC0zLjI4eiIgZmlsbD0iY3VycmVudGNvbG9yIiAvPjwvc3ZnPg==)
-   [](../index.xml){alt="rss" rel="noopener noreferrer"
    target="_blank"}

    ![](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld2JveD0iMCAwIDI0IDI0IiBmaWxsPSJub25lIiBzdHJva2U9ImN1cnJlbnRjb2xvciIgc3Ryb2tlLXdpZHRoPSIyIiBzdHJva2UtbGluZWNhcD0icm91bmQiIHN0cm9rZS1saW5lam9pbj0icm91bmQiPjx0aXRsZT5SU1M8L3RpdGxlPjxwYXRoIGQ9Ik00IDExYTkgOSAwIDAxOSA5IiAvPjxwYXRoIGQ9Ik00IDRhMTYgMTYgMCAwMTE2IDE2IiAvPjxjaXJjbGUgY3g9IjUiIGN5PSIxOSIgcj0iMSI+PC9jaXJjbGU+PC9zdmc+)

[![](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMzAiIHdpZHRoPSIzMCIgdmlld2JveD0iMCAwIDQ4IDQ4IiBmaWxsPSJjdXJyZW50Y29sb3IiPjx0aXRsZT5FbmFibGUgZGFyayBtb2RlPC90aXRsZT48cGF0aCBkPSJNMjQgNDJxLTcuNS4wLTEyLjc1LTUuMjVUNiAyNHQ1LjI1LTEyLjc1VDI0IDZxLjQuMC44NS4wMjUuNDUuMDI1IDEuMTUuMDc1LTEuOCAxLjYtMi44IDMuOTV0LTEgNC45NXEwIDQuNSAzLjE1IDcuNjVRMjguNSAyNS44IDMzIDI1LjhxMi42LjAgNC45NS0uOTI1VDQxLjkgMjIuM3EuMDUuNi4wNzUuOTc1UTQyIDIzLjY1IDQyIDI0cTAgNy41LTUuMjUgMTIuNzVUMjQgNDJ6bTAtM3E1LjQ1LjAgOS41LTMuMzc1dDUuMDUtNy45MjVxLTEuMjUuNTUtMi42NzUuODI1UTM0LjQ1IDI4LjggMzMgMjguOHEtNS43NS4wLTkuNzc1LTQuMDI1VDE5LjIgMTVxMC0xLjIuMjUtMi41NzV0LjktMy4xMjVxLTQuOSAxLjM1LTguMTI1IDUuNDc1UTkgMTguOSA5IDI0cTAgNi4yNSA0LjM3NSAxMC42MjVUMjQgMzl6bS0uMi0xNC44NXoiIC8+PC9zdmc+)]{.toggle-dark}[![](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMzAiIHdpZHRoPSIzMCIgdmlld2JveD0iMCAwIDQ4IDQ4IiBmaWxsPSJjdXJyZW50Y29sb3IiPjx0aXRsZT5FbmFibGUgbGlnaHQgbW9kZTwvdGl0bGU+PHBhdGggZD0iTTI0IDMxcTIuOS4wIDQuOTUtMi4wNVQzMSAyNHQtMi4wNS00Ljk1VDI0IDE3dC00Ljk1IDIuMDVUMTcgMjR0Mi4wNSA0Ljk1VDI0IDMxem0wIDNxLTQuMTUuMC03LjA3NS0yLjkyNVQxNCAyNHQyLjkyNS03LjA3NVQyNCAxNHQ3LjA3NSAyLjkyNVQzNCAyNHQtMi45MjUgNy4wNzVUMjQgMzR6TTMuNSAyNS41cS0uNjUuMC0xLjA3NS0uNDI1UTIgMjQuNjUgMiAyNHQuNDI1LTEuMDc1UTIuODUgMjIuNSAzLjUgMjIuNWg1cS42NS4wIDEuMDc1LjQyNVExMCAyMy4zNSAxMCAyNHQtLjQyNSAxLjA3NVQ4LjUgMjUuNXptMzYgMHEtLjY1LjAtMS4wNzUtLjQyNVEzOCAyNC42NSAzOCAyNHQuNDI1LTEuMDc1VDM5LjUgMjIuNWg1cS42NS4wIDEuMDc1LjQyNVE0NiAyMy4zNSA0NiAyNHQtLjQyNSAxLjA3NS0xLjA3NS40MjV6TTI0IDEwcS0uNjUuMC0xLjA3NS0uNDI1UTIyLjUgOS4xNSAyMi41IDguNXYtNXEwLS42NS40MjUtMS4wNzVRMjMuMzUgMiAyNCAydDEuMDc1LjQyNVQyNS41IDMuNXY1cTAgLjY1LS40MjUgMS4wNzVRMjQuNjUgMTAgMjQgMTB6bTAgMzZxLS42NS4wLTEuMDc1LS40MjVUMjIuNSA0NC41di01cTAtLjY1LjQyNS0xLjA3NVEyMy4zNSAzOCAyNCAzOHQxLjA3NS40MjUuNDI1IDEuMDc1djVxMCAuNjUtLjQyNSAxLjA3NVEyNC42NSA0NiAyNCA0NnpNMTIgMTQuMWwtMi44NS0yLjhxLS40NS0uNDUtLjQyNS0xLjA3NS4wMjUtLjYyNS40MjUtMS4wNzUuNDUtLjQ1IDEuMDc1LS40NXQxLjA3NS40NUwxNC4xIDEycS40LjQ1LjQgMS4wNS4wLjYtLjQgMS0uNC40NS0xLjAyNS40NVQxMiAxNC4xem0yNC43IDI0Ljc1TDMzLjkgMzZxLS40LS40NS0uNC0xLjA3NXQuNDUtMS4wMjVxLjQtLjQ1IDEtLjQ1dDEuMDUuNDVsMi44NSAyLjhxLjQ1LjQ1LjQyNSAxLjA3NS0uMDI1LjYyNS0uNDI1IDEuMDc1LS40NS40NS0xLjA3NS40NXQtMS4wNzUtLjQ1ek0zMy45IDE0LjFxLS40NS0uNDUtLjQ1LTEuMDUuMC0uNi40NS0xLjA1bDIuOC0yLjg1cS40NS0uNDUgMS4wNzUtLjQyNS42MjUuMDI1IDEuMDc1LjQyNS40NS40NS40NSAxLjA3NXQtLjQ1IDEuMDc1TDM2IDE0LjFxLS40LjQtMS4wMjUuNHQtMS4wNzUtLjR6TTkuMTUgMzguODVxLS40NS0uNDUtLjQ1LTEuMDc1dC40NS0xLjA3NUwxMiAzMy45cS40NS0uNDUgMS4wNS0uNDUuNi4wIDEuMDUuNDUuNDUuNDUuNDUgMS4wNS4wLjYtLjQ1IDEuMDVsLTIuOCAyLjg1cS0uNDUuNDUtMS4wNzUuNDI1LS42MjUtLjAyNS0xLjA3NS0uNDI1ek0yNCAyNHoiIC8+PC9zdmc+)]{.toggle-light}

[![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTUiIGhlaWdodD0iMTUiIGFyaWEtbGFiZWw9IkFycm93IGRvd24iIHJvbGU9ImltZyI+PGcgZmlsbD0ibm9uZSIgc3Ryb2tlPSJjdXJyZW50Y29sb3IiIHN0cm9rZS1saW5lY2FwPSJyb3VuZCIgc3Ryb2tlLWxpbmVqb2luPSJyb3VuZCIgc3Ryb2tlLXdpZHRoPSIxLjIiPjxwYXRoIGQ9Ik03LjUgMy41djhtMy0zLTMgMy0zLTMiIC8+PC9nPjwvc3ZnPg==)]{.kbd
.flexsearch-button-cmd-key}[![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTUiIGhlaWdodD0iMTUiIGFyaWEtbGFiZWw9IkFycm93IHVwIiByb2xlPSJpbWciPjxnIGZpbGw9Im5vbmUiIHN0cm9rZT0iY3VycmVudGNvbG9yIiBzdHJva2UtbGluZWNhcD0icm91bmQiIHN0cm9rZS1saW5lam9pbj0icm91bmQiIHN0cm9rZS13aWR0aD0iMS4yIj48cGF0aCBkPSJNNy41IDExLjV2LThtMyAzLTMtMy0zIDMiIC8+PC9nPjwvc3ZnPg==)]{.kbd
.flexsearch-button-cmd-key}[to navigate]{.flexsearch-key-label}

[![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTUiIGhlaWdodD0iMTUiIGFyaWEtbGFiZWw9IkVudGVyIGtleSIgcm9sZT0iaW1nIj48ZyBmaWxsPSJub25lIiBzdHJva2U9ImN1cnJlbnRjb2xvciIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIiBzdHJva2UtbGluZWpvaW49InJvdW5kIiBzdHJva2Utd2lkdGg9IjEuMiI+PHBhdGggZD0iTTEyIDMuNTMwODh2M2MwIDEtMSAyLTIgMkg0bTMgMy0zLTMgMy0zIiAvPjwvZz48L3N2Zz4=)]{.kbd
.flexsearch-button-cmd-key}[to select]{.flexsearch-key-label}

[![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTUiIGhlaWdodD0iMTUiIGFyaWEtbGFiZWw9IkVzY2FwZSBrZXkiIHJvbGU9ImltZyI+PGcgZmlsbD0ibm9uZSIgc3Ryb2tlPSJjdXJyZW50Y29sb3IiIHN0cm9rZS1saW5lY2FwPSJyb3VuZCIgc3Ryb2tlLWxpbmVqb2luPSJyb3VuZCIgc3Ryb2tlLXdpZHRoPSIxLjIiPjxwYXRoIGQ9Ik0xMy42MTY3IDguOTM2Yy0uMTA2NS4zNTgzLS42ODgzLjk2Mi0xLjQ4NzUuOTYyLS43OTkzLjAtMS42NTMtLjkxNjUtMS42NTMtMi4xMjU4di0uNTY3OGMwLTEuMjU0OC43ODk2LTIuMTAxNiAxLjY1My0yLjEwMTZzMS4zNjAxLjQ3NzggMS40ODc1IDEuMDcyNE05IDZjLS4xMzUyLS40NzM1LS43NTA2LS45MjE5LTEuNDYtLjg5NzItLjcwOTIuMDI0Ni0xLjM0NC41Ny0xLjM0NCAxLjIxNjZzLjQxOTguODgxMiAxLjM0NDUuOTgwNUM4LjQ2NSA3LjM5OTIgOC45NjggNy45MzM3IDkgOC41cy0uNDU0IDEuMzk4LTEuNDU5NSAxLjM5OEM2LjY1OTMgOS44OTggNiA5IDUuOTYzIDguNDg1MW0tMS40NzQ4LjUzNjhjLS4yNjM1LjU5NDEtLjgwOTkuODc2LTEuNTQ0My44NzZzLTEuNzA3My0uNjI0OC0xLjcwNzMtMi4yMDR2LS40NjAzYzAtMS4wNDE2LjcyMS0yLjEzMSAxLjcwNzMtMi4xMzEuOTg2NC4wIDEuNjQyNSAxLjAzMSAxLjU0NDMgMi4yNDkyaC0yLjk1NiIgLz48L2c+PC9zdmc+)]{.kbd
.flexsearch-button-cmd-key}[to close]{.flexsearch-key-label}

cancel


-   [*Home*](../docs){itemprop="item"}
-   [[Advanced]{itemprop="name"}](https://localai.io/docs/advanced/){itemprop="item"}
-   [Advanced usage]{itemprop="name"}

On this page

-   -   -   [Advanced configuration with YAML
            files](index.html#advanced-configuration-with-yaml-files)
        -   [Full config model file
            reference](index.html#full-config-model-file-reference)
        -   [Prompt templates](index.html#prompt-templates)
        -   [Install models using the
            API](index.html#install-models-using-the-api)
        -   [Preloading models during
            startup](index.html#preloading-models-during-startup)
        -   [Automatic prompt
            caching](index.html#automatic-prompt-caching)
        -   [Configuring a specific backend for the
            model](index.html#configuring-a-specific-backend-for-the-model)
        -   [Connect external
            backends](index.html#connect-external-backends)
        -   [Environment variables](index.html#environment-variables)
        -   [CLI parameters](index.html#cli-parameters)
        -   [.env files](index.html#env-files)
        -   [Request headers](index.html#request-headers)
        -   [Extra backends](index.html#extra-backends)
        -   [Concurrent requests](index.html#concurrent-requests)
        -   [Disable CPU flagset auto detection in
            llama.cpp](index.html#disable-cpu-flagset-auto-detection-in-llamacpp)

Table of Contents

-   -   -   [Advanced configuration with YAML
            files](index.html#advanced-configuration-with-yaml-files)
        -   [Full config model file
            reference](index.html#full-config-model-file-reference)
        -   [Prompt templates](index.html#prompt-templates)
        -   [Install models using the
            API](index.html#install-models-using-the-api)
        -   [Preloading models during
            startup](index.html#preloading-models-during-startup)
        -   [Automatic prompt
            caching](index.html#automatic-prompt-caching)
        -   [Configuring a specific backend for the
            model](index.html#configuring-a-specific-backend-for-the-model)
        -   [Connect external
            backends](index.html#connect-external-backends)
        -   [Environment variables](index.html#environment-variables)
        -   [CLI parameters](index.html#cli-parameters)
        -   [.env files](index.html#env-files)
        -   [Request headers](index.html#request-headers)
        -   [Extra backends](index.html#extra-backends)
        -   [Concurrent requests](index.html#concurrent-requests)
        -   [Disable CPU flagset auto detection in
            llama.cpp](index.html#disable-cpu-flagset-auto-detection-in-llamacpp)

*article*

# Advanced usage {#advanced-usage .content-title .mb-0}

### Advanced configuration with YAML files [*link*](index.html#advanced-configuration-with-yaml-files){.anchor aria-hidden="true"} {#advanced-configuration-with-yaml-files}

In order to define default prompts, model parameters (such as custom
default `top_p` or `top_k`), LocalAI can be configured to serve
user-defined models with a set of default parameters and templates.

In order to configure a model, you can create multiple `yaml` files in
the models path or either specify a single YAML configuration file.
Consider the following `models` folder in the `example/chatbot-ui`:

``` {#54df8c7 .language-}
  base ‚ùØ ls -liah examples/chatbot-ui/models 
36487587 drwxr-xr-x 2 mudler mudler 4.0K May  3 12:27 .
36487586 drwxr-xr-x 3 mudler mudler 4.0K May  3 10:42 ..
36465214 -rw-r--r-- 1 mudler mudler   10 Apr 27 07:46 completion.tmpl
36464855 -rw-r--r-- 1 mudler mudler   ?G Apr 27 00:08 luna-ai-llama2-uncensored.ggmlv3.q5_K_M.bin
36464537 -rw-r--r-- 1 mudler mudler  245 May  3 10:42 gpt-3.5-turbo.yaml
36467388 -rw-r--r-- 1 mudler mudler  180 Apr 27 07:46 chat.tmpl
  
```

In the `gpt-3.5-turbo.yaml` file it is defined the `gpt-3.5-turbo` model
which is an alias to use `luna-ai-llama2` with pre-defined options.

For instance, consider the following that declares `gpt-3.5-turbo`
backed by the `luna-ai-llama2` model:

``` {#fd36aba .language-yaml}
  name: gpt-3.5-turbo
# Default model parameters
parameters:
  # Relative to the models path
  model: luna-ai-llama2-uncensored.ggmlv3.q5_K_M.bin
  # temperature
  temperature: 0.3
  # all the OpenAI request options here..

# Default context size
context_size: 512
threads: 10
# Define a backend (optional). By default it will try to guess the backend the first time the model is interacted with.
backend: llama-stable # available: llama, stablelm, gpt2, gptj rwkv

# Enable prompt caching
prompt_cache_path: "alpaca-cache"
prompt_cache_all: true

# stopwords (if supported by the backend)
stopwords:
- "HUMAN:"
- "### Response:"
# define chat roles
roles:
  assistant: '### Response:'
  system: '### System Instruction:'
  user: '### Instruction:'
template:
  # template file ".tmpl" with the prompt template to use by default on the endpoint call. Note there is no extension in the files
  completion: completion
  chat: chat
  
```

Specifying a `config-file` via CLI allows to declare models in a single
file as a list, for instance:

``` {#3bfd229 .language-yaml}
  - name: list1
  parameters:
    model: testmodel
  context_size: 512
  threads: 10
  stopwords:
  - "HUMAN:"
  - "### Response:"
  roles:
    user: "HUMAN:"
    system: "GPT:"
  template:
    completion: completion
    chat: chat
- name: list2
  parameters:
    model: testmodel
  context_size: 512
  threads: 10
  stopwords:
  - "HUMAN:"
  - "### Response:"
  roles:
    user: "HUMAN:"
    system: "GPT:"
  template:
    completion: completion
   chat: chat
  
```

See also
[chatbot-ui![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIGhlaWdodD0iMTYiIHZpZXdib3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBmaWxsPSJjdXJyZW50Y29sb3IiIGQ9Ik0xNCA1Yy0uNTUyLjAtMS0uNDQ4LTEtMXMuNDQ4LTEgMS0xaDZjLjU1Mi4wIDEgLjQ0OCAxIDF2NmMwIC41NTItLjQ0OCAxLTEgMXMtMS0uNDQ4LTEtMVY2LjQxNGwtNy4yOTMgNy4yOTNjLS4zOTEuMzktMS4wMjQuMzktMS40MTQuMC0uMzkxLS4zOTEtLjM5MS0xLjAyNC4wLTEuNDE0TDE3LjU4NiA1SDE0ek01IDdjLS41NTIuMC0xIC40NDgtMSAxdjExYzAgLjU1Mi40NDggMSAxIDFoMTFjLjU1Mi4wIDEtLjQ0OCAxLTF2LTQuNTYzYzAtLjU1Mi40NDgtMSAxLTFzMSAuNDQ4IDEgMVYxOWMwIDEuNjU3LTEuMzQzIDMtMyAzSDVjLTEuNjU3LjAtMy0xLjM0My0zLTNWOGMwLTEuNjU3IDEuMzQzLTMgMy0zaDQuNTYzYy41NTIuMCAxIC40NDggMSAxcy0uNDQ4IDEtMSAxSDV6IiAvPjwvc3ZnPg==)](https://github.com/go-skynet/LocalAI/tree/master/examples/chatbot-ui){rel="external"
target="_blank"} as an example on how to use config files.

It is possible to specify a full URL or a short-hand URL to a YAML model
configuration file and use it on start with local-ai, for example to use
phi-2:

``` {#c903e0a .language-}
  local-ai github://mudler/LocalAI/examples/configurations/phi-2.yaml@master
  
```

### Full config model file reference [*link*](index.html#full-config-model-file-reference){.anchor aria-hidden="true"} {#full-config-model-file-reference}

``` {#864b8bb .language-yaml}
  # Main configuration of the model, template, and system features.
name: "" # Model name, used to identify the model in API calls.

# Precision settings for the model, reducing precision can enhance performance on some hardware.
f16: null # Whether to use 16-bit floating-point precision.

embeddings: true # Enable embeddings for the model.

# Concurrency settings for the application.
threads: null # Number of threads to use for processing.

# Roles define how different entities interact in a conversational model.
# It can be used to map roles to specific parts of the conversation.
roles: {} # Roles for entities like user, system, assistant, etc.

# Backend to use for computation (like llama-cpp, diffusers, whisper).
backend: "" # Backend for AI computations.

# Templates for various types of model interactions.
template:
    chat: "" # Template for chat interactions. Uses golang templates with Sprig functions.
    chat_message: "" # Template for individual chat messages.  Uses golang templates with Sprig functions.
    completion: "" # Template for generating text completions. Uses golang templates with Sprig functions.
    edit: "" # Template for edit operations. Uses golang templates with Sprig functions.
    function: "" # Template for function calls. Uses golang templates with Sprig functions.
    use_tokenizer_template: false # Whether to use a specific tokenizer template. (vLLM)
    join_chat_messages_by_character: null # Character to join chat messages, if applicable. Defaults to newline.

# Function-related settings to control behavior of specific function calls.
function:
    disable_no_action: false # Whether to disable the no-action behavior.
    grammar:
        parallel_calls: false # Allow to return parallel tools
        disable_parallel_new_lines: false # Disable parallel processing for new lines in grammar checks.
        mixed_mode: false # Allow mixed-mode grammar enforcing
        no_mixed_free_string: false # Disallow free strings in mixed mode.
        disable: false # Completely disable grammar enforcing functionality.
        prefix: "" # Prefix to add before grammars rules.
        expect_strings_after_json: false # Expect string after JSON data.
    no_action_function_name: "" # Function name to call when no action is determined.
    no_action_description_name: "" # Description name for no-action functions.
    response_regex: [] # Regular expressions to match response from
    argument_regex: [] # Named regular to extract function arguments from the response.
    argument_regex_key_name: "key" # Name of the named regex capture to capture the key of the function arguments
      argument_regex_value_name: "value" # Name of the named regex capture to capture the value of the function arguments
    json_regex_match: [] # Regular expressions to match JSON data when in tool mode
    replace_function_results: [] # Placeholder to replace function call results with arbitrary strings or patterns.
    replace_llm_results: [] # Replace language model results with arbitrary strings or patterns.
    capture_llm_results: [] # Capture language model results as text result, among JSON, in function calls. For instance, if a model returns a block for "thinking" and a block for "response", this will allow you to capture the thinking block.
    function_name_key: "name"
    function_arguments_key: "arguments"

# Feature gating flags to enable experimental or optional features.
feature_flags: {}

# System prompt to use by default.
system_prompt: ""

# Configuration for splitting tensors across GPUs.
tensor_split: ""

# Identifier for the main GPU used in multi-GPU setups.
main_gpu: ""

# Small value added to the denominator in RMS normalization to prevent division by zero.
rms_norm_eps: 0

# Natural question generation model parameter.
ngqa: 0

# Path where prompt cache is stored.
prompt_cache_path: ""

# Whether to cache all prompts.
prompt_cache_all: false

# Whether the prompt cache is read-only.
prompt_cache_ro: false

# Mirostat sampling settings.
mirostat_eta: null
mirostat_tau: null
mirostat: null

# GPU-specific layers configuration.
gpu_layers: null

# Memory mapping for efficient I/O operations.
mmap: null

# Memory locking to ensure data remains in RAM.
mmlock: null

# Mode to use minimal VRAM for GPU operations.
low_vram: null

# Words or phrases that halts processing.
stopwords: []

# Strings to cut from responses to maintain context or relevance.
cutstrings: []

# Strings to trim from responses for cleaner outputs.
trimspace: []
trimsuffix: []

# Default context size for the model's understanding of the conversation or text.
context_size: null

# Non-uniform memory access settings, useful for systems with multiple CPUs.
numa: false

# Configuration for LoRA
lora_adapter: ""
lora_base: ""
lora_scale: 0

# Disable matrix multiplication queuing in GPU operations.
no_mulmatq: false

# Model for generating draft responses.
draft_model: ""
n_draft: 0

# Quantization settings for the model, impacting memory and processing speed.
quantization: ""

# Utilization percentage of GPU memory to allocate for the model. (vLLM)
gpu_memory_utilization: 0

# Whether to trust and execute remote code.
trust_remote_code: false

# Force eager execution of TensorFlow operations if applicable. (vLLM)
enforce_eager: false

# Space allocated for swapping data in and out of memory. (vLLM)
swap_space: 0

# Maximum model length, possibly referring to the number of tokens or parameters. (vLLM)
max_model_len: 0

# Size of the tensor parallelism in distributed computing environments. (vLLM)
tensor_parallel_size: 0

# vision model to use for multimodal
mmproj: ""

# Disables offloading of key/value pairs in transformer models to save memory.
no_kv_offloading: false

# Scaling factor for the rope penalty.
rope_scaling: ""

# Type of configuration, often related to the type of task or model architecture.
type: ""

# YARN settings
yarn_ext_factor: 0
yarn_attn_factor: 0
yarn_beta_fast: 0
yarn_beta_slow: 0

# AutoGPT-Q settings, for configurations specific to GPT models.
autogptq:
    model_base_name: "" # Base name of the model.
    device: "" # Device to run the model on.
    triton: false # Whether to use Triton Inference Server.
    use_fast_tokenizer: false # Whether to use a fast tokenizer for quicker processing.

# configuration for diffusers model
diffusers:
    cuda: false # Whether to use CUDA
    pipeline_type: "" # Type of pipeline to use.
    scheduler_type: "" # Type of scheduler for controlling operations.
    enable_parameters: "" # Parameters to enable in the diffuser.
    cfg_scale: 0 # Scale for CFG in the diffuser setup.
    img2img: false # Whether image-to-image transformation is supported.
    clip_skip: 0 # Number of steps to skip in CLIP operations.
    clip_model: "" # Model to use for CLIP operations.
    clip_subfolder: "" # Subfolder for storing CLIP-related data.
    control_net: "" # Control net to use

# Step count, usually for image processing models
step: 0

# Configuration for gRPC communication.
grpc:
    attempts: 0 # Number of retry attempts for gRPC calls.
    attempts_sleep_time: 0 # Sleep time between retries.

# Text-to-Speech (TTS) configuration.
tts:
    voice: "" # Voice setting for TTS.
    vall-e:
        audio_path: "" # Path to audio files for Vall-E.

# Whether to use CUDA for GPU-based operations.
cuda: false

# List of files to download as part of the setup or operations.
download_files: []
  
```

### Prompt templates [*link*](index.html#prompt-templates){.anchor aria-hidden="true"} {#prompt-templates}

The API doesn't inject a default prompt for talking to the model. You
have to use a prompt similar to what's described in the standford-alpaca
docs:
[https://github.com/tatsu-lab/stanford_alpaca#data-release![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIGhlaWdodD0iMTYiIHZpZXdib3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBmaWxsPSJjdXJyZW50Y29sb3IiIGQ9Ik0xNCA1Yy0uNTUyLjAtMS0uNDQ4LTEtMXMuNDQ4LTEgMS0xaDZjLjU1Mi4wIDEgLjQ0OCAxIDF2NmMwIC41NTItLjQ0OCAxLTEgMXMtMS0uNDQ4LTEtMVY2LjQxNGwtNy4yOTMgNy4yOTNjLS4zOTEuMzktMS4wMjQuMzktMS40MTQuMC0uMzkxLS4zOTEtLjM5MS0xLjAyNC4wLTEuNDE0TDE3LjU4NiA1SDE0ek01IDdjLS41NTIuMC0xIC40NDgtMSAxdjExYzAgLjU1Mi40NDggMSAxIDFoMTFjLjU1Mi4wIDEtLjQ0OCAxLTF2LTQuNTYzYzAtLjU1Mi40NDgtMSAxLTFzMSAuNDQ4IDEgMVYxOWMwIDEuNjU3LTEuMzQzIDMtMyAzSDVjLTEuNjU3LjAtMy0xLjM0My0zLTNWOGMwLTEuNjU3IDEuMzQzLTMgMy0zaDQuNTYzYy41NTIuMCAxIC40NDggMSAxcy0uNDQ4IDEtMSAxSDV6IiAvPjwvc3ZnPg==)](https://github.com/tatsu-lab/stanford_alpaca#data-release){rel="external"
target="_blank"}.

You can use a default template for every model present in your model
path, by creating a corresponding file with the \`.tmpl\` suffix next to
your model. For instance, if the model is called \`foo.bin\`, you can
create a sibling file, \`foo.bin.tmpl\` which will be used as a default
prompt and can be used with alpaca:

``` {#b5bfa1d .language-}
  The below instruction describes a task. Write a response that appropriately completes the request.

### Instruction:
{{.Input}}

### Response:
  
```

See the
[prompt-templates![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIGhlaWdodD0iMTYiIHZpZXdib3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBmaWxsPSJjdXJyZW50Y29sb3IiIGQ9Ik0xNCA1Yy0uNTUyLjAtMS0uNDQ4LTEtMXMuNDQ4LTEgMS0xaDZjLjU1Mi4wIDEgLjQ0OCAxIDF2NmMwIC41NTItLjQ0OCAxLTEgMXMtMS0uNDQ4LTEtMVY2LjQxNGwtNy4yOTMgNy4yOTNjLS4zOTEuMzktMS4wMjQuMzktMS40MTQuMC0uMzkxLS4zOTEtLjM5MS0xLjAyNC4wLTEuNDE0TDE3LjU4NiA1SDE0ek01IDdjLS41NTIuMC0xIC40NDgtMSAxdjExYzAgLjU1Mi40NDggMSAxIDFoMTFjLjU1Mi4wIDEtLjQ0OCAxLTF2LTQuNTYzYzAtLjU1Mi40NDgtMSAxLTFzMSAuNDQ4IDEgMVYxOWMwIDEuNjU3LTEuMzQzIDMtMyAzSDVjLTEuNjU3LjAtMy0xLjM0My0zLTNWOGMwLTEuNjU3IDEuMzQzLTMgMy0zaDQuNTYzYy41NTIuMCAxIC40NDggMSAxcy0uNDQ4IDEtMSAxSDV6IiAvPjwvc3ZnPg==)](https://github.com/go-skynet/LocalAI/tree/master/prompt-templates){rel="external"
target="_blank"} directory in this repository for templates for some of
the most popular models.

For the edit endpoint, an example template for alpaca-based models can
be:

``` {#306eb7a .language-yaml}
  Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{{.Instruction}}

### Input:
{{.Input}}

### Response:
  
```

### Install models using the API [*link*](index.html#install-models-using-the-api){.anchor aria-hidden="true"} {#install-models-using-the-api}

Instead of installing models manually, you can use the LocalAI API
endpoints and a model definition to install programmatically via API
models in runtime.

A curated collection of model files is in the
[model-gallery![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIGhlaWdodD0iMTYiIHZpZXdib3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBmaWxsPSJjdXJyZW50Y29sb3IiIGQ9Ik0xNCA1Yy0uNTUyLjAtMS0uNDQ4LTEtMXMuNDQ4LTEgMS0xaDZjLjU1Mi4wIDEgLjQ0OCAxIDF2NmMwIC41NTItLjQ0OCAxLTEgMXMtMS0uNDQ4LTEtMVY2LjQxNGwtNy4yOTMgNy4yOTNjLS4zOTEuMzktMS4wMjQuMzktMS40MTQuMC0uMzkxLS4zOTEtLjM5MS0xLjAyNC4wLTEuNDE0TDE3LjU4NiA1SDE0ek01IDdjLS41NTIuMC0xIC40NDgtMSAxdjExYzAgLjU1Mi40NDggMSAxIDFoMTFjLjU1Mi4wIDEtLjQ0OCAxLTF2LTQuNTYzYzAtLjU1Mi40NDgtMSAxLTFzMSAuNDQ4IDEgMVYxOWMwIDEuNjU3LTEuMzQzIDMtMyAzSDVjLTEuNjU3LjAtMy0xLjM0My0zLTNWOGMwLTEuNjU3IDEuMzQzLTMgMy0zaDQuNTYzYy41NTIuMCAxIC40NDggMSAxcy0uNDQ4IDEtMSAxSDV6IiAvPjwvc3ZnPg==)](https://github.com/go-skynet/model-gallery){rel="external"
target="_blank"} (work in progress!). The files of the model gallery are
different from the model files used to configure LocalAI models. The
model gallery files contains information about the model setup, and the
files necessary to run the model locally.

To install for example `lunademo`, you can send a POST call to the
`/models/apply` endpoint with the model definition url (`url`) and the
name of the model should have in LocalAI (`name`, optional):

``` {#782b2fc .language-bash}
  curl --location 'http://localhost:8080/models/apply' \
--header 'Content-Type: application/json' \
--data-raw '{
    "id": "TheBloke/Luna-AI-Llama2-Uncensored-GGML/luna-ai-llama2-uncensored.ggmlv3.q5_K_M.bin",
    "name": "lunademo"
}'
  
```

### Preloading models during startup [*link*](index.html#preloading-models-during-startup){.anchor aria-hidden="true"} {#preloading-models-during-startup}

In order to allow the API to start-up with all the needed model on the
first-start, the model gallery files can be used during startup.

``` {#5c15a85 .language-bash}
  PRELOAD_MODELS='[{"url": "https://raw.githubusercontent.com/go-skynet/model-gallery/main/gpt4all-j.yaml","name": "gpt4all-j"}]' local-ai
  
```

`PRELOAD_MODELS` (or `--preload-models`) takes a list in JSON with the
same parameter of the API calls of the `/models/apply` endpoint.

Similarly it can be specified a path to a YAML configuration file
containing a list of models with `PRELOAD_MODELS_CONFIG` ( or
`--preload-models-config` ):

``` {#68febe0 .language-yaml}
  - url: https://raw.githubusercontent.com/go-skynet/model-gallery/main/gpt4all-j.yaml
  name: gpt4all-j
# ...
  
```

### Automatic prompt caching [*link*](index.html#automatic-prompt-caching){.anchor aria-hidden="true"} {#automatic-prompt-caching}

LocalAI can automatically cache prompts for faster loading of the
prompt. This can be useful if your model need a prompt template with
prefixed text in the prompt before the input.

To enable prompt caching, you can control the settings in the model
config YAML file:

``` {#f0f6449 .language-yaml}
  
# Enable prompt caching
prompt_cache_path: "cache"
prompt_cache_all: true
  
```

`prompt_cache_path` is relative to the models folder. you can enter here
a name for the file that will be automatically create during the first
load if `prompt_cache_all` is set to `true`.

### Configuring a specific backend for the model [*link*](index.html#configuring-a-specific-backend-for-the-model){.anchor aria-hidden="true"} {#configuring-a-specific-backend-for-the-model}

By default LocalAI will try to autoload the model by trying all the
backends. This might work for most of models, but some of the backends
are NOT configured to autoload.

The available backends are listed in the [model compatibility
table](../model-compatibility/index.html).

In order to specify a backend for your models, create a model config
file in your `models` directory specifying the backend:

``` {#7c409b9 .language-yaml}
  name: gpt-3.5-turbo

# Default model parameters
parameters:
  # Relative to the models path
  model: ...

backend: llama-stable
# ...
  
```

### Connect external backends [*link*](index.html#connect-external-backends){.anchor aria-hidden="true"} {#connect-external-backends}

LocalAI backends are internally implemented using `gRPC` services. This
also allows `LocalAI` to connect to external `gRPC` services on start
and extend LocalAI functionalities via third-party binaries.

The `--external-grpc-backends` parameter in the CLI can be used either
to specify a local backend (a file) or a remote URL. The syntax is
`<BACKEND_NAME>:<BACKEND_URI>`. Once LocalAI is started with it, the new
backend name will be available for all the API endpoints.

So for instance, to register a new backend which is a local file:

``` {#ae4c8e1 .language-}
  ./local-ai --debug --external-grpc-backends "my-awesome-backend:/path/to/my/backend.py"
  
```

Or a remote URI:

``` {#99617fb .language-}
  ./local-ai --debug --external-grpc-backends "my-awesome-backend:host:port"
  
```

For example, to start vllm manually after compiling LocalAI (also
assuming running the command from the root of the repository):

``` {#ee5d5e8 .language-bash}
  ./local-ai --external-grpc-backends "vllm:$PWD/backend/python/vllm/run.sh"
  
```

Note that first is is necessary to create the environment with:

``` {#0e121e8 .language-bash}
  make -C backend/python/vllm
  
```

### Environment variables [*link*](index.html#environment-variables){.anchor aria-hidden="true"} {#environment-variables}

When LocalAI runs in a container, there are additional environment
variables available that modify the behavior of LocalAI on startup:

  Environment variable         Default   Description
  ---------------------------- --------- -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  `REBUILD`                    `false`   Rebuild LocalAI on startup
  `BUILD_TYPE`                           Build type. Available: `cublas`, `openblas`, `clblas`
  `GO_TAGS`                              Go tags. Available: `stablediffusion`
  `HUGGINGFACEHUB_API_TOKEN`             Special token for interacting with HuggingFace Inference API, required only when using the `langchain-huggingface` backend
  `EXTRA_BACKENDS`                       A space separated list of backends to prepare. For example `EXTRA_BACKENDS="backend/python/diffusers backend/python/transformers"` prepares the python environment on start
  `DISABLE_AUTODETECT`         `false`   Disable autodetect of CPU flagset on start
  `LLAMACPP_GRPC_SERVERS`                A list of llama.cpp workers to distribute the workload. For example `LLAMACPP_GRPC_SERVERS="address1:port,address2:port"`

Here is how to configure these variables:

``` {#9240fb3 .language-bash}
  # Option 1: command line
docker run --env REBUILD=true localai
# Option 2: set within an env file
docker run --env-file .env localai
  
```

### CLI parameters [*link*](index.html#cli-parameters){.anchor aria-hidden="true"} {#cli-parameters}

You can control LocalAI with command line arguments, to specify a
binding address, or the number of threads. Any command line parameter
can be specified via an environment variable.

In the help text below, BASEPATH is the location that local-ai is being
executed from

#### Global Flags [*link*](index.html#global-flags){.anchor aria-hidden="true"} {#global-flags}

  Parameter     Default   Description                                                 Environment Variable
  ------------- --------- ----------------------------------------------------------- ----------------------
  -h, --help              Show context-sensitive help.                                
  --log-level   info      Set the level of logs to output \[error,warn,info,debug\]   \$LOCALAI_LOG_LEVEL

#### Storage Flags [*link*](index.html#storage-flags){.anchor aria-hidden="true"} {#storage-flags}

  Parameter                            Default                     Description                                                                                                                                                                         Environment Variable
  ------------------------------------ --------------------------- ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ------------------------------------
  --models-path                        BASEPATH/models             Path containing models used for inferencing                                                                                                                                         \$LOCALAI_MODELS_PATH
  --backend-assets-path                /tmp/localai/backend_data   Path used to extract libraries that are required by some of the backends in runtime                                                                                                 \$LOCALAI_BACKEND_ASSETS_PATH
  --image-path                         /tmp/generated/images       Location for images generated by backends (e.g. stablediffusion)                                                                                                                    \$LOCALAI_IMAGE_PATH
  --audio-path                         /tmp/generated/audio        Location for audio generated by backends (e.g. piper)                                                                                                                               \$LOCALAI_AUDIO_PATH
  --upload-path                        /tmp/localai/upload         Path to store uploads from files api                                                                                                                                                \$LOCALAI_UPLOAD_PATH
  --config-path                        /tmp/localai/config                                                                                                                                                                                             \$LOCALAI_CONFIG_PATH
  --localai-config-dir                 BASEPATH/configuration      Directory for dynamic loading of certain configuration files (currently api_keys.json and external_backends.json)                                                                   \$LOCALAI_CONFIG_DIR
  --localai-config-dir-poll-interval                               Typically the config path picks up changes automatically, but if your system has broken fsnotify events, set this to a time duration to poll the LocalAI Config Dir (example: 1m)   \$LOCALAI_CONFIG_DIR_POLL_INTERVAL
  --models-config-file                 STRING                      YAML file containing a list of model backend configs                                                                                                                                \$LOCALAI_MODELS_CONFIG_FILE

#### Models Flags [*link*](index.html#models-flags){.anchor aria-hidden="true"} {#models-flags}

  Parameter                 Default                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Description                                                        Environment Variable
  ------------------------- ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ------------------------------------------------------------------ ---------------------------------
  --galleries               STRING                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            JSON list of galleries                                             \$LOCALAI_GALLERIES
  --autoload-galleries                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \$LOCALAI_AUTOLOAD_GALLERIES
  --remote-library          "[https://raw.githubusercontent.com/mudler/LocalAI/master/embedded/model_library.yaml\"![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIGhlaWdodD0iMTYiIHZpZXdib3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBmaWxsPSJjdXJyZW50Y29sb3IiIGQ9Ik0xNCA1Yy0uNTUyLjAtMS0uNDQ4LTEtMXMuNDQ4LTEgMS0xaDZjLjU1Mi4wIDEgLjQ0OCAxIDF2NmMwIC41NTItLjQ0OCAxLTEgMXMtMS0uNDQ4LTEtMVY2LjQxNGwtNy4yOTMgNy4yOTNjLS4zOTEuMzktMS4wMjQuMzktMS40MTQuMC0uMzkxLS4zOTEtLjM5MS0xLjAyNC4wLTEuNDE0TDE3LjU4NiA1SDE0ek01IDdjLS41NTIuMC0xIC40NDgtMSAxdjExYzAgLjU1Mi40NDggMSAxIDFoMTFjLjU1Mi4wIDEtLjQ0OCAxLTF2LTQuNTYzYzAtLjU1Mi40NDgtMSAxLTFzMSAuNDQ4IDEgMVYxOWMwIDEuNjU3LTEuMzQzIDMtMyAzSDVjLTEuNjU3LjAtMy0xLjM0My0zLTNWOGMwLTEuNjU3IDEuMzQzLTMgMy0zaDQuNTYzYy41NTIuMCAxIC40NDggMSAxcy0uNDQ4IDEtMSAxSDV6IiAvPjwvc3ZnPg==)](https://raw.githubusercontent.com/mudler/LocalAI/master/embedded/model_library.yaml%22){rel="external" target="_blank"}   A LocalAI remote library URL                                       \$LOCALAI_REMOTE_LIBRARY
  --preload-models          STRING                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            A List of models to apply in JSON at start                         \$LOCALAI_PRELOAD_MODELS
  --models                  MODELS,...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        A List of model configuration URLs to load                         \$LOCALAI_MODELS
  --preload-models-config   STRING                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            A List of models to apply at startup. Path to a YAML config file   \$LOCALAI_PRELOAD_MODELS_CONFIG

#### Performance Flags [*link*](index.html#performance-flags){.anchor aria-hidden="true"} {#performance-flags}

  Parameter        Default   Description                                                                                                         Environment Variable
  ---------------- --------- ------------------------------------------------------------------------------------------------------------------- ------------------------
  --f16                      Enable GPU acceleration                                                                                             \$LOCALAI_F16
  -t, --threads    4         Number of threads used for parallel computation. Usage of the number of physical cores in the system is suggested   \$LOCALAI_THREADS
  --context-size   512       Default context size for models                                                                                     \$LOCALAI_CONTEXT_SIZE

#### API Flags [*link*](index.html#api-flags){.anchor aria-hidden="true"} {#api-flags}

  Parameter              Default        Description                                                                                                                                                  Environment Variable
  ---------------------- -------------- ------------------------------------------------------------------------------------------------------------------------------------------------------------ ------------------------------
  --address              ":8080"        Bind address for the API server                                                                                                                              \$LOCALAI_ADDRESS
  --cors                                                                                                                                                                                             \$LOCALAI_CORS
  --cors-allow-origins                                                                                                                                                                               \$LOCALAI_CORS_ALLOW_ORIGINS
  --upload-limit         15             Default upload-limit in MB                                                                                                                                   \$LOCALAI_UPLOAD_LIMIT
  --api-keys             API-KEYS,...   List of API Keys to enable API authentication. When this is set, all the requests must be authenticated with one of these API keys                           \$LOCALAI_API_KEY
  --disable-welcome                     Disable welcome pages                                                                                                                                        \$LOCALAI_DISABLE_WELCOME
  --machine-tag                         If not empty - put that string to Machine-Tag header in each response. Useful to track response from different machines using multiple P2P federated nodes   \$LOCALAI_MACHINE_TAG

#### Backend Flags [*link*](index.html#backend-flags){.anchor aria-hidden="true"} {#backend-flags}

  Parameter                  Default                      Description                                                                                                       Environment Variable
  -------------------------- ---------------------------- ----------------------------------------------------------------------------------------------------------------- ----------------------------------------------------------
  --parallel-requests                                     Enable backends to handle multiple requests in parallel if they support it (e.g.: llama.cpp or vllm)              \$LOCALAI_PARALLEL_REQUESTS
  --single-active-backend                                 Allow only one backend to be run at a time                                                                        \$LOCALAI_SINGLE_ACTIVE_BACKEND
  --preload-backend-only                                  Do not launch the API services, only the preloaded models / backends are started (useful for multi-node setups)   \$LOCALAI_PRELOAD_BACKEND_ONLY
  --external-grpc-backends   EXTERNAL-GRPC-BACKENDS,...   A list of external grpc backends                                                                                  \$LOCALAI_EXTERNAL_GRPC_BACKENDS
  --enable-watchdog-idle                                  Enable watchdog for stopping backends that are idle longer than the watchdog-idle-timeout                         \$LOCALAI_WATCHDOG_IDLE
  --watchdog-idle-timeout    15m                          Threshold beyond which an idle backend should be stopped                                                          \$LOCALAI_WATCHDOG_IDLE_TIMEOUT, \$WATCHDOG_IDLE_TIMEOUT
  --enable-watchdog-busy                                  Enable watchdog for stopping backends that are busy longer than the watchdog-busy-timeout                         \$LOCALAI_WATCHDOG_BUSY
  --watchdog-busy-timeout    5m                           Threshold beyond which a busy backend should be stopped                                                           \$LOCALAI_WATCHDOG_BUSY_TIMEOUT

### .env files [*link*](index.html#env-files){.anchor aria-hidden="true"} {#env-files}

Any settings being provided by an Environment Variable can also be
provided from within .env files. There are several locations that will
be checked for relevant .env files. In order of precedence they are:

-   .env within the current directory
-   localai.env within the current directory
-   localai.env within the home directory
-   .config/localai.env within the home directory
-   /etc/localai.env

Environment variables within files earlier in the list will take
precedence over environment variables defined in files later in the
list.

An example .env file is:

``` {#1fc22d8 .language-}
  LOCALAI_THREADS=10
LOCALAI_MODELS_PATH=/mnt/storage/localai/models
LOCALAI_F16=true
  
```

### Request headers [*link*](index.html#request-headers){.anchor aria-hidden="true"} {#request-headers}

You can use 'Extra-Usage' request header key presence ('Extra-Usage:
true') to receive inference timings in milliseconds extending default
OpenAI response model in the usage field:

``` {#6115728 .language-}
  ...
{
  "id": "...",
  "created": ...,
  "model": "...",
  "choices": [
    {
      ...
    },
    ...
  ],
  "object": "...",
  "usage": {
    "prompt_tokens": ...,
    "completion_tokens": ...,
    "total_tokens": ...,
    // Extra-Usage header key will include these two float fields:
    "timing_prompt_processing: ...,
    "timing_token_generation": ...,
  },
}
...
  
```

### Extra backends [*link*](index.html#extra-backends){.anchor aria-hidden="true"} {#extra-backends}

LocalAI can be extended with extra backends. The backends are
implemented as `gRPC` services and can be written in any language. The
container images that are built and published on
[quay.io![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIGhlaWdodD0iMTYiIHZpZXdib3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBmaWxsPSJjdXJyZW50Y29sb3IiIGQ9Ik0xNCA1Yy0uNTUyLjAtMS0uNDQ4LTEtMXMuNDQ4LTEgMS0xaDZjLjU1Mi4wIDEgLjQ0OCAxIDF2NmMwIC41NTItLjQ0OCAxLTEgMXMtMS0uNDQ4LTEtMVY2LjQxNGwtNy4yOTMgNy4yOTNjLS4zOTEuMzktMS4wMjQuMzktMS40MTQuMC0uMzkxLS4zOTEtLjM5MS0xLjAyNC4wLTEuNDE0TDE3LjU4NiA1SDE0ek01IDdjLS41NTIuMC0xIC40NDgtMSAxdjExYzAgLjU1Mi40NDggMSAxIDFoMTFjLjU1Mi4wIDEtLjQ0OCAxLTF2LTQuNTYzYzAtLjU1Mi40NDgtMSAxLTFzMSAuNDQ4IDEgMVYxOWMwIDEuNjU3LTEuMzQzIDMtMyAzSDVjLTEuNjU3LjAtMy0xLjM0My0zLTNWOGMwLTEuNjU3IDEuMzQzLTMgMy0zaDQuNTYzYy41NTIuMCAxIC40NDggMSAxcy0uNDQ4IDEtMSAxSDV6IiAvPjwvc3ZnPg==)](https://quay.io/repository/go-skynet/local-ai?tab=tags){rel="external"
target="_blank"} contain a set of images split in core and extra. By
default Images bring all the dependencies and backends supported by
LocalAI (we call those `extra` images). The `-core` images instead bring
only the strictly necessary dependencies to run LocalAI without only a
core set of backends.

If you wish to build a custom container image with extra backends, you
can use the core images and build only the backends you are interested
into or prepare the environment on startup by using the `EXTRA_BACKENDS`
environment variable. For instance, to use the diffusers backend:

``` {#74d5ed6 .language-Dockerfile}
  FROM quay.io/go-skynet/local-ai:master-ffmpeg-core

RUN make -C backend/python/diffusers
  
```

Remember also to set the `EXTERNAL_GRPC_BACKENDS` environment variable
(or `--external-grpc-backends` as CLI flag) to point to the backends you
are using (`EXTERNAL_GRPC_BACKENDS="backend_name:/path/to/backend"`),
for example with diffusers:

``` {#8ad3ffc .language-Dockerfile}
  FROM quay.io/go-skynet/local-ai:master-ffmpeg-core

RUN make -C backend/python/diffusers

ENV EXTERNAL_GRPC_BACKENDS="diffusers:/build/backend/python/diffusers/run.sh"
  
```

[notifications]{.material-icons .size-20 .me-2}

You can specify remote external backends or path to local files. The
syntax is `backend-name:/path/to/backend` or `backend-name:host:port`.

#### In runtime [*link*](index.html#in-runtime){.anchor aria-hidden="true"} {#in-runtime}

When using the `-core` container image it is possible to prepare the
python backends you are interested into by using the `EXTRA_BACKENDS`
variable, for instance:

``` {#9983717 .language-bash}
  docker run --env EXTRA_BACKENDS="backend/python/diffusers" quay.io/go-skynet/local-ai:master-ffmpeg-core
  
```

### Concurrent requests [*link*](index.html#concurrent-requests){.anchor aria-hidden="true"} {#concurrent-requests}

LocalAI supports parallel requests for the backends that supports it.
For instance, vLLM and llama.cpp supports parallel requests, and thus
LocalAI allows to run multiple requests in parallel.

In order to enable parallel requests, you have to pass
`--parallel-requests` or set the `PARALLEL_REQUEST` to true as
environment variable.

A list of the environment variable that tweaks parallelism is the
following:

``` {#304b435 .language-}
  ### Python backends GRPC max workers
### Default number of workers for GRPC Python backends.
### This actually controls wether a backend can process multiple requests or not.
# PYTHON_GRPC_MAX_WORKERS=1

### Define the number of parallel LLAMA.cpp workers (Defaults to 1)
# LLAMACPP_PARALLEL=1

### Enable to run parallel requests
# LOCALAI_PARALLEL_REQUESTS=true
  
```

Note that, for llama.cpp you need to set accordingly `LLAMACPP_PARALLEL`
to the number of parallel processes your GPU/CPU can handle. For
python-based backends (like vLLM) you can set `PYTHON_GRPC_MAX_WORKERS`
to the number of parallel requests.

### Disable CPU flagset auto detection in llama.cpp [*link*](index.html#disable-cpu-flagset-auto-detection-in-llamacpp){.anchor aria-hidden="true"} {#disable-cpu-flagset-auto-detection-in-llamacpp}

LocalAI will automatically discover the CPU flagset available in your
host and will use the most optimized version of the backends.

If you want to disable this behavior, you can set `DISABLE_AUTODETECT`
to `true` in the environment variables.

[[![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjAiIGhlaWdodD0iMjAiIHZpZXdib3g9IjAgMCAzMiAzMiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiBmaWxsPSJjdXJyZW50Y29sb3IiPjxwYXRoIGQ9Ik0xNiAuMzk2Yy04LjgzOS4wLTE2IDcuMTY3LTE2IDE2IDAgNy4wNzMgNC41ODQgMTMuMDY4IDEwLjkzNyAxNS4xODMuODAzLjE1MSAxLjA5My0uMzQ0IDEuMDkzLS43NzIuMC0uMzgtLjAwOS0xLjM4NS0uMDE1LTIuNzE5LTQuNDUzLjk2NC01LjM5MS0yLjE1MS01LjM5MS0yLjE1MS0uNzI5LTEuODQ0LTEuNzgxLTIuMzM5LTEuNzgxLTIuMzM5LTEuNDQ4LS45ODkuMTE1LS45NjguMTE1LS45NjggMS42MDQuMTA5IDIuNDQ4IDEuNjQ1IDIuNDQ4IDEuNjQ1IDEuNDI3IDIuNDQ4IDMuNzQ0IDEuNzQgNC42NjEgMS4zMjguMTQtMS4wMzEuNTU3LTEuNzQgMS4wMTEtMi4xMzUtMy41NTItLjQwMS03LjI4Ny0xLjc3Ni03LjI4Ny03LjkwNy4wLTEuNzUxLjYyLTMuMTc3IDEuNjQ1LTQuMjk3LS4xNzctLjQwMS0uNzE5LTIuMDMxLjE0MS00LjIzNS4wLjAgMS4zMzktLjQyNyA0LjQgMS42NDEgMS4yODEtLjM1NSAyLjY0MS0uNTMyIDQtLjU0MSAxLjM2LjAwOSAyLjcxOS4xODcgNCAuNTQxIDMuMDQzLTIuMDY4IDQuMzgxLTEuNjQxIDQuMzgxLTEuNjQxLjg1OSAyLjIwNC4zMTcgMy44MzMuMTYxIDQuMjM1IDEuMDE1IDEuMTIgMS42MzUgMi41NDcgMS42MzUgNC4yOTcuMCA2LjE0NS0zLjc0IDcuNS03LjI5NiA3Ljg5MS41NTYuNDc5IDEuMDc3IDEuNDY0IDEuMDc3IDIuOTU5LjAgMi4xNC0uMDIgMy44NjQtLjAyIDQuMzg1LjAuNDE2LjI4LjkxNiAxLjEwNC43NTUgNi40LTIuMDkzIDEwLjk3OS04LjA5MyAxMC45NzktMTUuMTU2LjAtOC44MzMtNy4xNjEtMTYtMTYtMTZ6IiAvPjwvc3ZnPg==)]{.me-1
.align-text-bottom}Edit this
page](https://github.com/mudler/LocalAI/blob/master/docs/content/docs/advanced/advanced-usage.md){alt="Advanced usage"
rel="noopener noreferrer" target="_blank"}

Last updated [28 Jan 2025, 22:58 +0100 ]{#relativetime
authdate="2025-01-28T22:58:35+0100" title="28 Jan 2025, 22:58 +0100"}.
[history]{.material-icons .size-20 .align-text-bottom .opacity-75}

<div>

------------------------------------------------------------------------

[](https://localai.io/docs/integrations/)

*navigate_before* Integrations

[](https://localai.io/docs/advanced/fine-tuning/){.ms-auto}

Fine-tuning LLMs for text generation *navigate_next*

</div>

¬© 2023-2025 [Ettore Di Giacinto](https://mudler.pm){target="_blank"}

![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiPjxwYXRoIGQ9Ik0xMiAxMC4yMjRsLTYuMyA2LjMtMS4zOC0xLjM3MkwxMiA3LjQ3Mmw3LjY4IDcuNjgtMS4zOCAxLjM3NnoiIHN0eWxlPSJmaWxsOiNmZmYiIC8+PC9zdmc+)
