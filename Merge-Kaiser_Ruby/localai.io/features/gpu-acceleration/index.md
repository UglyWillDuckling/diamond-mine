-   [![](https://github.com/go-skynet/LocalAI/assets/2420543/0966aa2a-166e-4f99-a3e5-6c915fc997dd){style="width:calc(65%);height:calc(65%)"}](../../index.html){aria-label="HomePage"
    alt="HomePage"}

    LocalAI

-   [![](https://img.shields.io/github/release/go-skynet/LocalAI?&label=Latest&style=for-the-badge)](https://github.com/go-skynet/LocalAI/releases)

-   [![](https://img.shields.io/badge/dockerhub-images-important.svg?logo=Docker)](https://hub.docker.com/r/localai/localai){target="_blank"}
    [![](https://img.shields.io/badge/quay.io-images-important.svg?)](https://quay.io/repository/go-skynet/local-ai?tab=tags&tag=latest){target="_blank"}

-   [*info* Overview](../../index.html){.sidebar-root-link}
-   *rocket_launch* Getting started

    -   [Quickstart](../../basics/getting_started/index.html){.sidebar-nested-link}
    -   [Install and Run
        Models](https://localai.io/docs/getting-started/models/){.sidebar-nested-link}
    -   [Try it out](../../basics/try/index.html){.sidebar-nested-link}
    -   [Customizing the
        Model](https://localai.io/docs/getting-started/customize-model/){.sidebar-nested-link}
    -   [Build LocalAI from
        source](../../basics/build/index.html){.sidebar-nested-link}
    -   [Run with container
        images](../../basics/container/index.html){.sidebar-nested-link}
    -   [Run with
        Kubernetes](../../basics/kubernetes/index.html){.sidebar-nested-link}
-   [*newspaper* News](../../basics/news/index.html){.sidebar-root-link}
-   *feature_search* Features

    -   [âš¡ GPU acceleration](index.html){.sidebar-nested-link}
    -   [ðŸ“– Text generation
        (GPT)](../text-generation/index.html){.sidebar-nested-link}
    -   [ðŸ“ˆ Reranker](../reranker/index.html){.sidebar-nested-link}
    -   [ðŸ—£ Text to audio
        (TTS)](../text-to-audio/index.html){.sidebar-nested-link}
    -   [ðŸŽ¨ Image
        generation](../image-generation/index.html){.sidebar-nested-link}
    -   [ðŸ§  Embeddings](../embeddings/index.html){.sidebar-nested-link}
    -   [ðŸ¥½ GPT Vision](../gpt-vision/index.html){.sidebar-nested-link}
    -   [âœï¸ Constrained
        Grammars](../constrained_grammars/index.html){.sidebar-nested-link}
    -   [ðŸ†•ðŸ–§ Distributed
        Inference](../distribute/index.html){.sidebar-nested-link}
    -   [ðŸ”ˆ Audio to
        text](../audio-to-text/index.html){.sidebar-nested-link}
    -   [ðŸ”¥ OpenAI functions and
        tools](../openai-functions/index.html){.sidebar-nested-link}
    -   [ðŸ’¾ Stores](../../stores/index.html){.sidebar-nested-link}
    -   [ðŸ–¼ï¸ Model
        gallery](../../models/index.html){.sidebar-nested-link}
-   [*sync*
    Integrations](https://localai.io/docs/integrations/){.sidebar-root-link}
-   *settings* Advanced

    -   [Advanced
        usage](../../advanced/index.html){.sidebar-nested-link}
    -   [Fine-tuning LLMs for text
        generation](https://localai.io/docs/advanced/fine-tuning/){.sidebar-nested-link}
    -   [Installer
        options](https://localai.io/docs/advanced/installer/){.sidebar-nested-link}
-   *menu_book* References

    -   [Model compatibility
        table](../../model-compatibility/index.html){.sidebar-nested-link}
    -   [Architecture](https://localai.io/docs/reference/architecture/){.sidebar-nested-link}
    -   [LocalAI
        binaries](https://localai.io/docs/reference/binaries/){.sidebar-nested-link}
    -   [Running on Nvidia
        ARM64](https://localai.io/docs/reference/nvidia-l4t/){.sidebar-nested-link}
-   [*quiz* FAQ](../../faq/index.html){.sidebar-root-link}

[](../../index.html){.logo-icon .me-3 aria-label="HomePage"
alt="HomePage"}

![](data:image/svg+xml;base64,PHN2ZyBpZD0iTGF5ZXJfMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Ym94PSIwIDAgMjUwIDI1MCI+PHBhdGggZD0ibTE0MyAzOS41Yy0xOCAwLTE4IDE4LTE4IDE4czAtMTgtMTgtMThIMjJjLTIuNzYuMC01IDIuMjQtNSA1djE0M2MwIDIuNzYgMi4yNCA1IDUgNWg3NmM3LjIuMCA4LjY0IDExLjUyIDguOTMgMTYuMTMuMDcgMS4wNS45NSAxLjg3IDIgMS44N2gzMi4xNGMxLjA2LjAgMS45NC0uODIgMi0xLjg3LjI5LTQuNjEgMS43My0xNi4xMyA4LjkzLTE2LjEzaDc2YzIuNzYuMCA1LTIuMjQgNS01VjQ0LjVjMC0yLjc2LTIuMjQtNS01LTVoLTg1ek0yMDYgMTYzYzAgMS4zOC0xLjEyIDIuNS0yLjUgMi41SDE0M2MtMTggMC0xOCAxOC0xOCAxOHMwLTE4LTE4LTE4SDQ2LjVjLTEuMzguMC0yLjUtMS4xMi0yLjUtMi41VjY5YzAtMS4zOCAxLjEyLTIuNSAyLjUtMi41SDk4YzcuMi4wIDguNjQgMTEuNTIgOC45MyAxNi4xMy4wNyAxLjA1Ljk1IDEuODcgMiAxLjg3aDMyLjE0YzEuMDYuMCAxLjk0LS44MiAyLTEuODcuMjktNC42MSAxLjczLTE2LjEzIDguOTMtMTYuMTNoNTEuNWMxLjM4LjAgMi41IDEuMTIgMi41IDIuNXY5NHoiIHN0eWxlPSJmaWxsOiMwNmYiIC8+PC9zdmc+){#Layer_1}

![](data:image/svg+xml;base64,PHN2ZyBpZD0iTGF5ZXJfMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Ym94PSIwIDAgMjUwIDI1MCI+PHBhdGggZD0ibTE0MyAzOS41Yy0xOCAwLTE4IDE4LTE4IDE4czAtMTgtMTgtMThIMjJjLTIuNzYuMC01IDIuMjQtNSA1djE0M2MwIDIuNzYgMi4yNCA1IDUgNWg3NmM3LjIuMCA4LjY0IDExLjUyIDguOTMgMTYuMTMuMDcgMS4wNS45NSAxLjg3IDIgMS44N2gzMi4xNGMxLjA2LjAgMS45NC0uODIgMi0xLjg3LjI5LTQuNjEgMS43My0xNi4xMyA4LjkzLTE2LjEzaDc2YzIuNzYuMCA1LTIuMjQgNS01VjQ0LjVjMC0yLjc2LTIuMjQtNS01LTVoLTg1ek0yMDYgMTYzYzAgMS4zOC0xLjEyIDIuNS0yLjUgMi41SDE0M2MtMTggMC0xOCAxOC0xOCAxOHMwLTE4LTE4LTE4SDQ2LjVjLTEuMzguMC0yLjUtMS4xMi0yLjUtMi41VjY5YzAtMS4zOCAxLjEyLTIuNSAyLjUtMi41SDk4YzcuMi4wIDguNjQgMTEuNTIgOC45MyAxNi4xMy4wNyAxLjA1Ljk1IDEuODcgMiAxLjg3aDMyLjE0YzEuMDYuMCAxLjk0LS44MiAyLTEuODcuMjktNC42MSAxLjczLTE2LjEzIDguOTMtMTYuMTNoNTEuNWMxLjM4LjAgMi41IDEuMTIgMi41IDIuNXY5NHoiIHN0eWxlPSJmaWxsOiMwNmYiIC8+PC9zdmc+){#Layer_1}

[menu]{.material-icons .size-20 .menu-icon .align-middle}

[search]{.material-icons .size-20 .menu-icon .align-middle}
[Search]{.flexsearch-button-placeholder .ms-1 .me-2 .d-none .d-sm-block}

[[![](data:image/svg+xml;base64,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)]{.kbd
.flexsearch-button-cmd-key}[![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTUiIGhlaWdodD0iMTUiPjxwYXRoIGQ9Ik01LjkyNiAxMi4yNzlINC40MUw5LjA3MyAyLjcyMUgxMC41OXoiIGZpbGw9ImN1cnJlbnRjb2xvciIgLz48L3N2Zz4=)]{.kbd
.flexsearch-button-key}]{.flexsearch-button-keys}

##### Star us on GitHub !Â 

[Star](https://github.com/mudler/LocalAI){.github-button
color-scheme="no-preference: light; light: light; dark: dark;"
icon="octicon-star" data-size="large" show-count="true"
aria-label="Star mudler/LocalAI on GitHub"}

-   [](https://github.com/mudler/LocalAI){alt="github"
    rel="noopener noreferrer" target="_blank"}

    ![](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld2JveD0iMCAwIDI0IDI0IiBmaWxsPSJub25lIiBzdHJva2U9ImN1cnJlbnRjb2xvciIgc3Ryb2tlLXdpZHRoPSIyIiBzdHJva2UtbGluZWNhcD0icm91bmQiIHN0cm9rZS1saW5lam9pbj0icm91bmQiPjx0aXRsZT5HaXRIdWI8L3RpdGxlPjxwYXRoIGQ9Ik05IDE5Yy01IDEuNS01LTIuNS03LTNtMTQgNnYtMy44N2EzLjM3IDMuMzcuMCAwMC0uOTQtMi42MWMzLjE0LS4zNSA2LjQ0LTEuNTQgNi40NC03QTUuNDQgNS40NC4wIDAwMjAgNC43NyA1LjA3IDUuMDcuMCAwMDE5LjkxIDFTMTguNzMuNjUgMTYgMi40OGExMy4zOCAxMy4zOC4wIDAwLTcgMEM2LjI3LjY1IDUuMDkgMSA1LjA5IDFBNS4wNyA1LjA3LjAgMDA1IDQuNzcgNS40NCA1LjQ0LjAgMDAzLjUgOC41NWMwIDUuNDIgMy4zIDYuNjEgNi40NCA3QTMuMzcgMy4zNy4wIDAwOSAxOC4xM1YyMiIgLz48L3N2Zz4=)
-   [](https://twitter.com/LocalAI_API){alt="twitter"
    rel="noopener noreferrer" target="_blank"}

    ![](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld2JveD0iMCAwIDI0IDI0Ij48dGl0bGU+VHdpdHRlciAvIFg8L3RpdGxlPjxwYXRoIGQ9Ik0uMDg4Ljc2OGw5LjI2NiAxMi4zOUwuMDI5IDIzLjIzMWgyLjFsOC4xNjMtOC44MTkgNi42IDguODE5aDcuMTQyTDE0LjI0MiAxMC4xNDUgMjIuOTIxLjc2OGgtMi4xTDEzLjMgOC44OTEgNy4yMjkuNzY4ek0zLjE3NCAyLjMxNEg2LjQ1NUwyMC45NDIgMjEuNjg1aC0zLjI4eiIgZmlsbD0iY3VycmVudGNvbG9yIiAvPjwvc3ZnPg==)
-   [](../../index.xml){alt="rss" rel="noopener noreferrer"
    target="_blank"}

    ![](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld2JveD0iMCAwIDI0IDI0IiBmaWxsPSJub25lIiBzdHJva2U9ImN1cnJlbnRjb2xvciIgc3Ryb2tlLXdpZHRoPSIyIiBzdHJva2UtbGluZWNhcD0icm91bmQiIHN0cm9rZS1saW5lam9pbj0icm91bmQiPjx0aXRsZT5SU1M8L3RpdGxlPjxwYXRoIGQ9Ik00IDExYTkgOSAwIDAxOSA5IiAvPjxwYXRoIGQ9Ik00IDRhMTYgMTYgMCAwMTE2IDE2IiAvPjxjaXJjbGUgY3g9IjUiIGN5PSIxOSIgcj0iMSI+PC9jaXJjbGU+PC9zdmc+)

[![](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMzAiIHdpZHRoPSIzMCIgdmlld2JveD0iMCAwIDQ4IDQ4IiBmaWxsPSJjdXJyZW50Y29sb3IiPjx0aXRsZT5FbmFibGUgZGFyayBtb2RlPC90aXRsZT48cGF0aCBkPSJNMjQgNDJxLTcuNS4wLTEyLjc1LTUuMjVUNiAyNHQ1LjI1LTEyLjc1VDI0IDZxLjQuMC44NS4wMjUuNDUuMDI1IDEuMTUuMDc1LTEuOCAxLjYtMi44IDMuOTV0LTEgNC45NXEwIDQuNSAzLjE1IDcuNjVRMjguNSAyNS44IDMzIDI1LjhxMi42LjAgNC45NS0uOTI1VDQxLjkgMjIuM3EuMDUuNi4wNzUuOTc1UTQyIDIzLjY1IDQyIDI0cTAgNy41LTUuMjUgMTIuNzVUMjQgNDJ6bTAtM3E1LjQ1LjAgOS41LTMuMzc1dDUuMDUtNy45MjVxLTEuMjUuNTUtMi42NzUuODI1UTM0LjQ1IDI4LjggMzMgMjguOHEtNS43NS4wLTkuNzc1LTQuMDI1VDE5LjIgMTVxMC0xLjIuMjUtMi41NzV0LjktMy4xMjVxLTQuOSAxLjM1LTguMTI1IDUuNDc1UTkgMTguOSA5IDI0cTAgNi4yNSA0LjM3NSAxMC42MjVUMjQgMzl6bS0uMi0xNC44NXoiIC8+PC9zdmc+)]{.toggle-dark}[![](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMzAiIHdpZHRoPSIzMCIgdmlld2JveD0iMCAwIDQ4IDQ4IiBmaWxsPSJjdXJyZW50Y29sb3IiPjx0aXRsZT5FbmFibGUgbGlnaHQgbW9kZTwvdGl0bGU+PHBhdGggZD0iTTI0IDMxcTIuOS4wIDQuOTUtMi4wNVQzMSAyNHQtMi4wNS00Ljk1VDI0IDE3dC00Ljk1IDIuMDVUMTcgMjR0Mi4wNSA0Ljk1VDI0IDMxem0wIDNxLTQuMTUuMC03LjA3NS0yLjkyNVQxNCAyNHQyLjkyNS03LjA3NVQyNCAxNHQ3LjA3NSAyLjkyNVQzNCAyNHQtMi45MjUgNy4wNzVUMjQgMzR6TTMuNSAyNS41cS0uNjUuMC0xLjA3NS0uNDI1UTIgMjQuNjUgMiAyNHQuNDI1LTEuMDc1UTIuODUgMjIuNSAzLjUgMjIuNWg1cS42NS4wIDEuMDc1LjQyNVExMCAyMy4zNSAxMCAyNHQtLjQyNSAxLjA3NVQ4LjUgMjUuNXptMzYgMHEtLjY1LjAtMS4wNzUtLjQyNVEzOCAyNC42NSAzOCAyNHQuNDI1LTEuMDc1VDM5LjUgMjIuNWg1cS42NS4wIDEuMDc1LjQyNVE0NiAyMy4zNSA0NiAyNHQtLjQyNSAxLjA3NS0xLjA3NS40MjV6TTI0IDEwcS0uNjUuMC0xLjA3NS0uNDI1UTIyLjUgOS4xNSAyMi41IDguNXYtNXEwLS42NS40MjUtMS4wNzVRMjMuMzUgMiAyNCAydDEuMDc1LjQyNVQyNS41IDMuNXY1cTAgLjY1LS40MjUgMS4wNzVRMjQuNjUgMTAgMjQgMTB6bTAgMzZxLS42NS4wLTEuMDc1LS40MjVUMjIuNSA0NC41di01cTAtLjY1LjQyNS0xLjA3NVEyMy4zNSAzOCAyNCAzOHQxLjA3NS40MjUuNDI1IDEuMDc1djVxMCAuNjUtLjQyNSAxLjA3NVEyNC42NSA0NiAyNCA0NnpNMTIgMTQuMWwtMi44NS0yLjhxLS40NS0uNDUtLjQyNS0xLjA3NS4wMjUtLjYyNS40MjUtMS4wNzUuNDUtLjQ1IDEuMDc1LS40NXQxLjA3NS40NUwxNC4xIDEycS40LjQ1LjQgMS4wNS4wLjYtLjQgMS0uNC40NS0xLjAyNS40NVQxMiAxNC4xem0yNC43IDI0Ljc1TDMzLjkgMzZxLS40LS40NS0uNC0xLjA3NXQuNDUtMS4wMjVxLjQtLjQ1IDEtLjQ1dDEuMDUuNDVsMi44NSAyLjhxLjQ1LjQ1LjQyNSAxLjA3NS0uMDI1LjYyNS0uNDI1IDEuMDc1LS40NS40NS0xLjA3NS40NXQtMS4wNzUtLjQ1ek0zMy45IDE0LjFxLS40NS0uNDUtLjQ1LTEuMDUuMC0uNi40NS0xLjA1bDIuOC0yLjg1cS40NS0uNDUgMS4wNzUtLjQyNS42MjUuMDI1IDEuMDc1LjQyNS40NS40NS40NSAxLjA3NXQtLjQ1IDEuMDc1TDM2IDE0LjFxLS40LjQtMS4wMjUuNHQtMS4wNzUtLjR6TTkuMTUgMzguODVxLS40NS0uNDUtLjQ1LTEuMDc1dC40NS0xLjA3NUwxMiAzMy45cS40NS0uNDUgMS4wNS0uNDUuNi4wIDEuMDUuNDUuNDUuNDUuNDUgMS4wNS4wLjYtLjQ1IDEuMDVsLTIuOCAyLjg1cS0uNDUuNDUtMS4wNzUuNDI1LS42MjUtLjAyNS0xLjA3NS0uNDI1ek0yNCAyNHoiIC8+PC9zdmc+)]{.toggle-light}

[![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTUiIGhlaWdodD0iMTUiIGFyaWEtbGFiZWw9IkFycm93IGRvd24iIHJvbGU9ImltZyI+PGcgZmlsbD0ibm9uZSIgc3Ryb2tlPSJjdXJyZW50Y29sb3IiIHN0cm9rZS1saW5lY2FwPSJyb3VuZCIgc3Ryb2tlLWxpbmVqb2luPSJyb3VuZCIgc3Ryb2tlLXdpZHRoPSIxLjIiPjxwYXRoIGQ9Ik03LjUgMy41djhtMy0zLTMgMy0zLTMiIC8+PC9nPjwvc3ZnPg==)]{.kbd
.flexsearch-button-cmd-key}[![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTUiIGhlaWdodD0iMTUiIGFyaWEtbGFiZWw9IkFycm93IHVwIiByb2xlPSJpbWciPjxnIGZpbGw9Im5vbmUiIHN0cm9rZT0iY3VycmVudGNvbG9yIiBzdHJva2UtbGluZWNhcD0icm91bmQiIHN0cm9rZS1saW5lam9pbj0icm91bmQiIHN0cm9rZS13aWR0aD0iMS4yIj48cGF0aCBkPSJNNy41IDExLjV2LThtMyAzLTMtMy0zIDMiIC8+PC9nPjwvc3ZnPg==)]{.kbd
.flexsearch-button-cmd-key}[to navigate]{.flexsearch-key-label}

[![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTUiIGhlaWdodD0iMTUiIGFyaWEtbGFiZWw9IkVudGVyIGtleSIgcm9sZT0iaW1nIj48ZyBmaWxsPSJub25lIiBzdHJva2U9ImN1cnJlbnRjb2xvciIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIiBzdHJva2UtbGluZWpvaW49InJvdW5kIiBzdHJva2Utd2lkdGg9IjEuMiI+PHBhdGggZD0iTTEyIDMuNTMwODh2M2MwIDEtMSAyLTIgMkg0bTMgMy0zLTMgMy0zIiAvPjwvZz48L3N2Zz4=)]{.kbd
.flexsearch-button-cmd-key}[to select]{.flexsearch-key-label}

[![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTUiIGhlaWdodD0iMTUiIGFyaWEtbGFiZWw9IkVzY2FwZSBrZXkiIHJvbGU9ImltZyI+PGcgZmlsbD0ibm9uZSIgc3Ryb2tlPSJjdXJyZW50Y29sb3IiIHN0cm9rZS1saW5lY2FwPSJyb3VuZCIgc3Ryb2tlLWxpbmVqb2luPSJyb3VuZCIgc3Ryb2tlLXdpZHRoPSIxLjIiPjxwYXRoIGQ9Ik0xMy42MTY3IDguOTM2Yy0uMTA2NS4zNTgzLS42ODgzLjk2Mi0xLjQ4NzUuOTYyLS43OTkzLjAtMS42NTMtLjkxNjUtMS42NTMtMi4xMjU4di0uNTY3OGMwLTEuMjU0OC43ODk2LTIuMTAxNiAxLjY1My0yLjEwMTZzMS4zNjAxLjQ3NzggMS40ODc1IDEuMDcyNE05IDZjLS4xMzUyLS40NzM1LS43NTA2LS45MjE5LTEuNDYtLjg5NzItLjcwOTIuMDI0Ni0xLjM0NC41Ny0xLjM0NCAxLjIxNjZzLjQxOTguODgxMiAxLjM0NDUuOTgwNUM4LjQ2NSA3LjM5OTIgOC45NjggNy45MzM3IDkgOC41cy0uNDU0IDEuMzk4LTEuNDU5NSAxLjM5OEM2LjY1OTMgOS44OTggNiA5IDUuOTYzIDguNDg1MW0tMS40NzQ4LjUzNjhjLS4yNjM1LjU5NDEtLjgwOTkuODc2LTEuNTQ0My44NzZzLTEuNzA3My0uNjI0OC0xLjcwNzMtMi4yMDR2LS40NjAzYzAtMS4wNDE2LjcyMS0yLjEzMSAxLjcwNzMtMi4xMzEuOTg2NC4wIDEuNjQyNSAxLjAzMSAxLjU0NDMgMi4yNDkyaC0yLjk1NiIgLz48L2c+PC9zdmc+)]{.kbd
.flexsearch-button-cmd-key}[to close]{.flexsearch-key-label}

cancel


-   [*Home*](../../docs){itemprop="item"}
-   [[Features]{itemprop="name"}](../index.html){itemprop="item"}
-   [âš¡ GPU acceleration]{itemprop="name"}

On this page

-   -   [Model configuration](index.html#model-configuration)
    -   [CUDA(NVIDIA) acceleration](index.html#cudanvidia-acceleration)
        -   [Requirements](index.html#requirements)
    -   [ROCM(AMD) acceleration](index.html#rocmamd-acceleration)
        -   [Requirements](index.html#requirements-1)
        -   [Recommendations](index.html#recommendations)
        -   [Limitations](index.html#limitations)
        -   [Verified](index.html#verified)
        -   [System Prep](index.html#system-prep)
        -   [Notes](index.html#notes)
    -   [Intel acceleration (sycl)](index.html#intel-acceleration-sycl)
        -   [Requirements](index.html#requirements-2)
        -   [Container images](index.html#container-images)
        -   [Notes](index.html#notes-1)

Table of Contents

-   -   [Model configuration](index.html#model-configuration)
    -   [CUDA(NVIDIA) acceleration](index.html#cudanvidia-acceleration)
        -   [Requirements](index.html#requirements)
    -   [ROCM(AMD) acceleration](index.html#rocmamd-acceleration)
        -   [Requirements](index.html#requirements-1)
        -   [Recommendations](index.html#recommendations)
        -   [Limitations](index.html#limitations)
        -   [Verified](index.html#verified)
        -   [System Prep](index.html#system-prep)
        -   [Notes](index.html#notes)
    -   [Intel acceleration (sycl)](index.html#intel-acceleration-sycl)
        -   [Requirements](index.html#requirements-2)
        -   [Container images](index.html#container-images)
        -   [Notes](index.html#notes-1)

*article*

# âš¡ GPU acceleration {#gpu-acceleration .content-title .mb-0}

[warning]{.material-icons .size-20 .me-2}

Section under construction

This section contains instruction on how to use LocalAI with GPU
acceleration.

âš¡

For accelleration for AMD or Metal HW is still in development, for
additional details see the
[build](../../basics/build/index.html#Acceleration)

## Model configuration [*link*](index.html#model-configuration){.anchor aria-hidden="true"} {#model-configuration}

Depending on the model architecture and backend used, there might be
different ways to enable GPU acceleration. It is required to configure
the model you intend to use with a YAML config file. For example, for
`llama.cpp` workloads a configuration file might look like this (where
`gpu_layers` is the number of layers to offload to the GPU):

``` {#2551b1b .language-yaml}
  name: my-model-name
# Default model parameters
parameters:
  # Relative to the models path
  model: llama.cpp-model.ggmlv3.q5_K_M.bin

context_size: 1024
threads: 1

f16: true # enable with GPU acceleration
gpu_layers: 22 # GPU Layers (only used when built with cublas)
  
```

For diffusers instead, it might look like this instead:

``` {#7512088 .language-yaml}
  name: stablediffusion
parameters:
  model: toonyou_beta6.safetensors
backend: diffusers
step: 30
f16: true
diffusers:
  pipeline_type: StableDiffusionPipeline
  cuda: true
  enable_parameters: "negative_prompt,num_inference_steps,clip_skip"
  scheduler_type: "k_dpmpp_sde"
  
```

## CUDA(NVIDIA) acceleration [*link*](index.html#cudanvidia-acceleration){.anchor aria-hidden="true"} {#cudanvidia-acceleration}

### Requirements [*link*](index.html#requirements){.anchor aria-hidden="true"} {#requirements}

Requirement: nvidia-container-toolkit (installation instructions
[1![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIGhlaWdodD0iMTYiIHZpZXdib3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBmaWxsPSJjdXJyZW50Y29sb3IiIGQ9Ik0xNCA1Yy0uNTUyLjAtMS0uNDQ4LTEtMXMuNDQ4LTEgMS0xaDZjLjU1Mi4wIDEgLjQ0OCAxIDF2NmMwIC41NTItLjQ0OCAxLTEgMXMtMS0uNDQ4LTEtMVY2LjQxNGwtNy4yOTMgNy4yOTNjLS4zOTEuMzktMS4wMjQuMzktMS40MTQuMC0uMzkxLS4zOTEtLjM5MS0xLjAyNC4wLTEuNDE0TDE3LjU4NiA1SDE0ek01IDdjLS41NTIuMC0xIC40NDgtMSAxdjExYzAgLjU1Mi40NDggMSAxIDFoMTFjLjU1Mi4wIDEtLjQ0OCAxLTF2LTQuNTYzYzAtLjU1Mi40NDgtMSAxLTFzMSAuNDQ4IDEgMVYxOWMwIDEuNjU3LTEuMzQzIDMtMyAzSDVjLTEuNjU3LjAtMy0xLjM0My0zLTNWOGMwLTEuNjU3IDEuMzQzLTMgMy0zaDQuNTYzYy41NTIuMCAxIC40NDggMSAxcy0uNDQ4IDEtMSAxSDV6IiAvPjwvc3ZnPg==)](https://www.server-world.info/en/note?os=Ubuntu_22.04&p=nvidia&f=2){rel="external"
target="_blank"}
[2![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIGhlaWdodD0iMTYiIHZpZXdib3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBmaWxsPSJjdXJyZW50Y29sb3IiIGQ9Ik0xNCA1Yy0uNTUyLjAtMS0uNDQ4LTEtMXMuNDQ4LTEgMS0xaDZjLjU1Mi4wIDEgLjQ0OCAxIDF2NmMwIC41NTItLjQ0OCAxLTEgMXMtMS0uNDQ4LTEtMVY2LjQxNGwtNy4yOTMgNy4yOTNjLS4zOTEuMzktMS4wMjQuMzktMS40MTQuMC0uMzkxLS4zOTEtLjM5MS0xLjAyNC4wLTEuNDE0TDE3LjU4NiA1SDE0ek01IDdjLS41NTIuMC0xIC40NDgtMSAxdjExYzAgLjU1Mi40NDggMSAxIDFoMTFjLjU1Mi4wIDEtLjQ0OCAxLTF2LTQuNTYzYzAtLjU1Mi40NDgtMSAxLTFzMSAuNDQ4IDEgMVYxOWMwIDEuNjU3LTEuMzQzIDMtMyAzSDVjLTEuNjU3LjAtMy0xLjM0My0zLTNWOGMwLTEuNjU3IDEuMzQzLTMgMy0zaDQuNTYzYy41NTIuMCAxIC40NDggMSAxcy0uNDQ4IDEtMSAxSDV6IiAvPjwvc3ZnPg==)](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html){rel="external"
target="_blank"})

To check what CUDA version do you need, you can either run `nvidia-smi`
or `nvcc --version`.

Alternatively, you can also check nvidia-smi with docker:

``` {#31425fe .language-}
  docker run --runtime=nvidia --rm nvidia/cuda nvidia-smi
  
```

To use CUDA, use the images with the `cublas` tag, for example.

The image list is on
[quay![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIGhlaWdodD0iMTYiIHZpZXdib3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBmaWxsPSJjdXJyZW50Y29sb3IiIGQ9Ik0xNCA1Yy0uNTUyLjAtMS0uNDQ4LTEtMXMuNDQ4LTEgMS0xaDZjLjU1Mi4wIDEgLjQ0OCAxIDF2NmMwIC41NTItLjQ0OCAxLTEgMXMtMS0uNDQ4LTEtMVY2LjQxNGwtNy4yOTMgNy4yOTNjLS4zOTEuMzktMS4wMjQuMzktMS40MTQuMC0uMzkxLS4zOTEtLjM5MS0xLjAyNC4wLTEuNDE0TDE3LjU4NiA1SDE0ek01IDdjLS41NTIuMC0xIC40NDgtMSAxdjExYzAgLjU1Mi40NDggMSAxIDFoMTFjLjU1Mi4wIDEtLjQ0OCAxLTF2LTQuNTYzYzAtLjU1Mi40NDgtMSAxLTFzMSAuNDQ4IDEgMVYxOWMwIDEuNjU3LTEuMzQzIDMtMyAzSDVjLTEuNjU3LjAtMy0xLjM0My0zLTNWOGMwLTEuNjU3IDEuMzQzLTMgMy0zaDQuNTYzYy41NTIuMCAxIC40NDggMSAxcy0uNDQ4IDEtMSAxSDV6IiAvPjwvc3ZnPg==)](https://quay.io/repository/go-skynet/local-ai?tab=tags){rel="external"
target="_blank"}:

-   CUDA `11` tags: `master-cublas-cuda11`, `v1.40.0-cublas-cuda11`, ...
-   CUDA `12` tags: `master-cublas-cuda12`, `v1.40.0-cublas-cuda12`, ...
-   CUDA `11` + FFmpeg tags: `master-cublas-cuda11-ffmpeg`,
    `v1.40.0-cublas-cuda11-ffmpeg`, ...
-   CUDA `12` + FFmpeg tags: `master-cublas-cuda12-ffmpeg`,
    `v1.40.0-cublas-cuda12-ffmpeg`, ...

In addition to the commands to run LocalAI normally, you need to specify
`--gpus all` to docker, for example:

``` {#ae3f419 .language-bash}
  docker run --rm -ti --gpus all -p 8080:8080 -e DEBUG=true -e MODELS_PATH=/models -e THREADS=1 -v $PWD/models:/models quay.io/go-skynet/local-ai:v1.40.0-cublas-cuda12
  
```

If the GPU inferencing is working, you should be able to see something
like:

``` {#07862d9 .language-}
  5:22PM DBG Loading model in memory from file: /models/open-llama-7b-q4_0.bin
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla T4
llama.cpp: loading model from /models/open-llama-7b-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 1024
llama_model_load_internal: n_embd     = 4096
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 32
llama_model_load_internal: n_layer    = 32
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 11008
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 7B
llama_model_load_internal: ggml ctx size =    0.07 MB
llama_model_load_internal: using CUDA for GPU acceleration
llama_model_load_internal: mem required  = 4321.77 MB (+ 1026.00 MB per state)
llama_model_load_internal: allocating batch_size x 1 MB = 512 MB VRAM for the scratch buffer
llama_model_load_internal: offloading 10 repeating layers to GPU
llama_model_load_internal: offloaded 10/35 layers to GPU
llama_model_load_internal: total VRAM used: 1598 MB
...................................................................................................
llama_init_from_file: kv self size  =  512.00 MB
  
```

## ROCM(AMD) acceleration [*link*](index.html#rocmamd-acceleration){.anchor aria-hidden="true"} {#rocmamd-acceleration}

There are a limited number of tested configurations for ROCm systems
however most newer deditated GPU consumer grade devices seem to be
supported under the current ROCm6 implementation.

Due to the nature of ROCm it is best to run all implementations in
containers as this limits the number of packages required for
installation on host system, compatability and package versions for
dependencies across all variations of OS must be tested independently if
disired, please refer to the
[build](../../basics/build/index.html#Acceleration) documentation.

### Requirements [*link*](index.html#requirements-1){.anchor aria-hidden="true"} {#requirements-1}

-   `ROCm 6.x.x` compatible GPU/accelerator
-   OS: `Ubuntu` (22.04, 20.04), `RHEL` (9.3, 9.2, 8.9, 8.8), `SLES`
    (15.5, 15.4)
-   Installed to host: `amdgpu-dkms` and `rocm` \>=6.0.0 as per ROCm
    documentation.

### Recommendations [*link*](index.html#recommendations){.anchor aria-hidden="true"} {#recommendations}

-   Do not use on a system running Wayland.
-   If running with Xorg do not use GPU assigned for compute for desktop
    rendering.
-   Ensure at least 100GB of free space on disk hosting container
    runtime and storing images prior to installation.

### Limitations [*link*](index.html#limitations){.anchor aria-hidden="true"} {#limitations}

Ongoing verification testing of ROCm compatability with integrated
backends. Please note the following list of verified backends and
devices.

LocalAI hipblas images are built against the following targets:
gfx900,gfx906,gfx908,gfx940,gfx941,gfx942,gfx90a,gfx1030,gfx1031,gfx1100,gfx1101

If your device is not one of these you must specify the corresponding
`GPU_TARGETS` and specify `REBUILD=true`. Otherwise you don't need to
specify these in the commands below.

### Verified [*link*](index.html#verified){.anchor aria-hidden="true"} {#verified}

The devices in the following list have been tested with `hipblas` images
running `ROCm 6.0.0`

  Backend                 Verified   Devices
  ----------------------- ---------- ---------------------
  llama.cpp               yes        Radeon VII (gfx906)
  diffusers               yes        Radeon VII (gfx906)
  piper                   yes        Radeon VII (gfx906)
  whisper                 no         none
  autogptq                no         none
  bark                    no         none
  coqui                   no         none
  transformers            no         none
  exllama                 no         none
  exllama2                no         none
  mamba                   no         none
  sentencetransformers    no         none
  transformers-musicgen   no         none
  vall-e-x                no         none
  vllm                    no         none

**You can help by expanding this list.**

### System Prep [*link*](index.html#system-prep){.anchor aria-hidden="true"} {#system-prep}

1.  Check your GPU LLVM target is compatible with the version of ROCm.
    This can be found in the [LLVM
    Docs![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIGhlaWdodD0iMTYiIHZpZXdib3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBmaWxsPSJjdXJyZW50Y29sb3IiIGQ9Ik0xNCA1Yy0uNTUyLjAtMS0uNDQ4LTEtMXMuNDQ4LTEgMS0xaDZjLjU1Mi4wIDEgLjQ0OCAxIDF2NmMwIC41NTItLjQ0OCAxLTEgMXMtMS0uNDQ4LTEtMVY2LjQxNGwtNy4yOTMgNy4yOTNjLS4zOTEuMzktMS4wMjQuMzktMS40MTQuMC0uMzkxLS4zOTEtLjM5MS0xLjAyNC4wLTEuNDE0TDE3LjU4NiA1SDE0ek01IDdjLS41NTIuMC0xIC40NDgtMSAxdjExYzAgLjU1Mi40NDggMSAxIDFoMTFjLjU1Mi4wIDEtLjQ0OCAxLTF2LTQuNTYzYzAtLjU1Mi40NDgtMSAxLTFzMSAuNDQ4IDEgMVYxOWMwIDEuNjU3LTEuMzQzIDMtMyAzSDVjLTEuNjU3LjAtMy0xLjM0My0zLTNWOGMwLTEuNjU3IDEuMzQzLTMgMy0zaDQuNTYzYy41NTIuMCAxIC40NDggMSAxcy0uNDQ4IDEtMSAxSDV6IiAvPjwvc3ZnPg==)](https://llvm.org/docs/AMDGPUUsage.html){rel="external"
    target="_blank"}.
2.  Check which ROCm version is compatible with your LLVM target and
    your chosen OS (pay special attention to supported kernel versions).
    See the following for compatability for ([ROCm
    6.0.0![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIGhlaWdodD0iMTYiIHZpZXdib3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBmaWxsPSJjdXJyZW50Y29sb3IiIGQ9Ik0xNCA1Yy0uNTUyLjAtMS0uNDQ4LTEtMXMuNDQ4LTEgMS0xaDZjLjU1Mi4wIDEgLjQ0OCAxIDF2NmMwIC41NTItLjQ0OCAxLTEgMXMtMS0uNDQ4LTEtMVY2LjQxNGwtNy4yOTMgNy4yOTNjLS4zOTEuMzktMS4wMjQuMzktMS40MTQuMC0uMzkxLS4zOTEtLjM5MS0xLjAyNC4wLTEuNDE0TDE3LjU4NiA1SDE0ek01IDdjLS41NTIuMC0xIC40NDgtMSAxdjExYzAgLjU1Mi40NDggMSAxIDFoMTFjLjU1Mi4wIDEtLjQ0OCAxLTF2LTQuNTYzYzAtLjU1Mi40NDgtMSAxLTFzMSAuNDQ4IDEgMVYxOWMwIDEuNjU3LTEuMzQzIDMtMyAzSDVjLTEuNjU3LjAtMy0xLjM0My0zLTNWOGMwLTEuNjU3IDEuMzQzLTMgMy0zaDQuNTYzYy41NTIuMCAxIC40NDggMSAxcy0uNDQ4IDEtMSAxSDV6IiAvPjwvc3ZnPg==)](https://rocm.docs.amd.com/projects/install-on-linux/en/docs-6.0.0/reference/system-requirements.html){rel="external"
    target="_blank"}) or ([ROCm
    6.0.2![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIGhlaWdodD0iMTYiIHZpZXdib3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBmaWxsPSJjdXJyZW50Y29sb3IiIGQ9Ik0xNCA1Yy0uNTUyLjAtMS0uNDQ4LTEtMXMuNDQ4LTEgMS0xaDZjLjU1Mi4wIDEgLjQ0OCAxIDF2NmMwIC41NTItLjQ0OCAxLTEgMXMtMS0uNDQ4LTEtMVY2LjQxNGwtNy4yOTMgNy4yOTNjLS4zOTEuMzktMS4wMjQuMzktMS40MTQuMC0uMzkxLS4zOTEtLjM5MS0xLjAyNC4wLTEuNDE0TDE3LjU4NiA1SDE0ek01IDdjLS41NTIuMC0xIC40NDgtMSAxdjExYzAgLjU1Mi40NDggMSAxIDFoMTFjLjU1Mi4wIDEtLjQ0OCAxLTF2LTQuNTYzYzAtLjU1Mi40NDgtMSAxLTFzMSAuNDQ4IDEgMVYxOWMwIDEuNjU3LTEuMzQzIDMtMyAzSDVjLTEuNjU3LjAtMy0xLjM0My0zLTNWOGMwLTEuNjU3IDEuMzQzLTMgMy0zaDQuNTYzYy41NTIuMCAxIC40NDggMSAxcy0uNDQ4IDEtMSAxSDV6IiAvPjwvc3ZnPg==)](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html){rel="external"
    target="_blank"})
3.  Install you chosen version of the `dkms` and `rocm` (it is
    recommended that the native package manager be used for this process
    for any OS as version changes are executed more easily via this
    method if updates are required). Take care to restart after
    installing `amdgpu-dkms` and before installing `rocm`, for details
    regarding this see the installation documentation for your chosen OS
    ([6.0.2![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIGhlaWdodD0iMTYiIHZpZXdib3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBmaWxsPSJjdXJyZW50Y29sb3IiIGQ9Ik0xNCA1Yy0uNTUyLjAtMS0uNDQ4LTEtMXMuNDQ4LTEgMS0xaDZjLjU1Mi4wIDEgLjQ0OCAxIDF2NmMwIC41NTItLjQ0OCAxLTEgMXMtMS0uNDQ4LTEtMVY2LjQxNGwtNy4yOTMgNy4yOTNjLS4zOTEuMzktMS4wMjQuMzktMS40MTQuMC0uMzkxLS4zOTEtLjM5MS0xLjAyNC4wLTEuNDE0TDE3LjU4NiA1SDE0ek01IDdjLS41NTIuMC0xIC40NDgtMSAxdjExYzAgLjU1Mi40NDggMSAxIDFoMTFjLjU1Mi4wIDEtLjQ0OCAxLTF2LTQuNTYzYzAtLjU1Mi40NDgtMSAxLTFzMSAuNDQ4IDEgMVYxOWMwIDEuNjU3LTEuMzQzIDMtMyAzSDVjLTEuNjU3LjAtMy0xLjM0My0zLTNWOGMwLTEuNjU3IDEuMzQzLTMgMy0zaDQuNTYzYy41NTIuMCAxIC40NDggMSAxcy0uNDQ4IDEtMSAxSDV6IiAvPjwvc3ZnPg==)](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/how-to/native-install/index.html){rel="external"
    target="_blank"} or
    [6.0.0![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIGhlaWdodD0iMTYiIHZpZXdib3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBmaWxsPSJjdXJyZW50Y29sb3IiIGQ9Ik0xNCA1Yy0uNTUyLjAtMS0uNDQ4LTEtMXMuNDQ4LTEgMS0xaDZjLjU1Mi4wIDEgLjQ0OCAxIDF2NmMwIC41NTItLjQ0OCAxLTEgMXMtMS0uNDQ4LTEtMVY2LjQxNGwtNy4yOTMgNy4yOTNjLS4zOTEuMzktMS4wMjQuMzktMS40MTQuMC0uMzkxLS4zOTEtLjM5MS0xLjAyNC4wLTEuNDE0TDE3LjU4NiA1SDE0ek01IDdjLS41NTIuMC0xIC40NDgtMSAxdjExYzAgLjU1Mi40NDggMSAxIDFoMTFjLjU1Mi4wIDEtLjQ0OCAxLTF2LTQuNTYzYzAtLjU1Mi40NDgtMSAxLTFzMSAuNDQ4IDEgMVYxOWMwIDEuNjU3LTEuMzQzIDMtMyAzSDVjLTEuNjU3LjAtMy0xLjM0My0zLTNWOGMwLTEuNjU3IDEuMzQzLTMgMy0zaDQuNTYzYy41NTIuMCAxIC40NDggMSAxcy0uNDQ4IDEtMSAxSDV6IiAvPjwvc3ZnPg==)](https://rocm.docs.amd.com/projects/install-on-linux/en/docs-6.0.0/how-to/native-install/index.html){rel="external"
    target="_blank"})
4.  Deploy. Yes it's that easy.

#### Setup Example (Docker/containerd) [*link*](index.html#setup-example-dockercontainerd){.anchor aria-hidden="true"} {#setup-example-dockercontainerd}

The following are examples of the ROCm specific configuration elements
required.

``` {#07d13ed .language-yaml}
  # docker-compose.yaml
    # For full functionality select a non-'core' image, version locking the image is recommended for debug purposes.
    image: quay.io/go-skynet/local-ai:master-aio-gpu-hipblas
    environment:
      - DEBUG=true
      # If your gpu is not already included in the current list of default targets the following build details are required.
      - REBUILD=true
      - BUILD_TYPE=hipblas
      - GPU_TARGETS=gfx906 # Example for Radeon VII
    devices:
      # AMD GPU only require the following devices be passed through to the container for offloading to occur.
      - /dev/dri
      - /dev/kfd
  
```

The same can also be executed as a `run` for your container runtime

``` {#3cb46d7 .language-}
  docker run \
 -e DEBUG=true \
 -e REBUILD=true \
 -e BUILD_TYPE=hipblas \
 -e GPU_TARGETS=gfx906 \
 --device /dev/dri \
 --device /dev/kfd \
 quay.io/go-skynet/local-ai:master-aio-gpu-hipblas
  
```

Please ensure to add all other required environment variables, port
forwardings, etc to your `compose` file or `run` command.

The rebuild process will take some time to complete when deploying these
containers and it is recommended that you `pull` the image prior to
deployment as depending on the version these images may be \~20GB in
size.

#### Example (k8s) (Advanced Deployment/WIP) [*link*](index.html#example-k8s-advanced-deploymentwip){.anchor aria-hidden="true"} {#example-k8s-advanced-deploymentwip}

For k8s deployments there is an additional step required before
deployment, this is the deployment of the
[ROCm/k8s-device-plugin![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIGhlaWdodD0iMTYiIHZpZXdib3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBmaWxsPSJjdXJyZW50Y29sb3IiIGQ9Ik0xNCA1Yy0uNTUyLjAtMS0uNDQ4LTEtMXMuNDQ4LTEgMS0xaDZjLjU1Mi4wIDEgLjQ0OCAxIDF2NmMwIC41NTItLjQ0OCAxLTEgMXMtMS0uNDQ4LTEtMVY2LjQxNGwtNy4yOTMgNy4yOTNjLS4zOTEuMzktMS4wMjQuMzktMS40MTQuMC0uMzkxLS4zOTEtLjM5MS0xLjAyNC4wLTEuNDE0TDE3LjU4NiA1SDE0ek01IDdjLS41NTIuMC0xIC40NDgtMSAxdjExYzAgLjU1Mi40NDggMSAxIDFoMTFjLjU1Mi4wIDEtLjQ0OCAxLTF2LTQuNTYzYzAtLjU1Mi40NDgtMSAxLTFzMSAuNDQ4IDEgMVYxOWMwIDEuNjU3LTEuMzQzIDMtMyAzSDVjLTEuNjU3LjAtMy0xLjM0My0zLTNWOGMwLTEuNjU3IDEuMzQzLTMgMy0zaDQuNTYzYy41NTIuMCAxIC40NDggMSAxcy0uNDQ4IDEtMSAxSDV6IiAvPjwvc3ZnPg==)](https://artifacthub.io/packages/helm/amd-gpu-helm/amd-gpu){rel="external"
target="_blank"}. For any k8s environment the documentation provided by
AMD from the ROCm project should be successful. It is recommended that
if you use rke2 or OpenShift that you deploy the SUSE or RedHat provided
version of this resource to ensure compatability. After this has been
completed the [helm chart from
go-skynet![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIGhlaWdodD0iMTYiIHZpZXdib3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBmaWxsPSJjdXJyZW50Y29sb3IiIGQ9Ik0xNCA1Yy0uNTUyLjAtMS0uNDQ4LTEtMXMuNDQ4LTEgMS0xaDZjLjU1Mi4wIDEgLjQ0OCAxIDF2NmMwIC41NTItLjQ0OCAxLTEgMXMtMS0uNDQ4LTEtMVY2LjQxNGwtNy4yOTMgNy4yOTNjLS4zOTEuMzktMS4wMjQuMzktMS40MTQuMC0uMzkxLS4zOTEtLjM5MS0xLjAyNC4wLTEuNDE0TDE3LjU4NiA1SDE0ek01IDdjLS41NTIuMC0xIC40NDgtMSAxdjExYzAgLjU1Mi40NDggMSAxIDFoMTFjLjU1Mi4wIDEtLjQ0OCAxLTF2LTQuNTYzYzAtLjU1Mi40NDgtMSAxLTFzMSAuNDQ4IDEgMVYxOWMwIDEuNjU3LTEuMzQzIDMtMyAzSDVjLTEuNjU3LjAtMy0xLjM0My0zLTNWOGMwLTEuNjU3IDEuMzQzLTMgMy0zaDQuNTYzYy41NTIuMCAxIC40NDggMSAxcy0uNDQ4IDEtMSAxSDV6IiAvPjwvc3ZnPg==)](https://github.com/go-skynet/helm-charts){rel="external"
target="_blank"} can be configured and deployed mostly un-edited.

The following are details of the changes that should be made to ensure
proper function. While these details may be configurable in the
`values.yaml` development of this Helm chart is ongoing and is subject
to change.

The following details indicate the final state of the localai deployment
relevant to GPU function.

``` {#1da5122 .language-yaml}
  apiVersion: apps/v1
kind: Deployment
metadata:
  name: {NAME}-local-ai
...
spec:
  ...
  template:
    ...
    spec:
      containers:
        - env:
            - name: HIP_VISIBLE_DEVICES
              value: '0'
              # This variable indicates the devices availible to container (0:device1 1:device2 2:device3) etc.
              # For multiple devices (say device 1 and 3) the value would be equivelant to HIP_VISIBLE_DEVICES="0,2"
              # Please take note of this when an iGPU is present in host system as compatability is not assured.
          ...
          resources:
            limits:
              amd.com/gpu: '1'
            requests:
              amd.com/gpu: '1'
  
```

This configuration has been tested on a 'custom' cluster managed by SUSE
Rancher that was deployed on top of Ubuntu 22.04.4, certification of
other configuration is ongoing and compatability is not gauranteed.

### Notes [*link*](index.html#notes){.anchor aria-hidden="true"} {#notes}

-   When installing the ROCM kernel driver on your system ensure that
    you are installing an equal or newer version that that which is
    currently implemented in LocalAI (6.0.0 at time of writing).
-   AMD documentation indicates that this will ensure functionality
    however your milage may vary depending on the GPU and distro you are
    using.
-   If you encounter an `Error 413` on attempting to upload an audio
    file or image for whisper or llava/bakllava on a k8s deployment,
    note that the ingress for your deployment may require the
    annontation `nginx.ingress.kubernetes.io/proxy-body-size: "25m"` to
    allow larger uploads. This may be included in future versions of the
    helm chart.

## Intel acceleration (sycl) [*link*](index.html#intel-acceleration-sycl){.anchor aria-hidden="true"} {#intel-acceleration-sycl}

### Requirements [*link*](index.html#requirements-2){.anchor aria-hidden="true"} {#requirements-2}

If building from source, you need to install [Intel oneAPI Base
Toolkit![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIGhlaWdodD0iMTYiIHZpZXdib3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBmaWxsPSJjdXJyZW50Y29sb3IiIGQ9Ik0xNCA1Yy0uNTUyLjAtMS0uNDQ4LTEtMXMuNDQ4LTEgMS0xaDZjLjU1Mi4wIDEgLjQ0OCAxIDF2NmMwIC41NTItLjQ0OCAxLTEgMXMtMS0uNDQ4LTEtMVY2LjQxNGwtNy4yOTMgNy4yOTNjLS4zOTEuMzktMS4wMjQuMzktMS40MTQuMC0uMzkxLS4zOTEtLjM5MS0xLjAyNC4wLTEuNDE0TDE3LjU4NiA1SDE0ek01IDdjLS41NTIuMC0xIC40NDgtMSAxdjExYzAgLjU1Mi40NDggMSAxIDFoMTFjLjU1Mi4wIDEtLjQ0OCAxLTF2LTQuNTYzYzAtLjU1Mi40NDgtMSAxLTFzMSAuNDQ4IDEgMVYxOWMwIDEuNjU3LTEuMzQzIDMtMyAzSDVjLTEuNjU3LjAtMy0xLjM0My0zLTNWOGMwLTEuNjU3IDEuMzQzLTMgMy0zaDQuNTYzYy41NTIuMCAxIC40NDggMSAxcy0uNDQ4IDEtMSAxSDV6IiAvPjwvc3ZnPg==)](https://software.intel.com/content/www/us/en/develop/tools/oneapi/base-toolkit/download.html){rel="external"
target="_blank"} and have the Intel drivers available in the system.

### Container images [*link*](index.html#container-images){.anchor aria-hidden="true"} {#container-images}

To use SYCL, use the images with the `sycl-f16` or `sycl-f32` tag, for
example `v2.27.0-sycl-f32-core`, `v2.27.0-sycl-f16-ffmpeg-core`, ...

The image list is on
[quay![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIGhlaWdodD0iMTYiIHZpZXdib3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBmaWxsPSJjdXJyZW50Y29sb3IiIGQ9Ik0xNCA1Yy0uNTUyLjAtMS0uNDQ4LTEtMXMuNDQ4LTEgMS0xaDZjLjU1Mi4wIDEgLjQ0OCAxIDF2NmMwIC41NTItLjQ0OCAxLTEgMXMtMS0uNDQ4LTEtMVY2LjQxNGwtNy4yOTMgNy4yOTNjLS4zOTEuMzktMS4wMjQuMzktMS40MTQuMC0uMzkxLS4zOTEtLjM5MS0xLjAyNC4wLTEuNDE0TDE3LjU4NiA1SDE0ek01IDdjLS41NTIuMC0xIC40NDgtMSAxdjExYzAgLjU1Mi40NDggMSAxIDFoMTFjLjU1Mi4wIDEtLjQ0OCAxLTF2LTQuNTYzYzAtLjU1Mi40NDgtMSAxLTFzMSAuNDQ4IDEgMVYxOWMwIDEuNjU3LTEuMzQzIDMtMyAzSDVjLTEuNjU3LjAtMy0xLjM0My0zLTNWOGMwLTEuNjU3IDEuMzQzLTMgMy0zaDQuNTYzYy41NTIuMCAxIC40NDggMSAxcy0uNDQ4IDEtMSAxSDV6IiAvPjwvc3ZnPg==)](https://quay.io/repository/go-skynet/local-ai?tab=tags){rel="external"
target="_blank"}.

#### Example [*link*](index.html#example){.anchor aria-hidden="true"} {#example}

To run LocalAI with Docker and sycl starting `phi-2`, you can use the
following command as an example:

``` {#5b99b83 .language-bash}
  docker run -e DEBUG=true --privileged -ti -v $PWD/models:/build/models -p 8080:8080  -v /dev/dri:/dev/dri --rm quay.io/go-skynet/local-ai:master-sycl-f32-ffmpeg-core phi-2
  
```

### Notes [*link*](index.html#notes-1){.anchor aria-hidden="true"} {#notes-1}

In addition to the commands to run LocalAI normally, you need to specify
`--device /dev/dri` to docker, for example:

``` {#a0a4041 .language-bash}
  docker run --rm -ti --device /dev/dri -p 8080:8080 -e DEBUG=true -e MODELS_PATH=/models -e THREADS=1 -v $PWD/models:/models quay.io/go-skynet/local-ai:v2.27.0-sycl-f16-ffmpeg-core
  
```

Note also that sycl does have a known issue to hang with `mmap: true`.
You have to disable it in the model configuration if explicitly enabled.

[[![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjAiIGhlaWdodD0iMjAiIHZpZXdib3g9IjAgMCAzMiAzMiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiBmaWxsPSJjdXJyZW50Y29sb3IiPjxwYXRoIGQ9Ik0xNiAuMzk2Yy04LjgzOS4wLTE2IDcuMTY3LTE2IDE2IDAgNy4wNzMgNC41ODQgMTMuMDY4IDEwLjkzNyAxNS4xODMuODAzLjE1MSAxLjA5My0uMzQ0IDEuMDkzLS43NzIuMC0uMzgtLjAwOS0xLjM4NS0uMDE1LTIuNzE5LTQuNDUzLjk2NC01LjM5MS0yLjE1MS01LjM5MS0yLjE1MS0uNzI5LTEuODQ0LTEuNzgxLTIuMzM5LTEuNzgxLTIuMzM5LTEuNDQ4LS45ODkuMTE1LS45NjguMTE1LS45NjggMS42MDQuMTA5IDIuNDQ4IDEuNjQ1IDIuNDQ4IDEuNjQ1IDEuNDI3IDIuNDQ4IDMuNzQ0IDEuNzQgNC42NjEgMS4zMjguMTQtMS4wMzEuNTU3LTEuNzQgMS4wMTEtMi4xMzUtMy41NTItLjQwMS03LjI4Ny0xLjc3Ni03LjI4Ny03LjkwNy4wLTEuNzUxLjYyLTMuMTc3IDEuNjQ1LTQuMjk3LS4xNzctLjQwMS0uNzE5LTIuMDMxLjE0MS00LjIzNS4wLjAgMS4zMzktLjQyNyA0LjQgMS42NDEgMS4yODEtLjM1NSAyLjY0MS0uNTMyIDQtLjU0MSAxLjM2LjAwOSAyLjcxOS4xODcgNCAuNTQxIDMuMDQzLTIuMDY4IDQuMzgxLTEuNjQxIDQuMzgxLTEuNjQxLjg1OSAyLjIwNC4zMTcgMy44MzMuMTYxIDQuMjM1IDEuMDE1IDEuMTIgMS42MzUgMi41NDcgMS42MzUgNC4yOTcuMCA2LjE0NS0zLjc0IDcuNS03LjI5NiA3Ljg5MS41NTYuNDc5IDEuMDc3IDEuNDY0IDEuMDc3IDIuOTU5LjAgMi4xNC0uMDIgMy44NjQtLjAyIDQuMzg1LjAuNDE2LjI4LjkxNiAxLjEwNC43NTUgNi40LTIuMDkzIDEwLjk3OS04LjA5MyAxMC45NzktMTUuMTU2LjAtOC44MzMtNy4xNjEtMTYtMTYtMTZ6IiAvPjwvc3ZnPg==)]{.me-1
.align-text-bottom}Edit this
page](https://github.com/mudler/LocalAI/blob/master/docs/content/docs/features/GPU-acceleration.md){alt="âš¡ GPU acceleration"
rel="noopener noreferrer" target="_blank"}

Last updated [24 Aug 2024, 09:58 +0200 ]{#relativetime
authdate="2024-08-24T09:58:49+0200" title="24 Aug 2024, 09:58 +0200"}.
[history]{.material-icons .size-20 .align-text-bottom .opacity-75}

<div>

------------------------------------------------------------------------

[](../../basics/news/index.html)

*navigate_before* News

[](../text-generation/index.html){.ms-auto}

ðŸ“– Text generation (GPT) *navigate_next*

</div>

Â© 2023-2025 [Ettore Di Giacinto](https://mudler.pm){target="_blank"}

![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiPjxwYXRoIGQ9Ik0xMiAxMC4yMjRsLTYuMyA2LjMtMS4zOC0xLjM3MkwxMiA3LjQ3Mmw3LjY4IDcuNjgtMS4zOCAxLjM3NnoiIHN0eWxlPSJmaWxsOiNmZmYiIC8+PC9zdmc+)
