-   [![](https://github.com/go-skynet/LocalAI/assets/2420543/0966aa2a-166e-4f99-a3e5-6c915fc997dd){style="width:calc(65%);height:calc(65%)"}](../../index.html){aria-label="HomePage"
    alt="HomePage"}

    LocalAI

-   [![](https://img.shields.io/github/release/go-skynet/LocalAI?&label=Latest&style=for-the-badge)](https://github.com/go-skynet/LocalAI/releases)

-   [![](https://img.shields.io/badge/dockerhub-images-important.svg?logo=Docker)](https://hub.docker.com/r/localai/localai){target="_blank"}
    [![](https://img.shields.io/badge/quay.io-images-important.svg?)](https://quay.io/repository/go-skynet/local-ai?tab=tags&tag=latest){target="_blank"}

-   [*info* Overview](../../index.html){.sidebar-root-link}
-   *rocket_launch* Getting started

    -   [Quickstart](../../basics/getting_started/index.html){.sidebar-nested-link}
    -   [Install and Run
        Models](https://localai.io/docs/getting-started/models/){.sidebar-nested-link}
    -   [Try it out](../../basics/try/index.html){.sidebar-nested-link}
    -   [Customizing the
        Model](https://localai.io/docs/getting-started/customize-model/){.sidebar-nested-link}
    -   [Build LocalAI from
        source](../../basics/build/index.html){.sidebar-nested-link}
    -   [Run with container
        images](../../basics/container/index.html){.sidebar-nested-link}
    -   [Run with
        Kubernetes](../../basics/kubernetes/index.html){.sidebar-nested-link}
-   [*newspaper* News](../../basics/news/index.html){.sidebar-root-link}
-   *feature_search* Features

    -   [‚ö° GPU
        acceleration](../gpu-acceleration/index.html){.sidebar-nested-link}
    -   [üìñ Text generation (GPT)](index.html){.sidebar-nested-link}
    -   [üìà Reranker](../reranker/index.html){.sidebar-nested-link}
    -   [üó£ Text to audio
        (TTS)](../text-to-audio/index.html){.sidebar-nested-link}
    -   [üé® Image
        generation](../image-generation/index.html){.sidebar-nested-link}
    -   [üß† Embeddings](../embeddings/index.html){.sidebar-nested-link}
    -   [ü•Ω GPT Vision](../gpt-vision/index.html){.sidebar-nested-link}
    -   [‚úçÔ∏è Constrained
        Grammars](../constrained_grammars/index.html){.sidebar-nested-link}
    -   [üÜïüñß Distributed
        Inference](../distribute/index.html){.sidebar-nested-link}
    -   [üîà Audio to
        text](../audio-to-text/index.html){.sidebar-nested-link}
    -   [üî• OpenAI functions and
        tools](../openai-functions/index.html){.sidebar-nested-link}
    -   [üíæ Stores](../../stores/index.html){.sidebar-nested-link}
    -   [üñºÔ∏è Model
        gallery](../../models/index.html){.sidebar-nested-link}
-   [*sync*
    Integrations](https://localai.io/docs/integrations/){.sidebar-root-link}
-   *settings* Advanced

    -   [Advanced
        usage](../../advanced/index.html){.sidebar-nested-link}
    -   [Fine-tuning LLMs for text
        generation](https://localai.io/docs/advanced/fine-tuning/){.sidebar-nested-link}
    -   [Installer
        options](https://localai.io/docs/advanced/installer/){.sidebar-nested-link}
-   *menu_book* References

    -   [Model compatibility
        table](../../model-compatibility/index.html){.sidebar-nested-link}
    -   [Architecture](https://localai.io/docs/reference/architecture/){.sidebar-nested-link}
    -   [LocalAI
        binaries](https://localai.io/docs/reference/binaries/){.sidebar-nested-link}
    -   [Running on Nvidia
        ARM64](https://localai.io/docs/reference/nvidia-l4t/){.sidebar-nested-link}
-   [*quiz* FAQ](../../faq/index.html){.sidebar-root-link}

[](../../index.html){.logo-icon .me-3 aria-label="HomePage"
alt="HomePage"}

![](data:image/svg+xml;base64,PHN2ZyBpZD0iTGF5ZXJfMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Ym94PSIwIDAgMjUwIDI1MCI+PHBhdGggZD0ibTE0MyAzOS41Yy0xOCAwLTE4IDE4LTE4IDE4czAtMTgtMTgtMThIMjJjLTIuNzYuMC01IDIuMjQtNSA1djE0M2MwIDIuNzYgMi4yNCA1IDUgNWg3NmM3LjIuMCA4LjY0IDExLjUyIDguOTMgMTYuMTMuMDcgMS4wNS45NSAxLjg3IDIgMS44N2gzMi4xNGMxLjA2LjAgMS45NC0uODIgMi0xLjg3LjI5LTQuNjEgMS43My0xNi4xMyA4LjkzLTE2LjEzaDc2YzIuNzYuMCA1LTIuMjQgNS01VjQ0LjVjMC0yLjc2LTIuMjQtNS01LTVoLTg1ek0yMDYgMTYzYzAgMS4zOC0xLjEyIDIuNS0yLjUgMi41SDE0M2MtMTggMC0xOCAxOC0xOCAxOHMwLTE4LTE4LTE4SDQ2LjVjLTEuMzguMC0yLjUtMS4xMi0yLjUtMi41VjY5YzAtMS4zOCAxLjEyLTIuNSAyLjUtMi41SDk4YzcuMi4wIDguNjQgMTEuNTIgOC45MyAxNi4xMy4wNyAxLjA1Ljk1IDEuODcgMiAxLjg3aDMyLjE0YzEuMDYuMCAxLjk0LS44MiAyLTEuODcuMjktNC42MSAxLjczLTE2LjEzIDguOTMtMTYuMTNoNTEuNWMxLjM4LjAgMi41IDEuMTIgMi41IDIuNXY5NHoiIHN0eWxlPSJmaWxsOiMwNmYiIC8+PC9zdmc+){#Layer_1}

![](data:image/svg+xml;base64,PHN2ZyBpZD0iTGF5ZXJfMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Ym94PSIwIDAgMjUwIDI1MCI+PHBhdGggZD0ibTE0MyAzOS41Yy0xOCAwLTE4IDE4LTE4IDE4czAtMTgtMTgtMThIMjJjLTIuNzYuMC01IDIuMjQtNSA1djE0M2MwIDIuNzYgMi4yNCA1IDUgNWg3NmM3LjIuMCA4LjY0IDExLjUyIDguOTMgMTYuMTMuMDcgMS4wNS45NSAxLjg3IDIgMS44N2gzMi4xNGMxLjA2LjAgMS45NC0uODIgMi0xLjg3LjI5LTQuNjEgMS43My0xNi4xMyA4LjkzLTE2LjEzaDc2YzIuNzYuMCA1LTIuMjQgNS01VjQ0LjVjMC0yLjc2LTIuMjQtNS01LTVoLTg1ek0yMDYgMTYzYzAgMS4zOC0xLjEyIDIuNS0yLjUgMi41SDE0M2MtMTggMC0xOCAxOC0xOCAxOHMwLTE4LTE4LTE4SDQ2LjVjLTEuMzguMC0yLjUtMS4xMi0yLjUtMi41VjY5YzAtMS4zOCAxLjEyLTIuNSAyLjUtMi41SDk4YzcuMi4wIDguNjQgMTEuNTIgOC45MyAxNi4xMy4wNyAxLjA1Ljk1IDEuODcgMiAxLjg3aDMyLjE0YzEuMDYuMCAxLjk0LS44MiAyLTEuODcuMjktNC42MSAxLjczLTE2LjEzIDguOTMtMTYuMTNoNTEuNWMxLjM4LjAgMi41IDEuMTIgMi41IDIuNXY5NHoiIHN0eWxlPSJmaWxsOiMwNmYiIC8+PC9zdmc+){#Layer_1}

[menu]{.material-icons .size-20 .menu-icon .align-middle}

[search]{.material-icons .size-20 .menu-icon .align-middle}
[Search]{.flexsearch-button-placeholder .ms-1 .me-2 .d-none .d-sm-block}

[[![](data:image/svg+xml;base64,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)]{.kbd
.flexsearch-button-cmd-key}[![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTUiIGhlaWdodD0iMTUiPjxwYXRoIGQ9Ik01LjkyNiAxMi4yNzlINC40MUw5LjA3MyAyLjcyMUgxMC41OXoiIGZpbGw9ImN1cnJlbnRjb2xvciIgLz48L3N2Zz4=)]{.kbd
.flexsearch-button-key}]{.flexsearch-button-keys}

##### Star us on GitHub !¬†

[Star](https://github.com/mudler/LocalAI){.github-button
color-scheme="no-preference: light; light: light; dark: dark;"
icon="octicon-star" data-size="large" show-count="true"
aria-label="Star mudler/LocalAI on GitHub"}

-   [](https://github.com/mudler/LocalAI){alt="github"
    rel="noopener noreferrer" target="_blank"}

    ![](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld2JveD0iMCAwIDI0IDI0IiBmaWxsPSJub25lIiBzdHJva2U9ImN1cnJlbnRjb2xvciIgc3Ryb2tlLXdpZHRoPSIyIiBzdHJva2UtbGluZWNhcD0icm91bmQiIHN0cm9rZS1saW5lam9pbj0icm91bmQiPjx0aXRsZT5HaXRIdWI8L3RpdGxlPjxwYXRoIGQ9Ik05IDE5Yy01IDEuNS01LTIuNS03LTNtMTQgNnYtMy44N2EzLjM3IDMuMzcuMCAwMC0uOTQtMi42MWMzLjE0LS4zNSA2LjQ0LTEuNTQgNi40NC03QTUuNDQgNS40NC4wIDAwMjAgNC43NyA1LjA3IDUuMDcuMCAwMDE5LjkxIDFTMTguNzMuNjUgMTYgMi40OGExMy4zOCAxMy4zOC4wIDAwLTcgMEM2LjI3LjY1IDUuMDkgMSA1LjA5IDFBNS4wNyA1LjA3LjAgMDA1IDQuNzcgNS40NCA1LjQ0LjAgMDAzLjUgOC41NWMwIDUuNDIgMy4zIDYuNjEgNi40NCA3QTMuMzcgMy4zNy4wIDAwOSAxOC4xM1YyMiIgLz48L3N2Zz4=)
-   [](https://twitter.com/LocalAI_API){alt="twitter"
    rel="noopener noreferrer" target="_blank"}

    ![](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld2JveD0iMCAwIDI0IDI0Ij48dGl0bGU+VHdpdHRlciAvIFg8L3RpdGxlPjxwYXRoIGQ9Ik0uMDg4Ljc2OGw5LjI2NiAxMi4zOUwuMDI5IDIzLjIzMWgyLjFsOC4xNjMtOC44MTkgNi42IDguODE5aDcuMTQyTDE0LjI0MiAxMC4xNDUgMjIuOTIxLjc2OGgtMi4xTDEzLjMgOC44OTEgNy4yMjkuNzY4ek0zLjE3NCAyLjMxNEg2LjQ1NUwyMC45NDIgMjEuNjg1aC0zLjI4eiIgZmlsbD0iY3VycmVudGNvbG9yIiAvPjwvc3ZnPg==)
-   [](../../index.xml){alt="rss" rel="noopener noreferrer"
    target="_blank"}

    ![](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld2JveD0iMCAwIDI0IDI0IiBmaWxsPSJub25lIiBzdHJva2U9ImN1cnJlbnRjb2xvciIgc3Ryb2tlLXdpZHRoPSIyIiBzdHJva2UtbGluZWNhcD0icm91bmQiIHN0cm9rZS1saW5lam9pbj0icm91bmQiPjx0aXRsZT5SU1M8L3RpdGxlPjxwYXRoIGQ9Ik00IDExYTkgOSAwIDAxOSA5IiAvPjxwYXRoIGQ9Ik00IDRhMTYgMTYgMCAwMTE2IDE2IiAvPjxjaXJjbGUgY3g9IjUiIGN5PSIxOSIgcj0iMSI+PC9jaXJjbGU+PC9zdmc+)

[![](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMzAiIHdpZHRoPSIzMCIgdmlld2JveD0iMCAwIDQ4IDQ4IiBmaWxsPSJjdXJyZW50Y29sb3IiPjx0aXRsZT5FbmFibGUgZGFyayBtb2RlPC90aXRsZT48cGF0aCBkPSJNMjQgNDJxLTcuNS4wLTEyLjc1LTUuMjVUNiAyNHQ1LjI1LTEyLjc1VDI0IDZxLjQuMC44NS4wMjUuNDUuMDI1IDEuMTUuMDc1LTEuOCAxLjYtMi44IDMuOTV0LTEgNC45NXEwIDQuNSAzLjE1IDcuNjVRMjguNSAyNS44IDMzIDI1LjhxMi42LjAgNC45NS0uOTI1VDQxLjkgMjIuM3EuMDUuNi4wNzUuOTc1UTQyIDIzLjY1IDQyIDI0cTAgNy41LTUuMjUgMTIuNzVUMjQgNDJ6bTAtM3E1LjQ1LjAgOS41LTMuMzc1dDUuMDUtNy45MjVxLTEuMjUuNTUtMi42NzUuODI1UTM0LjQ1IDI4LjggMzMgMjguOHEtNS43NS4wLTkuNzc1LTQuMDI1VDE5LjIgMTVxMC0xLjIuMjUtMi41NzV0LjktMy4xMjVxLTQuOSAxLjM1LTguMTI1IDUuNDc1UTkgMTguOSA5IDI0cTAgNi4yNSA0LjM3NSAxMC42MjVUMjQgMzl6bS0uMi0xNC44NXoiIC8+PC9zdmc+)]{.toggle-dark}[![](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMzAiIHdpZHRoPSIzMCIgdmlld2JveD0iMCAwIDQ4IDQ4IiBmaWxsPSJjdXJyZW50Y29sb3IiPjx0aXRsZT5FbmFibGUgbGlnaHQgbW9kZTwvdGl0bGU+PHBhdGggZD0iTTI0IDMxcTIuOS4wIDQuOTUtMi4wNVQzMSAyNHQtMi4wNS00Ljk1VDI0IDE3dC00Ljk1IDIuMDVUMTcgMjR0Mi4wNSA0Ljk1VDI0IDMxem0wIDNxLTQuMTUuMC03LjA3NS0yLjkyNVQxNCAyNHQyLjkyNS03LjA3NVQyNCAxNHQ3LjA3NSAyLjkyNVQzNCAyNHQtMi45MjUgNy4wNzVUMjQgMzR6TTMuNSAyNS41cS0uNjUuMC0xLjA3NS0uNDI1UTIgMjQuNjUgMiAyNHQuNDI1LTEuMDc1UTIuODUgMjIuNSAzLjUgMjIuNWg1cS42NS4wIDEuMDc1LjQyNVExMCAyMy4zNSAxMCAyNHQtLjQyNSAxLjA3NVQ4LjUgMjUuNXptMzYgMHEtLjY1LjAtMS4wNzUtLjQyNVEzOCAyNC42NSAzOCAyNHQuNDI1LTEuMDc1VDM5LjUgMjIuNWg1cS42NS4wIDEuMDc1LjQyNVE0NiAyMy4zNSA0NiAyNHQtLjQyNSAxLjA3NS0xLjA3NS40MjV6TTI0IDEwcS0uNjUuMC0xLjA3NS0uNDI1UTIyLjUgOS4xNSAyMi41IDguNXYtNXEwLS42NS40MjUtMS4wNzVRMjMuMzUgMiAyNCAydDEuMDc1LjQyNVQyNS41IDMuNXY1cTAgLjY1LS40MjUgMS4wNzVRMjQuNjUgMTAgMjQgMTB6bTAgMzZxLS42NS4wLTEuMDc1LS40MjVUMjIuNSA0NC41di01cTAtLjY1LjQyNS0xLjA3NVEyMy4zNSAzOCAyNCAzOHQxLjA3NS40MjUuNDI1IDEuMDc1djVxMCAuNjUtLjQyNSAxLjA3NVEyNC42NSA0NiAyNCA0NnpNMTIgMTQuMWwtMi44NS0yLjhxLS40NS0uNDUtLjQyNS0xLjA3NS4wMjUtLjYyNS40MjUtMS4wNzUuNDUtLjQ1IDEuMDc1LS40NXQxLjA3NS40NUwxNC4xIDEycS40LjQ1LjQgMS4wNS4wLjYtLjQgMS0uNC40NS0xLjAyNS40NVQxMiAxNC4xem0yNC43IDI0Ljc1TDMzLjkgMzZxLS40LS40NS0uNC0xLjA3NXQuNDUtMS4wMjVxLjQtLjQ1IDEtLjQ1dDEuMDUuNDVsMi44NSAyLjhxLjQ1LjQ1LjQyNSAxLjA3NS0uMDI1LjYyNS0uNDI1IDEuMDc1LS40NS40NS0xLjA3NS40NXQtMS4wNzUtLjQ1ek0zMy45IDE0LjFxLS40NS0uNDUtLjQ1LTEuMDUuMC0uNi40NS0xLjA1bDIuOC0yLjg1cS40NS0uNDUgMS4wNzUtLjQyNS42MjUuMDI1IDEuMDc1LjQyNS40NS40NS40NSAxLjA3NXQtLjQ1IDEuMDc1TDM2IDE0LjFxLS40LjQtMS4wMjUuNHQtMS4wNzUtLjR6TTkuMTUgMzguODVxLS40NS0uNDUtLjQ1LTEuMDc1dC40NS0xLjA3NUwxMiAzMy45cS40NS0uNDUgMS4wNS0uNDUuNi4wIDEuMDUuNDUuNDUuNDUuNDUgMS4wNS4wLjYtLjQ1IDEuMDVsLTIuOCAyLjg1cS0uNDUuNDUtMS4wNzUuNDI1LS42MjUtLjAyNS0xLjA3NS0uNDI1ek0yNCAyNHoiIC8+PC9zdmc+)]{.toggle-light}

[![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTUiIGhlaWdodD0iMTUiIGFyaWEtbGFiZWw9IkFycm93IGRvd24iIHJvbGU9ImltZyI+PGcgZmlsbD0ibm9uZSIgc3Ryb2tlPSJjdXJyZW50Y29sb3IiIHN0cm9rZS1saW5lY2FwPSJyb3VuZCIgc3Ryb2tlLWxpbmVqb2luPSJyb3VuZCIgc3Ryb2tlLXdpZHRoPSIxLjIiPjxwYXRoIGQ9Ik03LjUgMy41djhtMy0zLTMgMy0zLTMiIC8+PC9nPjwvc3ZnPg==)]{.kbd
.flexsearch-button-cmd-key}[![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTUiIGhlaWdodD0iMTUiIGFyaWEtbGFiZWw9IkFycm93IHVwIiByb2xlPSJpbWciPjxnIGZpbGw9Im5vbmUiIHN0cm9rZT0iY3VycmVudGNvbG9yIiBzdHJva2UtbGluZWNhcD0icm91bmQiIHN0cm9rZS1saW5lam9pbj0icm91bmQiIHN0cm9rZS13aWR0aD0iMS4yIj48cGF0aCBkPSJNNy41IDExLjV2LThtMyAzLTMtMy0zIDMiIC8+PC9nPjwvc3ZnPg==)]{.kbd
.flexsearch-button-cmd-key}[to navigate]{.flexsearch-key-label}

[![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTUiIGhlaWdodD0iMTUiIGFyaWEtbGFiZWw9IkVudGVyIGtleSIgcm9sZT0iaW1nIj48ZyBmaWxsPSJub25lIiBzdHJva2U9ImN1cnJlbnRjb2xvciIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIiBzdHJva2UtbGluZWpvaW49InJvdW5kIiBzdHJva2Utd2lkdGg9IjEuMiI+PHBhdGggZD0iTTEyIDMuNTMwODh2M2MwIDEtMSAyLTIgMkg0bTMgMy0zLTMgMy0zIiAvPjwvZz48L3N2Zz4=)]{.kbd
.flexsearch-button-cmd-key}[to select]{.flexsearch-key-label}

[![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTUiIGhlaWdodD0iMTUiIGFyaWEtbGFiZWw9IkVzY2FwZSBrZXkiIHJvbGU9ImltZyI+PGcgZmlsbD0ibm9uZSIgc3Ryb2tlPSJjdXJyZW50Y29sb3IiIHN0cm9rZS1saW5lY2FwPSJyb3VuZCIgc3Ryb2tlLWxpbmVqb2luPSJyb3VuZCIgc3Ryb2tlLXdpZHRoPSIxLjIiPjxwYXRoIGQ9Ik0xMy42MTY3IDguOTM2Yy0uMTA2NS4zNTgzLS42ODgzLjk2Mi0xLjQ4NzUuOTYyLS43OTkzLjAtMS42NTMtLjkxNjUtMS42NTMtMi4xMjU4di0uNTY3OGMwLTEuMjU0OC43ODk2LTIuMTAxNiAxLjY1My0yLjEwMTZzMS4zNjAxLjQ3NzggMS40ODc1IDEuMDcyNE05IDZjLS4xMzUyLS40NzM1LS43NTA2LS45MjE5LTEuNDYtLjg5NzItLjcwOTIuMDI0Ni0xLjM0NC41Ny0xLjM0NCAxLjIxNjZzLjQxOTguODgxMiAxLjM0NDUuOTgwNUM4LjQ2NSA3LjM5OTIgOC45NjggNy45MzM3IDkgOC41cy0uNDU0IDEuMzk4LTEuNDU5NSAxLjM5OEM2LjY1OTMgOS44OTggNiA5IDUuOTYzIDguNDg1MW0tMS40NzQ4LjUzNjhjLS4yNjM1LjU5NDEtLjgwOTkuODc2LTEuNTQ0My44NzZzLTEuNzA3My0uNjI0OC0xLjcwNzMtMi4yMDR2LS40NjAzYzAtMS4wNDE2LjcyMS0yLjEzMSAxLjcwNzMtMi4xMzEuOTg2NC4wIDEuNjQyNSAxLjAzMSAxLjU0NDMgMi4yNDkyaC0yLjk1NiIgLz48L2c+PC9zdmc+)]{.kbd
.flexsearch-button-cmd-key}[to close]{.flexsearch-key-label}

cancel


-   [*Home*](../../docs){itemprop="item"}
-   [[Features]{itemprop="name"}](../index.html){itemprop="item"}
-   [üìñ Text generation (GPT)]{itemprop="name"}

On this page

-   -   [API Reference](index.html#api-reference)
        -   [Chat completions](index.html#chat-completions)
        -   [Edit completions](index.html#edit-completions)
        -   [Completions](index.html#completions)
        -   [List models](index.html#list-models)
    -   [Backends](index.html#backends)
        -   [AutoGPTQ](index.html#autogptq)
        -   [RWKV](index.html#rwkv)
        -   [llama.cpp](index.html#llamacpp)
        -   [exllama/2](index.html#exllama2)
        -   [vLLM](index.html#vllm)
        -   [Transformers](index.html#transformers)

Table of Contents

-   -   [API Reference](index.html#api-reference)
        -   [Chat completions](index.html#chat-completions)
        -   [Edit completions](index.html#edit-completions)
        -   [Completions](index.html#completions)
        -   [List models](index.html#list-models)
    -   [Backends](index.html#backends)
        -   [AutoGPTQ](index.html#autogptq)
        -   [RWKV](index.html#rwkv)
        -   [llama.cpp](index.html#llamacpp)
        -   [exllama/2](index.html#exllama2)
        -   [vLLM](index.html#vllm)
        -   [Transformers](index.html#transformers)

*article*

# üìñ Text generation (GPT) {#text-generation-gpt .content-title .mb-0}

LocalAI supports generating text with GPT with `llama.cpp` and other
backends (such as `rwkv.cpp` as ) see also the [Model
compatibility](../../model-compatibility/index.html) for an up-to-date
list of the supported model families.

Note:

-   You can also specify the model name as part of the OpenAI token.
-   If only one model is available, the API will use it for all the
    requests.

## API Reference [*link*](index.html#api-reference){.anchor aria-hidden="true"} {#api-reference}

### Chat completions [*link*](index.html#chat-completions){.anchor aria-hidden="true"} {#chat-completions}

[https://platform.openai.com/docs/api-reference/chat![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIGhlaWdodD0iMTYiIHZpZXdib3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBmaWxsPSJjdXJyZW50Y29sb3IiIGQ9Ik0xNCA1Yy0uNTUyLjAtMS0uNDQ4LTEtMXMuNDQ4LTEgMS0xaDZjLjU1Mi4wIDEgLjQ0OCAxIDF2NmMwIC41NTItLjQ0OCAxLTEgMXMtMS0uNDQ4LTEtMVY2LjQxNGwtNy4yOTMgNy4yOTNjLS4zOTEuMzktMS4wMjQuMzktMS40MTQuMC0uMzkxLS4zOTEtLjM5MS0xLjAyNC4wLTEuNDE0TDE3LjU4NiA1SDE0ek01IDdjLS41NTIuMC0xIC40NDgtMSAxdjExYzAgLjU1Mi40NDggMSAxIDFoMTFjLjU1Mi4wIDEtLjQ0OCAxLTF2LTQuNTYzYzAtLjU1Mi40NDgtMSAxLTFzMSAuNDQ4IDEgMVYxOWMwIDEuNjU3LTEuMzQzIDMtMyAzSDVjLTEuNjU3LjAtMy0xLjM0My0zLTNWOGMwLTEuNjU3IDEuMzQzLTMgMy0zaDQuNTYzYy41NTIuMCAxIC40NDggMSAxcy0uNDQ4IDEtMSAxSDV6IiAvPjwvc3ZnPg==)](https://platform.openai.com/docs/api-reference/chat){rel="external"
target="_blank"}

For example, to generate a chat completion, you can send a POST request
to the `/v1/chat/completions` endpoint with the instruction as the
request body:

``` {#40d8b64 .language-bash}
  curl http://localhost:8080/v1/chat/completions -H "Content-Type: application/json" -d '{
  "model": "ggml-koala-7b-model-q4_0-r2.bin",
  "messages": [{"role": "user", "content": "Say this is a test!"}],
  "temperature": 0.7
}'
  
```

Available additional parameters: `top_p`, `top_k`, `max_tokens`

### Edit completions [*link*](index.html#edit-completions){.anchor aria-hidden="true"} {#edit-completions}

[https://platform.openai.com/docs/api-reference/edits![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIGhlaWdodD0iMTYiIHZpZXdib3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBmaWxsPSJjdXJyZW50Y29sb3IiIGQ9Ik0xNCA1Yy0uNTUyLjAtMS0uNDQ4LTEtMXMuNDQ4LTEgMS0xaDZjLjU1Mi4wIDEgLjQ0OCAxIDF2NmMwIC41NTItLjQ0OCAxLTEgMXMtMS0uNDQ4LTEtMVY2LjQxNGwtNy4yOTMgNy4yOTNjLS4zOTEuMzktMS4wMjQuMzktMS40MTQuMC0uMzkxLS4zOTEtLjM5MS0xLjAyNC4wLTEuNDE0TDE3LjU4NiA1SDE0ek01IDdjLS41NTIuMC0xIC40NDgtMSAxdjExYzAgLjU1Mi40NDggMSAxIDFoMTFjLjU1Mi4wIDEtLjQ0OCAxLTF2LTQuNTYzYzAtLjU1Mi40NDgtMSAxLTFzMSAuNDQ4IDEgMVYxOWMwIDEuNjU3LTEuMzQzIDMtMyAzSDVjLTEuNjU3LjAtMy0xLjM0My0zLTNWOGMwLTEuNjU3IDEuMzQzLTMgMy0zaDQuNTYzYy41NTIuMCAxIC40NDggMSAxcy0uNDQ4IDEtMSAxSDV6IiAvPjwvc3ZnPg==)](https://platform.openai.com/docs/api-reference/edits){rel="external"
target="_blank"}

To generate an edit completion you can send a POST request to the
`/v1/edits` endpoint with the instruction as the request body:

``` {#d5c656b .language-bash}
  curl http://localhost:8080/v1/edits -H "Content-Type: application/json" -d '{
  "model": "ggml-koala-7b-model-q4_0-r2.bin",
  "instruction": "rephrase",
  "input": "Black cat jumped out of the window",
  "temperature": 0.7
}'
  
```

Available additional parameters: `top_p`, `top_k`, `max_tokens`.

### Completions [*link*](index.html#completions){.anchor aria-hidden="true"} {#completions}

[https://platform.openai.com/docs/api-reference/completions![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIGhlaWdodD0iMTYiIHZpZXdib3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBmaWxsPSJjdXJyZW50Y29sb3IiIGQ9Ik0xNCA1Yy0uNTUyLjAtMS0uNDQ4LTEtMXMuNDQ4LTEgMS0xaDZjLjU1Mi4wIDEgLjQ0OCAxIDF2NmMwIC41NTItLjQ0OCAxLTEgMXMtMS0uNDQ4LTEtMVY2LjQxNGwtNy4yOTMgNy4yOTNjLS4zOTEuMzktMS4wMjQuMzktMS40MTQuMC0uMzkxLS4zOTEtLjM5MS0xLjAyNC4wLTEuNDE0TDE3LjU4NiA1SDE0ek01IDdjLS41NTIuMC0xIC40NDgtMSAxdjExYzAgLjU1Mi40NDggMSAxIDFoMTFjLjU1Mi4wIDEtLjQ0OCAxLTF2LTQuNTYzYzAtLjU1Mi40NDgtMSAxLTFzMSAuNDQ4IDEgMVYxOWMwIDEuNjU3LTEuMzQzIDMtMyAzSDVjLTEuNjU3LjAtMy0xLjM0My0zLTNWOGMwLTEuNjU3IDEuMzQzLTMgMy0zaDQuNTYzYy41NTIuMCAxIC40NDggMSAxcy0uNDQ4IDEtMSAxSDV6IiAvPjwvc3ZnPg==)](https://platform.openai.com/docs/api-reference/completions){rel="external"
target="_blank"}

To generate a completion, you can send a POST request to the
`/v1/completions` endpoint with the instruction as per the request body:

``` {#aeebd5f .language-bash}
  curl http://localhost:8080/v1/completions -H "Content-Type: application/json" -d '{
  "model": "ggml-koala-7b-model-q4_0-r2.bin",
  "prompt": "A long time ago in a galaxy far, far away",
  "temperature": 0.7
}'
  
```

Available additional parameters: `top_p`, `top_k`, `max_tokens`

### List models [*link*](index.html#list-models){.anchor aria-hidden="true"} {#list-models}

You can list all the models available with:

``` {#5e0c208 .language-bash}
  curl http://localhost:8080/v1/models
  
```

## Backends [*link*](index.html#backends){.anchor aria-hidden="true"} {#backends}

### AutoGPTQ [*link*](index.html#autogptq){.anchor aria-hidden="true"} {#autogptq}

[AutoGPTQ![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIGhlaWdodD0iMTYiIHZpZXdib3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBmaWxsPSJjdXJyZW50Y29sb3IiIGQ9Ik0xNCA1Yy0uNTUyLjAtMS0uNDQ4LTEtMXMuNDQ4LTEgMS0xaDZjLjU1Mi4wIDEgLjQ0OCAxIDF2NmMwIC41NTItLjQ0OCAxLTEgMXMtMS0uNDQ4LTEtMVY2LjQxNGwtNy4yOTMgNy4yOTNjLS4zOTEuMzktMS4wMjQuMzktMS40MTQuMC0uMzkxLS4zOTEtLjM5MS0xLjAyNC4wLTEuNDE0TDE3LjU4NiA1SDE0ek01IDdjLS41NTIuMC0xIC40NDgtMSAxdjExYzAgLjU1Mi40NDggMSAxIDFoMTFjLjU1Mi4wIDEtLjQ0OCAxLTF2LTQuNTYzYzAtLjU1Mi40NDgtMSAxLTFzMSAuNDQ4IDEgMVYxOWMwIDEuNjU3LTEuMzQzIDMtMyAzSDVjLTEuNjU3LjAtMy0xLjM0My0zLTNWOGMwLTEuNjU3IDEuMzQzLTMgMy0zaDQuNTYzYy41NTIuMCAxIC40NDggMSAxcy0uNDQ4IDEtMSAxSDV6IiAvPjwvc3ZnPg==)](https://github.com/PanQiWei/AutoGPTQ){rel="external"
target="_blank"} is an easy-to-use LLMs quantization package with
user-friendly apis, based on GPTQ algorithm.

#### Prerequisites [*link*](index.html#prerequisites){.anchor aria-hidden="true"} {#prerequisites}

This is an extra backend - in the container images is already available
and there is nothing to do for the setup.

If you are building LocalAI locally, you need to install [AutoGPTQ
manually![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIGhlaWdodD0iMTYiIHZpZXdib3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBmaWxsPSJjdXJyZW50Y29sb3IiIGQ9Ik0xNCA1Yy0uNTUyLjAtMS0uNDQ4LTEtMXMuNDQ4LTEgMS0xaDZjLjU1Mi4wIDEgLjQ0OCAxIDF2NmMwIC41NTItLjQ0OCAxLTEgMXMtMS0uNDQ4LTEtMVY2LjQxNGwtNy4yOTMgNy4yOTNjLS4zOTEuMzktMS4wMjQuMzktMS40MTQuMC0uMzkxLS4zOTEtLjM5MS0xLjAyNC4wLTEuNDE0TDE3LjU4NiA1SDE0ek01IDdjLS41NTIuMC0xIC40NDgtMSAxdjExYzAgLjU1Mi40NDggMSAxIDFoMTFjLjU1Mi4wIDEtLjQ0OCAxLTF2LTQuNTYzYzAtLjU1Mi40NDgtMSAxLTFzMSAuNDQ4IDEgMVYxOWMwIDEuNjU3LTEuMzQzIDMtMyAzSDVjLTEuNjU3LjAtMy0xLjM0My0zLTNWOGMwLTEuNjU3IDEuMzQzLTMgMy0zaDQuNTYzYy41NTIuMCAxIC40NDggMSAxcy0uNDQ4IDEtMSAxSDV6IiAvPjwvc3ZnPg==)](https://github.com/PanQiWei/AutoGPTQ#quick-installation){rel="external"
target="_blank"}.

#### Model setup [*link*](index.html#model-setup){.anchor aria-hidden="true"} {#model-setup}

The models are automatically downloaded from `huggingface` if not
present the first time. It is possible to define models via `YAML`
config file, or just by querying the endpoint with the `huggingface`
repository model name. For example, create a `YAML` config file in
`models/`:

``` {#914ae62 .language-}
  name: orca
backend: autogptq
model_base_name: "orca_mini_v2_13b-GPTQ-4bit-128g.no-act.order"
parameters:
  model: "TheBloke/orca_mini_v2_13b-GPTQ"
# ...
  
```

Test with:

``` {#d8c3094 .language-bash}
  curl http://localhost:8080/v1/chat/completions -H "Content-Type: application/json" -d '{                                                                                                         
   "model": "orca",
   "messages": [{"role": "user", "content": "How are you?"}],
   "temperature": 0.1
 }'
  
```

### RWKV [*link*](index.html#rwkv){.anchor aria-hidden="true"} {#rwkv}

A full example on how to run a rwkv model is in the
[examples![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIGhlaWdodD0iMTYiIHZpZXdib3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBmaWxsPSJjdXJyZW50Y29sb3IiIGQ9Ik0xNCA1Yy0uNTUyLjAtMS0uNDQ4LTEtMXMuNDQ4LTEgMS0xaDZjLjU1Mi4wIDEgLjQ0OCAxIDF2NmMwIC41NTItLjQ0OCAxLTEgMXMtMS0uNDQ4LTEtMVY2LjQxNGwtNy4yOTMgNy4yOTNjLS4zOTEuMzktMS4wMjQuMzktMS40MTQuMC0uMzkxLS4zOTEtLjM5MS0xLjAyNC4wLTEuNDE0TDE3LjU4NiA1SDE0ek01IDdjLS41NTIuMC0xIC40NDgtMSAxdjExYzAgLjU1Mi40NDggMSAxIDFoMTFjLjU1Mi4wIDEtLjQ0OCAxLTF2LTQuNTYzYzAtLjU1Mi40NDgtMSAxLTFzMSAuNDQ4IDEgMVYxOWMwIDEuNjU3LTEuMzQzIDMtMyAzSDVjLTEuNjU3LjAtMy0xLjM0My0zLTNWOGMwLTEuNjU3IDEuMzQzLTMgMy0zaDQuNTYzYy41NTIuMCAxIC40NDggMSAxcy0uNDQ4IDEtMSAxSDV6IiAvPjwvc3ZnPg==)](https://github.com/go-skynet/LocalAI/tree/master/examples/rwkv){rel="external"
target="_blank"}.

Note: rwkv models needs to specify the backend `rwkv` in the YAML config
files and have an associated tokenizer along that needs to be provided
with it:

``` {#610a2d3 .language-}
  36464540 -rw-r--r--  1 mudler mudler 1.2G May  3 10:51 rwkv_small
36464543 -rw-r--r--  1 mudler mudler 2.4M May  3 10:51 rwkv_small.tokenizer.json
  
```

### llama.cpp [*link*](index.html#llamacpp){.anchor aria-hidden="true"} {#llamacpp}

[llama.cpp![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIGhlaWdodD0iMTYiIHZpZXdib3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBmaWxsPSJjdXJyZW50Y29sb3IiIGQ9Ik0xNCA1Yy0uNTUyLjAtMS0uNDQ4LTEtMXMuNDQ4LTEgMS0xaDZjLjU1Mi4wIDEgLjQ0OCAxIDF2NmMwIC41NTItLjQ0OCAxLTEgMXMtMS0uNDQ4LTEtMVY2LjQxNGwtNy4yOTMgNy4yOTNjLS4zOTEuMzktMS4wMjQuMzktMS40MTQuMC0uMzkxLS4zOTEtLjM5MS0xLjAyNC4wLTEuNDE0TDE3LjU4NiA1SDE0ek01IDdjLS41NTIuMC0xIC40NDgtMSAxdjExYzAgLjU1Mi40NDggMSAxIDFoMTFjLjU1Mi4wIDEtLjQ0OCAxLTF2LTQuNTYzYzAtLjU1Mi40NDgtMSAxLTFzMSAuNDQ4IDEgMVYxOWMwIDEuNjU3LTEuMzQzIDMtMyAzSDVjLTEuNjU3LjAtMy0xLjM0My0zLTNWOGMwLTEuNjU3IDEuMzQzLTMgMy0zaDQuNTYzYy41NTIuMCAxIC40NDggMSAxcy0uNDQ4IDEtMSAxSDV6IiAvPjwvc3ZnPg==)](https://github.com/ggerganov/llama.cpp){rel="external"
target="_blank"} is a popular port of Facebook's LLaMA model in C/C++.

[notifications]{.material-icons .size-20 .me-2}

The `ggml` file format has been deprecated. If you are using `ggml`
models and you are configuring your model with a YAML file, specify, use
a LocalAI version older than v2.25.0. For `gguf` models, use the `llama`
backend. The go backend is deprecated as well but still available as
`go-llama`.

#### Features [*link*](index.html#features){.anchor aria-hidden="true"} {#features}

The `llama.cpp` model supports the following features:

-   [üìñ Text generation (GPT)](index.html)
-   [üß† Embeddings](../embeddings/index.html)
-   [üî• OpenAI functions](../openai-functions/index.html)
-   [‚úçÔ∏è Constrained grammars](../constrained_grammars/index.html)

#### Setup [*link*](index.html#setup){.anchor aria-hidden="true"} {#setup}

LocalAI supports `llama.cpp` models out of the box. You can use the
`llama.cpp` model in the same way as any other model.

##### Manual setup [*link*](index.html#manual-setup){.anchor aria-hidden="true"} {#manual-setup}

It is sufficient to copy the `ggml` or `gguf` model files in the
`models` folder. You can refer to the model in the `model` parameter in
the API calls.

[You can optionally create an associated
YAML](https://localai.io/docs/advanced/){bs-delay="{\"hide\":300,\"show\":550}"
bs-html="true"
bs-title="<a href='/docs/advanced/'><p>DOCS</p><strong>Advanced</strong><br>Advanced usage</a>"
bs-toggle="tooltip"} model config file to tune the model's parameters or
apply a template to the prompt.

Prompt templates are useful for models that are fine-tuned towards a
specific prompt.

##### Automatic setup [*link*](index.html#automatic-setup){.anchor aria-hidden="true"} {#automatic-setup}

LocalAI supports model galleries which are indexes of models. For
instance, the huggingface gallery contains a large curated index of
models from the huggingface model hub for `ggml` or `gguf` models.

For instance, if you have the galleries enabled and LocalAI already
running, you can just start chatting with models in huggingface by
running:

``` {#b8df835 .language-bash}
  curl http://localhost:8080/v1/chat/completions -H "Content-Type: application/json" -d '{
     "model": "TheBloke/WizardLM-13B-V1.2-GGML/wizardlm-13b-v1.2.ggmlv3.q2_K.bin",
     "messages": [{"role": "user", "content": "Say this is a test!"}],
     "temperature": 0.1
   }'
  
```

LocalAI will automatically download and configure the model in the
`model` directory.

Models can be also preloaded or downloaded on demand. To learn about
model galleries, check out the [model gallery
documentation](../../models/index.html).

#### YAML configuration [*link*](index.html#yaml-configuration){.anchor aria-hidden="true"} {#yaml-configuration}

To use the `llama.cpp` backend, specify `llama` as the backend in the
YAML file:

``` {#7980a2f .language-yaml}
  name: llama
backend: llama
parameters:
  # Relative to the models path
  model: file.gguf
  
```

#### Reference [*link*](index.html#reference){.anchor aria-hidden="true"} {#reference}

-   [llama![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIGhlaWdodD0iMTYiIHZpZXdib3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBmaWxsPSJjdXJyZW50Y29sb3IiIGQ9Ik0xNCA1Yy0uNTUyLjAtMS0uNDQ4LTEtMXMuNDQ4LTEgMS0xaDZjLjU1Mi4wIDEgLjQ0OCAxIDF2NmMwIC41NTItLjQ0OCAxLTEgMXMtMS0uNDQ4LTEtMVY2LjQxNGwtNy4yOTMgNy4yOTNjLS4zOTEuMzktMS4wMjQuMzktMS40MTQuMC0uMzkxLS4zOTEtLjM5MS0xLjAyNC4wLTEuNDE0TDE3LjU4NiA1SDE0ek01IDdjLS41NTIuMC0xIC40NDgtMSAxdjExYzAgLjU1Mi40NDggMSAxIDFoMTFjLjU1Mi4wIDEtLjQ0OCAxLTF2LTQuNTYzYzAtLjU1Mi40NDgtMSAxLTFzMSAuNDQ4IDEgMVYxOWMwIDEuNjU3LTEuMzQzIDMtMyAzSDVjLTEuNjU3LjAtMy0xLjM0My0zLTNWOGMwLTEuNjU3IDEuMzQzLTMgMy0zaDQuNTYzYy41NTIuMCAxIC40NDggMSAxcy0uNDQ4IDEtMSAxSDV6IiAvPjwvc3ZnPg==)](https://github.com/ggerganov/llama.cpp){rel="external"
    target="_blank"}

### exllama/2 [*link*](index.html#exllama2){.anchor aria-hidden="true"} {#exllama2}

[Exllama![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIGhlaWdodD0iMTYiIHZpZXdib3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBmaWxsPSJjdXJyZW50Y29sb3IiIGQ9Ik0xNCA1Yy0uNTUyLjAtMS0uNDQ4LTEtMXMuNDQ4LTEgMS0xaDZjLjU1Mi4wIDEgLjQ0OCAxIDF2NmMwIC41NTItLjQ0OCAxLTEgMXMtMS0uNDQ4LTEtMVY2LjQxNGwtNy4yOTMgNy4yOTNjLS4zOTEuMzktMS4wMjQuMzktMS40MTQuMC0uMzkxLS4zOTEtLjM5MS0xLjAyNC4wLTEuNDE0TDE3LjU4NiA1SDE0ek01IDdjLS41NTIuMC0xIC40NDgtMSAxdjExYzAgLjU1Mi40NDggMSAxIDFoMTFjLjU1Mi4wIDEtLjQ0OCAxLTF2LTQuNTYzYzAtLjU1Mi40NDgtMSAxLTFzMSAuNDQ4IDEgMVYxOWMwIDEuNjU3LTEuMzQzIDMtMyAzSDVjLTEuNjU3LjAtMy0xLjM0My0zLTNWOGMwLTEuNjU3IDEuMzQzLTMgMy0zaDQuNTYzYy41NTIuMCAxIC40NDggMSAxcy0uNDQ4IDEtMSAxSDV6IiAvPjwvc3ZnPg==)](https://github.com/turboderp/exllama){rel="external"
target="_blank"} is a "A more memory-efficient rewrite of the HF
transformers implementation of Llama for use with quantized weights".
Both `exllama` and `exllama2` are supported.

#### Model setup [*link*](index.html#model-setup-1){.anchor aria-hidden="true"} {#model-setup-1}

Download the model as a folder inside the `model `directory and create a
YAML file specifying the `exllama` backend. For instance with the
`TheBloke/WizardLM-7B-uncensored-GPTQ` model:

``` {#0e11758 .language-}
  $ git lfs install
$ cd models && git clone https://huggingface.co/TheBloke/WizardLM-7B-uncensored-GPTQ
$ ls models/                                                                 
.keep                        WizardLM-7B-uncensored-GPTQ/ exllama.yaml
$ cat models/exllama.yaml                                                     
name: exllama
parameters:
  model: WizardLM-7B-uncensored-GPTQ
backend: exllama
# Note: you can also specify "exllama2" if it's an exllama2 model here
# ...
  
```

Test with:

``` {#bbf1291 .language-bash}
  curl http://localhost:8080/v1/chat/completions -H "Content-Type: application/json" -d '{                                                                                                         
   "model": "exllama",
   "messages": [{"role": "user", "content": "How are you?"}],
   "temperature": 0.1
 }'
  
```

### vLLM [*link*](index.html#vllm){.anchor aria-hidden="true"} {#vllm}

[vLLM![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIGhlaWdodD0iMTYiIHZpZXdib3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBmaWxsPSJjdXJyZW50Y29sb3IiIGQ9Ik0xNCA1Yy0uNTUyLjAtMS0uNDQ4LTEtMXMuNDQ4LTEgMS0xaDZjLjU1Mi4wIDEgLjQ0OCAxIDF2NmMwIC41NTItLjQ0OCAxLTEgMXMtMS0uNDQ4LTEtMVY2LjQxNGwtNy4yOTMgNy4yOTNjLS4zOTEuMzktMS4wMjQuMzktMS40MTQuMC0uMzkxLS4zOTEtLjM5MS0xLjAyNC4wLTEuNDE0TDE3LjU4NiA1SDE0ek01IDdjLS41NTIuMC0xIC40NDgtMSAxdjExYzAgLjU1Mi40NDggMSAxIDFoMTFjLjU1Mi4wIDEtLjQ0OCAxLTF2LTQuNTYzYzAtLjU1Mi40NDgtMSAxLTFzMSAuNDQ4IDEgMVYxOWMwIDEuNjU3LTEuMzQzIDMtMyAzSDVjLTEuNjU3LjAtMy0xLjM0My0zLTNWOGMwLTEuNjU3IDEuMzQzLTMgMy0zaDQuNTYzYy41NTIuMCAxIC40NDggMSAxcy0uNDQ4IDEtMSAxSDV6IiAvPjwvc3ZnPg==)](https://github.com/vllm-project/vllm){rel="external"
target="_blank"} is a fast and easy-to-use library for LLM inference.

LocalAI has a built-in integration with vLLM, and it can be used to run
models. You can check out `vllm` performance
[here![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIGhlaWdodD0iMTYiIHZpZXdib3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBmaWxsPSJjdXJyZW50Y29sb3IiIGQ9Ik0xNCA1Yy0uNTUyLjAtMS0uNDQ4LTEtMXMuNDQ4LTEgMS0xaDZjLjU1Mi4wIDEgLjQ0OCAxIDF2NmMwIC41NTItLjQ0OCAxLTEgMXMtMS0uNDQ4LTEtMVY2LjQxNGwtNy4yOTMgNy4yOTNjLS4zOTEuMzktMS4wMjQuMzktMS40MTQuMC0uMzkxLS4zOTEtLjM5MS0xLjAyNC4wLTEuNDE0TDE3LjU4NiA1SDE0ek01IDdjLS41NTIuMC0xIC40NDgtMSAxdjExYzAgLjU1Mi40NDggMSAxIDFoMTFjLjU1Mi4wIDEtLjQ0OCAxLTF2LTQuNTYzYzAtLjU1Mi40NDgtMSAxLTFzMSAuNDQ4IDEgMVYxOWMwIDEuNjU3LTEuMzQzIDMtMyAzSDVjLTEuNjU3LjAtMy0xLjM0My0zLTNWOGMwLTEuNjU3IDEuMzQzLTMgMy0zaDQuNTYzYy41NTIuMCAxIC40NDggMSAxcy0uNDQ4IDEtMSAxSDV6IiAvPjwvc3ZnPg==)](https://github.com/vllm-project/vllm#performance){rel="external"
target="_blank"}.

#### Setup [*link*](index.html#setup-1){.anchor aria-hidden="true"} {#setup-1}

Create a YAML file for the model you want to use with `vllm`.

To setup a model, you need to just specify the model name in the YAML
config file:

``` {#16bc231 .language-yaml}
  name: vllm
backend: vllm
parameters:
    model: "facebook/opt-125m"

# Uncomment to specify a quantization method (optional)
# quantization: "awq"
# Uncomment to limit the GPU memory utilization (vLLM default is 0.9 for 90%)
# gpu_memory_utilization: 0.5
# Uncomment to trust remote code from huggingface
# trust_remote_code: true
# Uncomment to enable eager execution
# enforce_eager: true
# Uncomment to specify the size of the CPU swap space per GPU (in GiB)
# swap_space: 2
# Uncomment to specify the maximum length of a sequence (including prompt and output)
# max_model_len: 32768
# Uncomment and specify the number of Tensor divisions.
# Allows you to partition and run large models. Performance gains are limited.
# https://github.com/vllm-project/vllm/issues/1435
# tensor_parallel_size: 2
  
```

The backend will automatically download the required files in order to
run the model.

#### Usage [*link*](index.html#usage){.anchor aria-hidden="true"} {#usage}

Use the `completions` endpoint by specifying the `vllm` backend:

``` {#d23b304 .language-}
  curl http://localhost:8080/v1/completions -H "Content-Type: application/json" -d '{   
   "model": "vllm",
   "prompt": "Hello, my name is",
   "temperature": 0.1, "top_p": 0.1
 }'
  
```

### Transformers [*link*](index.html#transformers){.anchor aria-hidden="true"} {#transformers}

[Transformers![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIGhlaWdodD0iMTYiIHZpZXdib3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBmaWxsPSJjdXJyZW50Y29sb3IiIGQ9Ik0xNCA1Yy0uNTUyLjAtMS0uNDQ4LTEtMXMuNDQ4LTEgMS0xaDZjLjU1Mi4wIDEgLjQ0OCAxIDF2NmMwIC41NTItLjQ0OCAxLTEgMXMtMS0uNDQ4LTEtMVY2LjQxNGwtNy4yOTMgNy4yOTNjLS4zOTEuMzktMS4wMjQuMzktMS40MTQuMC0uMzkxLS4zOTEtLjM5MS0xLjAyNC4wLTEuNDE0TDE3LjU4NiA1SDE0ek01IDdjLS41NTIuMC0xIC40NDgtMSAxdjExYzAgLjU1Mi40NDggMSAxIDFoMTFjLjU1Mi4wIDEtLjQ0OCAxLTF2LTQuNTYzYzAtLjU1Mi40NDgtMSAxLTFzMSAuNDQ4IDEgMVYxOWMwIDEuNjU3LTEuMzQzIDMtMyAzSDVjLTEuNjU3LjAtMy0xLjM0My0zLTNWOGMwLTEuNjU3IDEuMzQzLTMgMy0zaDQuNTYzYy41NTIuMCAxIC40NDggMSAxcy0uNDQ4IDEtMSAxSDV6IiAvPjwvc3ZnPg==)](https://huggingface.co/docs/transformers/index){rel="external"
target="_blank"} is a State-of-the-art Machine Learning library for
PyTorch, TensorFlow, and JAX.

LocalAI has a built-in integration with Transformers, and it can be used
to run models.

This is an extra backend - in the container images (the `extra` images
already contains python dependencies for Transformers) is already
available and there is nothing to do for the setup.

#### Setup [*link*](index.html#setup-2){.anchor aria-hidden="true"} {#setup-2}

Create a YAML file for the model you want to use with `transformers`.

To setup a model, you need to just specify the model name in the YAML
config file:

``` {#3e37b43 .language-yaml}
  name: transformers
backend: transformers
parameters:
    model: "facebook/opt-125m"
type: AutoModelForCausalLM
quantization: bnb_4bit # One of: bnb_8bit, bnb_4bit, xpu_4bit, xpu_8bit (optional)
  
```

The backend will automatically download the required files in order to
run the model.

#### Parameters [*link*](index.html#parameters){.anchor aria-hidden="true"} {#parameters}

##### Type [*link*](index.html#type){.anchor aria-hidden="true"} {#type}

  Type                            Description
  ------------------------------- ---------------------------------------------------------------------------------------------------------------------------------------------------------------
  `AutoModelForCausalLM`          `AutoModelForCausalLM` is a model that can be used to generate sequences. Use it for NVIDIA CUDA and Intel GPU with Intel Extensions for Pytorch acceleration
  `OVModelForCausalLM`            for Intel CPU/GPU/NPU OpenVINO Text Generation models
  `OVModelForFeatureExtraction`   for Intel CPU/GPU/NPU OpenVINO Embedding acceleration
  N/A                             Defaults to `AutoModel`

-   `OVModelForCausalLM` requires OpenVINO IR [Text
    Generation![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIGhlaWdodD0iMTYiIHZpZXdib3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBmaWxsPSJjdXJyZW50Y29sb3IiIGQ9Ik0xNCA1Yy0uNTUyLjAtMS0uNDQ4LTEtMXMuNDQ4LTEgMS0xaDZjLjU1Mi4wIDEgLjQ0OCAxIDF2NmMwIC41NTItLjQ0OCAxLTEgMXMtMS0uNDQ4LTEtMVY2LjQxNGwtNy4yOTMgNy4yOTNjLS4zOTEuMzktMS4wMjQuMzktMS40MTQuMC0uMzkxLS4zOTEtLjM5MS0xLjAyNC4wLTEuNDE0TDE3LjU4NiA1SDE0ek01IDdjLS41NTIuMC0xIC40NDgtMSAxdjExYzAgLjU1Mi40NDggMSAxIDFoMTFjLjU1Mi4wIDEtLjQ0OCAxLTF2LTQuNTYzYzAtLjU1Mi40NDgtMSAxLTFzMSAuNDQ4IDEgMVYxOWMwIDEuNjU3LTEuMzQzIDMtMyAzSDVjLTEuNjU3LjAtMy0xLjM0My0zLTNWOGMwLTEuNjU3IDEuMzQzLTMgMy0zaDQuNTYzYy41NTIuMCAxIC40NDggMSAxcy0uNDQ4IDEtMSAxSDV6IiAvPjwvc3ZnPg==)](https://huggingface.co/models?library=openvino&pipeline_tag=text-generation){rel="external"
    target="_blank"} models from Hugging face
-   `OVModelForFeatureExtraction` works with any Safetensors Transformer
    [Feature
    Extraction![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIGhlaWdodD0iMTYiIHZpZXdib3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBmaWxsPSJjdXJyZW50Y29sb3IiIGQ9Ik0xNCA1Yy0uNTUyLjAtMS0uNDQ4LTEtMXMuNDQ4LTEgMS0xaDZjLjU1Mi4wIDEgLjQ0OCAxIDF2NmMwIC41NTItLjQ0OCAxLTEgMXMtMS0uNDQ4LTEtMVY2LjQxNGwtNy4yOTMgNy4yOTNjLS4zOTEuMzktMS4wMjQuMzktMS40MTQuMC0uMzkxLS4zOTEtLjM5MS0xLjAyNC4wLTEuNDE0TDE3LjU4NiA1SDE0ek01IDdjLS41NTIuMC0xIC40NDgtMSAxdjExYzAgLjU1Mi40NDggMSAxIDFoMTFjLjU1Mi4wIDEtLjQ0OCAxLTF2LTQuNTYzYzAtLjU1Mi40NDgtMSAxLTFzMSAuNDQ4IDEgMVYxOWMwIDEuNjU3LTEuMzQzIDMtMyAzSDVjLTEuNjU3LjAtMy0xLjM0My0zLTNWOGMwLTEuNjU3IDEuMzQzLTMgMy0zaDQuNTYzYy41NTIuMCAxIC40NDggMSAxcy0uNDQ4IDEtMSAxSDV6IiAvPjwvc3ZnPg==)](https://huggingface.co/models?pipeline_tag=feature-extraction&library=transformers,safetensors){rel="external"
    target="_blank"} model from Huggingface (Embedding Model)

Please note that streaming is currently not implemente in
`AutoModelForCausalLM` for Intel GPU. AMD GPU support is not
implemented. Although AMD CPU is not officially supported by OpenVINO
there are reports that it works: YMMV.

##### Embeddings [*link*](index.html#embeddings){.anchor aria-hidden="true"} {#embeddings}

Use `embeddings: true` if the model is an embedding model

##### Inference device selection [*link*](index.html#inference-device-selection){.anchor aria-hidden="true"} {#inference-device-selection}

Transformer backend tries to automatically select the best device for
inference, anyway you can override the decision manually overriding with
the `main_gpu` parameter.

  Inference Engine   Applicable Values
  ------------------ ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  CUDA               `cuda`, `cuda.X` where X is the GPU device like in `nvidia-smi -L` output
  OpenVINO           Any applicable value from [Inference Modes![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIGhlaWdodD0iMTYiIHZpZXdib3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBmaWxsPSJjdXJyZW50Y29sb3IiIGQ9Ik0xNCA1Yy0uNTUyLjAtMS0uNDQ4LTEtMXMuNDQ4LTEgMS0xaDZjLjU1Mi4wIDEgLjQ0OCAxIDF2NmMwIC41NTItLjQ0OCAxLTEgMXMtMS0uNDQ4LTEtMVY2LjQxNGwtNy4yOTMgNy4yOTNjLS4zOTEuMzktMS4wMjQuMzktMS40MTQuMC0uMzkxLS4zOTEtLjM5MS0xLjAyNC4wLTEuNDE0TDE3LjU4NiA1SDE0ek01IDdjLS41NTIuMC0xIC40NDgtMSAxdjExYzAgLjU1Mi40NDggMSAxIDFoMTFjLjU1Mi4wIDEtLjQ0OCAxLTF2LTQuNTYzYzAtLjU1Mi40NDgtMSAxLTFzMSAuNDQ4IDEgMVYxOWMwIDEuNjU3LTEuMzQzIDMtMyAzSDVjLTEuNjU3LjAtMy0xLjM0My0zLTNWOGMwLTEuNjU3IDEuMzQzLTMgMy0zaDQuNTYzYy41NTIuMCAxIC40NDggMSAxcy0uNDQ4IDEtMSAxSDV6IiAvPjwvc3ZnPg==)](https://docs.openvino.ai/2024/openvino-workflow/running-inference/inference-devices-and-modes.html){rel="external" target="_blank"} like `AUTO`,`CPU`,`GPU`,`NPU`,`MULTI`,`HETERO`

Example for CUDA: `main_gpu: cuda.0`

Example for OpenVINO: `main_gpu: AUTO:-CPU`

This parameter applies to both Text Generation and Feature Extraction
(i.e. Embeddings) models.

##### Inference Precision [*link*](index.html#inference-precision){.anchor aria-hidden="true"} {#inference-precision}

Transformer backend automatically select the fastest applicable
inference precision according to the device support. CUDA backend can
manually enable *bfloat16* if your hardware support it with the
following parameter:

`f16: true`

##### Quantization [*link*](index.html#quantization){.anchor aria-hidden="true"} {#quantization}

  Quantization   Description
  -------------- -----------------------------------
  `bnb_8bit`     8-bit quantization
  `bnb_4bit`     4-bit quantization
  `xpu_8bit`     8-bit quantization for Intel XPUs
  `xpu_4bit`     4-bit quantization for Intel XPUs

##### Trust Remote Code [*link*](index.html#trust-remote-code){.anchor aria-hidden="true"} {#trust-remote-code}

Some models like Microsoft Phi-3 requires external code than what is
provided by the transformer library. By default it is disabled for
security. It can be manually enabled with: `trust_remote_code: true`

##### Maximum Context Size [*link*](index.html#maximum-context-size){.anchor aria-hidden="true"} {#maximum-context-size}

Maximum context size in bytes can be specified with the parameter:
`context_size`. Do not use values higher than what your model support.

Usage example: `context_size: 8192`

##### Auto Prompt Template [*link*](index.html#auto-prompt-template){.anchor aria-hidden="true"} {#auto-prompt-template}

Usually chat template is defined by the model author in the
`tokenizer_config.json` file. To enable it use the
`use_tokenizer_template: true` parameter in the `template` section.

Usage example:

``` {#95f8146 .language-}
  template:
  use_tokenizer_template: true
  
```

##### Custom Stop Words [*link*](index.html#custom-stop-words){.anchor aria-hidden="true"} {#custom-stop-words}

Stopwords are usually defined in `tokenizer_config.json` file. They can
be overridden with the `stopwords` parameter in case of need like in
llama3-Instruct model.

Usage example:

``` {#48585ad .language-}
  stopwords:
- "<|eot_id|>"
- "<|end_of_text|>"
  
```

#### Usage [*link*](index.html#usage-1){.anchor aria-hidden="true"} {#usage-1}

Use the `completions` endpoint by specifying the `transformers` model:

``` {#f460f5d .language-}
  curl http://localhost:8080/v1/completions -H "Content-Type: application/json" -d '{   
   "model": "transformers",
   "prompt": "Hello, my name is",
   "temperature": 0.1, "top_p": 0.1
 }'
  
```

#### Examples [*link*](index.html#examples){.anchor aria-hidden="true"} {#examples}

##### OpenVINO [*link*](index.html#openvino){.anchor aria-hidden="true"} {#openvino}

A model configuration file for openvion and starling model:

``` {#9044f62 .language-yaml}
  name: starling-openvino
backend: transformers
parameters:
  model: fakezeta/Starling-LM-7B-beta-openvino-int8
context_size: 8192
threads: 6
f16: true
type: OVModelForCausalLM
stopwords:
- <|end_of_turn|>
- <|endoftext|>
prompt_cache_path: "cache"
prompt_cache_all: true
template:
  chat_message: |
    {{if eq .RoleName "system"}}{{.Content}}<|end_of_turn|>{{end}}{{if eq .RoleName "assistant"}}<|end_of_turn|>GPT4 Correct Assistant: {{.Content}}<|end_of_turn|>{{end}}{{if eq .RoleName "user"}}GPT4 Correct User: {{.Content}}{{end}}

  chat: |
    {{.Input}}<|end_of_turn|>GPT4 Correct Assistant:

  completion: |
    {{.Input}}
  
```

[[![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjAiIGhlaWdodD0iMjAiIHZpZXdib3g9IjAgMCAzMiAzMiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiBmaWxsPSJjdXJyZW50Y29sb3IiPjxwYXRoIGQ9Ik0xNiAuMzk2Yy04LjgzOS4wLTE2IDcuMTY3LTE2IDE2IDAgNy4wNzMgNC41ODQgMTMuMDY4IDEwLjkzNyAxNS4xODMuODAzLjE1MSAxLjA5My0uMzQ0IDEuMDkzLS43NzIuMC0uMzgtLjAwOS0xLjM4NS0uMDE1LTIuNzE5LTQuNDUzLjk2NC01LjM5MS0yLjE1MS01LjM5MS0yLjE1MS0uNzI5LTEuODQ0LTEuNzgxLTIuMzM5LTEuNzgxLTIuMzM5LTEuNDQ4LS45ODkuMTE1LS45NjguMTE1LS45NjggMS42MDQuMTA5IDIuNDQ4IDEuNjQ1IDIuNDQ4IDEuNjQ1IDEuNDI3IDIuNDQ4IDMuNzQ0IDEuNzQgNC42NjEgMS4zMjguMTQtMS4wMzEuNTU3LTEuNzQgMS4wMTEtMi4xMzUtMy41NTItLjQwMS03LjI4Ny0xLjc3Ni03LjI4Ny03LjkwNy4wLTEuNzUxLjYyLTMuMTc3IDEuNjQ1LTQuMjk3LS4xNzctLjQwMS0uNzE5LTIuMDMxLjE0MS00LjIzNS4wLjAgMS4zMzktLjQyNyA0LjQgMS42NDEgMS4yODEtLjM1NSAyLjY0MS0uNTMyIDQtLjU0MSAxLjM2LjAwOSAyLjcxOS4xODcgNCAuNTQxIDMuMDQzLTIuMDY4IDQuMzgxLTEuNjQxIDQuMzgxLTEuNjQxLjg1OSAyLjIwNC4zMTcgMy44MzMuMTYxIDQuMjM1IDEuMDE1IDEuMTIgMS42MzUgMi41NDcgMS42MzUgNC4yOTcuMCA2LjE0NS0zLjc0IDcuNS03LjI5NiA3Ljg5MS41NTYuNDc5IDEuMDc3IDEuNDY0IDEuMDc3IDIuOTU5LjAgMi4xNC0uMDIgMy44NjQtLjAyIDQuMzg1LjAuNDE2LjI4LjkxNiAxLjEwNC43NTUgNi40LTIuMDkzIDEwLjk3OS04LjA5MyAxMC45NzktMTUuMTU2LjAtOC44MzMtNy4xNjEtMTYtMTYtMTZ6IiAvPjwvc3ZnPg==)]{.me-1
.align-text-bottom}Edit this
page](https://github.com/mudler/LocalAI/blob/master/docs/content/docs/features/text-generation.md){alt="üìñ Text generation (GPT)"
rel="noopener noreferrer" target="_blank"}

Last updated [06 Feb 2025, 18:36 +0100 ]{#relativetime
authdate="2025-02-06T18:36:23+0100" title="06 Feb 2025, 18:36 +0100"}.
[history]{.material-icons .size-20 .align-text-bottom .opacity-75}

<div>

------------------------------------------------------------------------

[](../gpu-acceleration/index.html)

*navigate_before* ‚ö° GPU acceleration

[](../reranker/index.html){.ms-auto}

üìà Reranker *navigate_next*

</div>

¬© 2023-2025 [Ettore Di Giacinto](https://mudler.pm){target="_blank"}

![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiPjxwYXRoIGQ9Ik0xMiAxMC4yMjRsLTYuMyA2LjMtMS4zOC0xLjM3MkwxMiA3LjQ3Mmw3LjY4IDcuNjgtMS4zOCAxLjM3NnoiIHN0eWxlPSJmaWxsOiNmZmYiIC8+PC9zdmc+)
