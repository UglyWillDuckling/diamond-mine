<!doctype html><html lang=en><head><meta charset=utf-8><title>🎨 Image generation | LocalAI documentation</title>
<meta name=viewport content="width=device-width,initial-scale=1"><meta name=keywords content="Documentation,Hugo,Hugo Theme,Bootstrap"><meta name=author content="Colin Wilson - Lotus Labs"><meta name=email content="support@aigis.uk"><meta name=website content="https://lotusdocs.dev"><meta name=Version content="v0.1.0"><link rel=icon href="../../favicon.ico" sizes=any><link rel=icon type=image/svg+xml href="../../favicon.svg"><link rel=apple-touch-icon sizes=180x180 href="../../apple-touch-icon.png"><link rel=icon type=image/png sizes=32x32 href="../../favicon-32x32.png"><link rel=icon type=image/png sizes=16x16 href="../../favicon-16x16.png"><link rel=manifest crossorigin=use-credentials href="../../site.webmanifest"><meta property="og:title" content="🎨 Image generation"><meta property="og:description" content="(Generated with AnimagineXL)
LocalAI supports generating images with Stable diffusion, running on CPU using C++ and Python implementations.
Usage linkOpenAI docs: https://platform.openai.com/docs/api-reference/images/create
To generate an image you can send a POST request to the /v1/images/generations endpoint with the instruction as the request body:
# 512x512 is supported too curl http://localhost:8080/v1/images/generations -H &#34;Content-Type: application/json&#34; -d '{ &#34;prompt&#34;: &#34;A cute baby sea otter&#34;, &#34;size&#34;: &#34;256x256&#34; }' Available additional parameters: mode, step."><meta property="og:type" content="article"><meta property="og:url" content="https://localai.io/features/image-generation/"><meta property="og:image" content="https://localai.io/opengraph/card-base-2_hu06b1a92291a380a0d2e0ec03dab66b2f_17642_filter_14224334656110636267.png"><meta property="article:section" content="docs"><meta property="article:modified_time" content="2025-02-17T16:51:06+01:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://localai.io/opengraph/card-base-2_hu06b1a92291a380a0d2e0ec03dab66b2f_17642_filter_14224334656110636267.png"><meta name=twitter:title content="🎨 Image generation"><meta name=twitter:description content="(Generated with AnimagineXL)
LocalAI supports generating images with Stable diffusion, running on CPU using C++ and Python implementations.
Usage linkOpenAI docs: https://platform.openai.com/docs/api-reference/images/create
To generate an image you can send a POST request to the /v1/images/generations endpoint with the instruction as the request body:
# 512x512 is supported too curl http://localhost:8080/v1/images/generations -H &#34;Content-Type: application/json&#34; -d '{ &#34;prompt&#34;: &#34;A cute baby sea otter&#34;, &#34;size&#34;: &#34;256x256&#34; }' Available additional parameters: mode, step."><script>(()=>{var t=window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches,e=localStorage.getItem("theme");t&&e===null&&(localStorage.setItem("theme","dark"),document.documentElement.setAttribute("data-dark-mode","")),t&&e==="dark"&&document.documentElement.setAttribute("data-dark-mode",""),e==="dark"&&document.documentElement.setAttribute("data-dark-mode","")})()</script><script type=text/javascript src=https://localai.io/docs/js/flexsearch.bundle.min.70b92c4c5202c26b45d7aa43e130e8440032cac751b9b9206ddba2052699286ac95beb01bb7f975514cc694a019a03c8.js integrity=sha384-cLksTFICwmtF16pD4TDoRAAyysdRubkgbduiBSaZKGrJW+sBu3+XVRTMaUoBmgPI crossorigin=anonymous></script><link rel=preconnect href=https://fonts.gstatic.com/><link rel=preconnect href=https://fonts.gstatic.com/ crossorigin><link href="https://fonts.googleapis.com/css?family=Inter:300,400,600,700|Fira+Code:500,700&display=block" rel=stylesheet><link rel=stylesheet href="https://localai.io/docs/scss/style.min.02162a3e86f49ccad73fbe4b65312ef7b90b9d9c0caa7b70d9d0edcec34c69991914189ed3aaeca3b8b9d0a17aad10ce.css" integrity=sha384-AhYqPob0nMrXP75LZTEu97kLnZwMqntw2dDtzsNMaZkZFBie06rso7i50KF6rRDO crossorigin=anonymous></head><body><div class=content><div class="page-wrapper toggled"><nav id=sidebar class=sidebar-wrapper><div class="sidebar-brand d-md-flex justify-content-between align-items-center" style=text-align:center;height:calc(35%)><ul><li><a href="../../index.html" aria-label=HomePage alt=HomePage><img style=width:calc(65%);height:calc(65%) src=https://github.com/go-skynet/LocalAI/assets/2420543/0966aa2a-166e-4f99-a3e5-6c915fc997dd><p class="lead mb-3">LocalAI</p></a></li><li><a href=https://github.com/go-skynet/LocalAI/releases><img src='https://img.shields.io/github/release/go-skynet/LocalAI?&label=Latest&style=for-the-badge'></a></li><li><a href=https://hub.docker.com/r/localai/localai target=_blank><img src="https://img.shields.io/badge/dockerhub-images-important.svg?logo=Docker"></a>
<a href='https://quay.io/repository/go-skynet/local-ai?tab=tags&tag=latest' target=_blank><img src=https://img.shields.io/badge/quay.io-images-important.svg?></a></li></ul></div><div class=sidebar-content style=height:calc(65%)><ul class=sidebar-menu><li class=current><a class=sidebar-root-link href="../../index.html"><i class="material-icons me-2">info</i>
Overview</a></li><li class=sidebar-dropdown><button class=btn>
<i class="material-icons me-2">rocket_launch</i>
Getting started</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href="../../basics/getting_started/index.html">Quickstart</a></li><li><a class=sidebar-nested-link href=https://localai.io/docs/getting-started/models/>Install and Run Models</a></li><li><a class=sidebar-nested-link href="../../basics/try/index.html">Try it out</a></li><li><a class=sidebar-nested-link href=https://localai.io/docs/getting-started/customize-model/>Customizing the Model</a></li><li><a class=sidebar-nested-link href="../../basics/build/index.html">Build LocalAI from source</a></li><li><a class=sidebar-nested-link href="../../basics/container/index.html">Run with container images</a></li><li><a class=sidebar-nested-link href="../../basics/kubernetes/index.html">Run with Kubernetes</a></li></ul></div></li><li><a class=sidebar-root-link href="../../basics/news/index.html"><i class="material-icons me-2">newspaper</i>
News</a></li><li class="sidebar-dropdown current active"><button class=btn>
<i class="material-icons me-2">feature_search</i>
Features</button><div class="sidebar-submenu d-block"><ul><li><a class=sidebar-nested-link href="../gpu-acceleration/index.html">⚡ GPU acceleration</a></li><li><a class=sidebar-nested-link href="../text-generation/index.html">📖 Text generation (GPT)</a></li><li><a class=sidebar-nested-link href="../reranker/index.html">📈 Reranker</a></li><li><a class=sidebar-nested-link href="../text-to-audio/index.html">🗣 Text to audio (TTS)</a></li><li class=current><a class=sidebar-nested-link href="index.html">🎨 Image generation</a></li><li><a class=sidebar-nested-link href="../embeddings/index.html">🧠 Embeddings</a></li><li><a class=sidebar-nested-link href="../gpt-vision/index.html">🥽 GPT Vision</a></li><li><a class=sidebar-nested-link href="../constrained_grammars/index.html">✍️ Constrained Grammars</a></li><li><a class=sidebar-nested-link href="../distribute/index.html">🆕🖧 Distributed Inference</a></li><li><a class=sidebar-nested-link href="../audio-to-text/index.html">🔈 Audio to text</a></li><li><a class=sidebar-nested-link href="../openai-functions/index.html">🔥 OpenAI functions and tools</a></li><li><a class=sidebar-nested-link href="../../stores/index.html">💾 Stores</a></li><li><a class=sidebar-nested-link href="../../models/index.html">🖼️ Model gallery</a></li></ul></div></li><li><a class=sidebar-root-link href=https://localai.io/docs/integrations/><i class="material-icons me-2">sync</i>
Integrations</a></li><li class=sidebar-dropdown><button class=btn>
<i class="material-icons me-2">settings</i>
Advanced</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href="../../advanced/index.html">Advanced usage</a></li><li><a class=sidebar-nested-link href=https://localai.io/docs/advanced/fine-tuning/>Fine-tuning LLMs for text generation</a></li><li><a class=sidebar-nested-link href=https://localai.io/docs/advanced/installer/>Installer options</a></li></ul></div></li><li class=sidebar-dropdown><button class=btn>
<i class="material-icons me-2">menu_book</i>
References</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href="../../model-compatibility/index.html">Model compatibility table</a></li><li><a class=sidebar-nested-link href=https://localai.io/docs/reference/architecture/>Architecture</a></li><li><a class=sidebar-nested-link href=https://localai.io/docs/reference/binaries/>LocalAI binaries</a></li><li><a class=sidebar-nested-link href=https://localai.io/docs/reference/nvidia-l4t/>Running on Nvidia ARM64</a></li></ul></div></li><li><a class=sidebar-root-link href="../../faq/index.html"><i class="material-icons me-2">quiz</i>
FAQ</a></li></ul></div><ul class="sidebar-footer list-unstyled mb-0"></ul></nav><main class="page-content bg-transparent"><div id=top-header class="top-header d-print-none"><div class="header-bar d-flex justify-content-between"><div class="d-flex align-items-center"><a href="../../index.html" class="logo-icon me-3" aria-label=HomePage alt=HomePage><div class=small><svg id="Layer_1" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 250 250"><path d="m143 39.5c-18 0-18 18-18 18s0-18-18-18H22c-2.76.0-5 2.24-5 5v143c0 2.76 2.24 5 5 5h76c7.2.0 8.64 11.52 8.93 16.13.07 1.05.95 1.87 2 1.87h32.14c1.06.0 1.94-.82 2-1.87.29-4.61 1.73-16.13 8.93-16.13h76c2.76.0 5-2.24 5-5V44.5c0-2.76-2.24-5-5-5h-85zM206 163c0 1.38-1.12 2.5-2.5 2.5H143c-18 0-18 18-18 18s0-18-18-18H46.5c-1.38.0-2.5-1.12-2.5-2.5V69c0-1.38 1.12-2.5 2.5-2.5H98c7.2.0 8.64 11.52 8.93 16.13.07 1.05.95 1.87 2 1.87h32.14c1.06.0 1.94-.82 2-1.87.29-4.61 1.73-16.13 8.93-16.13h51.5c1.38.0 2.5 1.12 2.5 2.5v94z" style="fill:#06f"/></svg></div><div class=big><svg id="Layer_1" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 250 250"><path d="m143 39.5c-18 0-18 18-18 18s0-18-18-18H22c-2.76.0-5 2.24-5 5v143c0 2.76 2.24 5 5 5h76c7.2.0 8.64 11.52 8.93 16.13.07 1.05.95 1.87 2 1.87h32.14c1.06.0 1.94-.82 2-1.87.29-4.61 1.73-16.13 8.93-16.13h76c2.76.0 5-2.24 5-5V44.5c0-2.76-2.24-5-5-5h-85zM206 163c0 1.38-1.12 2.5-2.5 2.5H143c-18 0-18 18-18 18s0-18-18-18H46.5c-1.38.0-2.5-1.12-2.5-2.5V69c0-1.38 1.12-2.5 2.5-2.5H98c7.2.0 8.64 11.52 8.93 16.13.07 1.05.95 1.87 2 1.87h32.14c1.06.0 1.94-.82 2-1.87.29-4.61 1.73-16.13 8.93-16.13h51.5c1.38.0 2.5 1.12 2.5 2.5v94z" style="fill:#06f"/></svg></div></a><button id=close-sidebar class="btn btn-icon btn-soft">
<span class="material-icons size-20 menu-icon align-middle">menu</span>
</button>
<button id=flexsearch-button class="ms-3 btn btn-soft" data-bs-toggle=collapse data-bs-target=#FlexSearchCollapse aria-expanded=false aria-controls=FlexSearchCollapse>
<span class="material-icons size-20 menu-icon align-middle">search</span>
<span class="flexsearch-button-placeholder ms-1 me-2 d-none d-sm-block">Search</span><div class="d-none d-sm-block"><span class=flexsearch-button-keys><kbd class=flexsearch-button-cmd-key><svg width="44" height="15"><path d="M2.118 11.5A1.519 1.519.0 011 11.042 1.583 1.583.0 011 8.815a1.519 1.519.0 011.113-.458h.715V6.643h-.71A1.519 1.519.0 011 6.185 1.519 1.519.0 01.547 5.071 1.519 1.519.0 011 3.958 1.519 1.519.0 012.118 3.5a1.519 1.519.0 011.114.458A1.519 1.519.0 013.69 5.071v.715H5.4V5.071A1.564 1.564.0 016.976 3.5 1.564 1.564.0 018.547 5.071 1.564 1.564.0 016.976 6.643H6.261V8.357h.715a1.575 1.575.0 011.113 2.685 1.583 1.583.0 01-2.227.0A1.519 1.519.0 015.4 9.929V9.214H3.69v.715a1.519 1.519.0 01-.458 1.113A1.519 1.519.0 012.118 11.5zm0-.857a.714.714.0 00.715-.714V9.214H2.118a.715.715.0 100 1.429zm4.858.0a.715.715.0 100-1.429H6.261v.715a.714.714.0 00.715.714zM3.69 8.357H5.4V6.643H3.69zM2.118 5.786h.715V5.071a.714.714.0 00-.715-.714.715.715.0 00-.5 1.22A.686.686.0 002.118 5.786zm4.143.0h.715a.715.715.0 00.5-1.22.715.715.0 00-1.22.5z" fill="currentcolor"/><path d="M12.4 11.475H11.344l3.879-7.95h1.056z" fill="currentcolor"/><path d="M25.073 5.384l-.864.576a2.121 2.121.0 00-1.786-.923 2.207 2.207.0 00-2.266 2.326 2.206 2.206.0 002.266 2.325 2.1 2.1.0 001.782-.918l.84.617a3.108 3.108.0 01-2.622 1.293 3.217 3.217.0 01-3.349-3.317 3.217 3.217.0 013.349-3.317A3.046 3.046.0 0125.073 5.384z" fill="currentcolor"/><path d="M30.993 5.142h-2.07v5.419H27.891V5.142h-2.07V4.164h5.172z" fill="currentcolor"/><path d="M34.67 4.164c1.471.0 2.266.658 2.266 1.851.0 1.087-.832 1.809-2.134 1.855l2.107 2.691h-1.28L33.591 7.87H33.07v2.691H32.038v-6.4zm-1.6.969v1.8h1.572c.832.0 1.22-.3 1.22-.918s-.411-.882-1.22-.882z" fill="currentcolor"/><path d="M42.883 10.561H38.31v-6.4h1.033V9.583h3.54z" fill="currentcolor"/></svg></kbd><kbd class=flexsearch-button-key><svg width="15" height="15"><path d="M5.926 12.279H4.41L9.073 2.721H10.59z" fill="currentcolor"/></svg></kbd></span></div></button></div><div class="d-flex align-items-center m-1"><h5>Star us on GitHub !&nbsp;</h5><script async defer src=https://buttons.github.io/buttons.js></script><a class=github-button href=https://github.com/mudler/LocalAI data-color-scheme="no-preference: light; light: light; dark: dark;" data-icon=octicon-star data-size=large data-show-count=true aria-label="Star mudler/LocalAI on GitHub">Star</a></div><div class="d-flex align-items-center"><ul class="list-unstyled mb-0"><li class="list-inline-item mb-0"><a href=https://github.com/mudler/LocalAI alt=github rel="noopener noreferrer" target=_blank><div class="btn btn-icon btn-default border-0"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>GitHub</title><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg></div></a></li><li class="list-inline-item mb-0"><a href=https://twitter.com/LocalAI_API alt=twitter rel="noopener noreferrer" target=_blank><div class="btn btn-icon btn-default border-0"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><title>Twitter / X</title><path d="M.088.768l9.266 12.39L.029 23.231h2.1l8.163-8.819 6.6 8.819h7.142L14.242 10.145 22.921.768h-2.1L13.3 8.891 7.229.768zM3.174 2.314H6.455L20.942 21.685h-3.28z" fill="currentcolor"/></svg></div></a></li><li class="list-inline-item mb-0"><a href="../../index.xml" alt=rss rel="noopener noreferrer" target=_blank><div class="btn btn-icon btn-default border-0"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>RSS</title><path d="M4 11a9 9 0 019 9"/><path d="M4 4a16 16 0 0116 16"/><circle cx="5" cy="19" r="1"/></svg></div></a></li></ul><button id=mode class="btn btn-icon btn-default ms-2" type=button aria-label="Toggle user interface mode">
<span class=toggle-dark><svg xmlns="http://www.w3.org/2000/svg" height="30" width="30" viewBox="0 0 48 48" fill="currentcolor"><title>Enable dark mode</title><path d="M24 42q-7.5.0-12.75-5.25T6 24t5.25-12.75T24 6q.4.0.85.025.45.025 1.15.075-1.8 1.6-2.8 3.95t-1 4.95q0 4.5 3.15 7.65Q28.5 25.8 33 25.8q2.6.0 4.95-.925T41.9 22.3q.05.6.075.975Q42 23.65 42 24q0 7.5-5.25 12.75T24 42zm0-3q5.45.0 9.5-3.375t5.05-7.925q-1.25.55-2.675.825Q34.45 28.8 33 28.8q-5.75.0-9.775-4.025T19.2 15q0-1.2.25-2.575t.9-3.125q-4.9 1.35-8.125 5.475Q9 18.9 9 24q0 6.25 4.375 10.625T24 39zm-.2-14.85z"/></svg></span><span class=toggle-light><svg xmlns="http://www.w3.org/2000/svg" height="30" width="30" viewBox="0 0 48 48" fill="currentcolor"><title>Enable light mode</title><path d="M24 31q2.9.0 4.95-2.05T31 24t-2.05-4.95T24 17t-4.95 2.05T17 24t2.05 4.95T24 31zm0 3q-4.15.0-7.075-2.925T14 24t2.925-7.075T24 14t7.075 2.925T34 24t-2.925 7.075T24 34zM3.5 25.5q-.65.0-1.075-.425Q2 24.65 2 24t.425-1.075Q2.85 22.5 3.5 22.5h5q.65.0 1.075.425Q10 23.35 10 24t-.425 1.075T8.5 25.5zm36 0q-.65.0-1.075-.425Q38 24.65 38 24t.425-1.075T39.5 22.5h5q.65.0 1.075.425Q46 23.35 46 24t-.425 1.075-1.075.425zM24 10q-.65.0-1.075-.425Q22.5 9.15 22.5 8.5v-5q0-.65.425-1.075Q23.35 2 24 2t1.075.425T25.5 3.5v5q0 .65-.425 1.075Q24.65 10 24 10zm0 36q-.65.0-1.075-.425T22.5 44.5v-5q0-.65.425-1.075Q23.35 38 24 38t1.075.425.425 1.075v5q0 .65-.425 1.075Q24.65 46 24 46zM12 14.1l-2.85-2.8q-.45-.45-.425-1.075.025-.625.425-1.075.45-.45 1.075-.45t1.075.45L14.1 12q.4.45.4 1.05.0.6-.4 1-.4.45-1.025.45T12 14.1zm24.7 24.75L33.9 36q-.4-.45-.4-1.075t.45-1.025q.4-.45 1-.45t1.05.45l2.85 2.8q.45.45.425 1.075-.025.625-.425 1.075-.45.45-1.075.45t-1.075-.45zM33.9 14.1q-.45-.45-.45-1.05.0-.6.45-1.05l2.8-2.85q.45-.45 1.075-.425.625.025 1.075.425.45.45.45 1.075t-.45 1.075L36 14.1q-.4.4-1.025.4t-1.075-.4zM9.15 38.85q-.45-.45-.45-1.075t.45-1.075L12 33.9q.45-.45 1.05-.45.6.0 1.05.45.45.45.45 1.05.0.6-.45 1.05l-2.8 2.85q-.45.45-1.075.425-.625-.025-1.075-.425zM24 24z"/></svg></span></button></div></div><div class=collapse id=FlexSearchCollapse><div class=flexsearch-container><div class=flexsearch-keymap><li><kbd class=flexsearch-button-cmd-key><svg width="15" height="15" aria-label="Arrow down" role="img"><g fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.2"><path d="M7.5 3.5v8m3-3-3 3-3-3"/></g></svg></kbd><kbd class=flexsearch-button-cmd-key><svg width="15" height="15" aria-label="Arrow up" role="img"><g fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.2"><path d="M7.5 11.5v-8m3 3-3-3-3 3"/></g></svg></kbd><span class=flexsearch-key-label>to navigate</span></li><li><kbd class=flexsearch-button-cmd-key><svg width="15" height="15" aria-label="Enter key" role="img"><g fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.2"><path d="M12 3.53088v3c0 1-1 2-2 2H4m3 3-3-3 3-3"/></g></svg></kbd><span class=flexsearch-key-label>to select</span></li><li><kbd class=flexsearch-button-cmd-key><svg width="15" height="15" aria-label="Escape key" role="img"><g fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.2"><path d="M13.6167 8.936c-.1065.3583-.6883.962-1.4875.962-.7993.0-1.653-.9165-1.653-2.1258v-.5678c0-1.2548.7896-2.1016 1.653-2.1016s1.3601.4778 1.4875 1.0724M9 6c-.1352-.4735-.7506-.9219-1.46-.8972-.7092.0246-1.344.57-1.344 1.2166s.4198.8812 1.3445.9805C8.465 7.3992 8.968 7.9337 9 8.5s-.454 1.398-1.4595 1.398C6.6593 9.898 6 9 5.963 8.4851m-1.4748.5368c-.2635.5941-.8099.876-1.5443.876s-1.7073-.6248-1.7073-2.204v-.4603c0-1.0416.721-2.131 1.7073-2.131.9864.0 1.6425 1.031 1.5443 2.2492h-2.956"/></g></svg></kbd><span class=flexsearch-key-label>to close</span></li></div><form class="flexsearch position-relative flex-grow-1 ms-2 me-2"><div class="d-flex flex-row"><input id=flexsearch class=form-control type=search placeholder=Search aria-label=Search autocomplete=off>
<button id=hideFlexsearch type=button class="ms-2 btn btn-soft">
cancel</button></div><div id=suggestions class="shadow rounded-1 d-none"></div></form></div></div></div><div class=container-fluid><div class=layout-spacing><div class="d-md-flex justify-content-between align-items-center"><nav aria-label=breadcrumb class="d-inline-block pb-2 mt-1 mt-sm-0"><ul id=breadcrumbs class="breadcrumb bg-transparent mb-0" itemscope itemtype=https://schema.org/BreadcrumbList><li class="breadcrumb-item text-capitalize active" aria-current=page itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a itemprop=item href="../../docs"><i class="material-icons size-20 align-text-bottom" itemprop=name>Home</i>
</a><meta itemprop=position content="1"></li><li class="breadcrumb-item text-capitalize" itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a itemprop=item href="../index.html"><span itemprop=name>Features</span>
</a><meta itemprop=position content="2"></li><li class="breadcrumb-item text-capitalize active" itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><span itemprop=name>🎨 Image generation</span>
<meta itemprop=position content="3"></li></ul></nav></div><div class="row flex-xl-nowrap"><div class="docs-toc col-xl-3 d-xl-block"><toc><div class="fw-bold text-uppercase mb-2">On this page</div><nav id=toc><ul><li><ul><li><a href="index.html#usage">Usage</a></li><li><a href="index.html#backends">Backends</a><ul><li><a href="index.html#stablediffusion-ggml">stablediffusion-ggml</a></li><li><a href="index.html#diffusers">Diffusers</a></li></ul></li></ul></li></ul></nav></toc></div><div class="docs-toc-mobile d-print-none d-xl-none"><button id=toc-dropdown-btn class="btn-secondary dropdown-toggle" type=button data-bs-toggle=dropdown data-bs-offset=0,0 aria-expanded=false>
Table of Contents</button><nav id=toc-mobile><ul class=dropdown-menu><li><ul><li><a href="index.html#usage">Usage</a></li><li><a href="index.html#backends">Backends</a><ul><li><a href="index.html#stablediffusion-ggml">stablediffusion-ggml</a></li><li><a href="index.html#diffusers">Diffusers</a></li></ul></li></ul></li></ul></nav></div><div class="docs-content col-12 col-xl-9 mt-0"><div class="mb-0 d-flex"><i class="material-icons title-icon me-2">article</i><h1 class="content-title mb-0">🎨 Image generation</h1></div><p class="lead mb-3"></p><div id=content class=main-content data-bs-spy=scroll data-bs-root-margin="0px 0px -65%" data-bs-target=#toc-mobile><div data-prismjs-copy data-prismjs-copy-success data-prismjs-copy-error><p><img src="../../8aaca62a-e864-4011-98ae-dcc708103928_499489486369471042.png" alt=anime_girl width=1024 height=1024 loading=lazy>
(Generated with <a href=https://huggingface.co/Linaqruf/animagine-xl rel=external target=_blank>AnimagineXL<svg width="16" height="16" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path fill="currentcolor" d="M14 5c-.552.0-1-.448-1-1s.448-1 1-1h6c.552.0 1 .448 1 1v6c0 .552-.448 1-1 1s-1-.448-1-1V6.414l-7.293 7.293c-.391.39-1.024.39-1.414.0-.391-.391-.391-1.024.0-1.414L17.586 5H14zM5 7c-.552.0-1 .448-1 1v11c0 .552.448 1 1 1h11c.552.0 1-.448 1-1v-4.563c0-.552.448-1 1-1s1 .448 1 1V19c0 1.657-1.343 3-3 3H5c-1.657.0-3-1.343-3-3V8c0-1.657 1.343-3 3-3h4.563c.552.0 1 .448 1 1s-.448 1-1 1H5z"/></svg></a>)</p><p>LocalAI supports generating images with Stable diffusion, running on CPU using C++ and Python implementations.</p><h2 id=usage>Usage <a href="index.html#usage" class=anchor aria-hidden=true><i class="material-icons align-middle">link</i></a></h2><p>OpenAI docs: <a href=https://platform.openai.com/docs/api-reference/images/create rel=external target=_blank>https://platform.openai.com/docs/api-reference/images/create<svg width="16" height="16" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path fill="currentcolor" d="M14 5c-.552.0-1-.448-1-1s.448-1 1-1h6c.552.0 1 .448 1 1v6c0 .552-.448 1-1 1s-1-.448-1-1V6.414l-7.293 7.293c-.391.39-1.024.39-1.414.0-.391-.391-.391-1.024.0-1.414L17.586 5H14zM5 7c-.552.0-1 .448-1 1v11c0 .552.448 1 1 1h11c.552.0 1-.448 1-1v-4.563c0-.552.448-1 1-1s1 .448 1 1V19c0 1.657-1.343 3-3 3H5c-1.657.0-3-1.343-3-3V8c0-1.657 1.343-3 3-3h4.563c.552.0 1 .448 1 1s-.448 1-1 1H5z"/></svg></a></p><p>To generate an image you can send a POST request to the <code>/v1/images/generations</code> endpoint with the instruction as the request body:</p><div class=prism-codeblock><pre id=08a2b22 class=language-bash>
  <code># 512x512 is supported too
curl http://localhost:8080/v1/images/generations -H &#34;Content-Type: application/json&#34; -d &#39;{
  &#34;prompt&#34;: &#34;A cute baby sea otter&#34;,
  &#34;size&#34;: &#34;256x256&#34;
}&#39;</code>
  </pre></div><p>Available additional parameters: <code>mode</code>, <code>step</code>.</p><p>Note: To set a negative prompt, you can split the prompt with <code>|</code>, for instance: <code>a cute baby sea otter|malformed</code>.</p><div class=prism-codeblock><pre id=87fbaa8 class=language-bash>
  <code>curl http://localhost:8080/v1/images/generations -H &#34;Content-Type: application/json&#34; -d &#39;{
  &#34;prompt&#34;: &#34;floating hair, portrait, ((loli)), ((one girl)), cute face, hidden hands, asymmetrical bangs, beautiful detailed eyes, eye shadow, hair ornament, ribbons, bowties, buttons, pleated skirt, (((masterpiece))), ((best quality)), colorful|((part of the head)), ((((mutated hands and fingers)))), deformed, blurry, bad anatomy, disfigured, poorly drawn face, mutation, mutated, extra limb, ugly, poorly drawn hands, missing limb, blurry, floating limbs, disconnected limbs, malformed hands, blur, out of focus, long neck, long body, Octane renderer, lowres, bad anatomy, bad hands, text&#34;,
  &#34;size&#34;: &#34;256x256&#34;
}&#39;</code>
  </pre></div><h2 id=backends>Backends <a href="index.html#backends" class=anchor aria-hidden=true><i class="material-icons align-middle">link</i></a></h2><h3 id=stablediffusion-ggml>stablediffusion-ggml <a href="index.html#stablediffusion-ggml" class=anchor aria-hidden=true><i class="material-icons align-middle">link</i></a></h3><p>This backend is based on <a href=https://github.com/leejet/stable-diffusion.cpp rel=external target=_blank>stable-diffusion.cpp<svg width="16" height="16" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path fill="currentcolor" d="M14 5c-.552.0-1-.448-1-1s.448-1 1-1h6c.552.0 1 .448 1 1v6c0 .552-.448 1-1 1s-1-.448-1-1V6.414l-7.293 7.293c-.391.39-1.024.39-1.414.0-.391-.391-.391-1.024.0-1.414L17.586 5H14zM5 7c-.552.0-1 .448-1 1v11c0 .552.448 1 1 1h11c.552.0 1-.448 1-1v-4.563c0-.552.448-1 1-1s1 .448 1 1V19c0 1.657-1.343 3-3 3H5c-1.657.0-3-1.343-3-3V8c0-1.657 1.343-3 3-3h4.563c.552.0 1 .448 1 1s-.448 1-1 1H5z"/></svg></a>. Every model supported by that backend is suppoerted indeed with LocalAI.</p><h4 id=setup>Setup <a href="index.html#setup" class=anchor aria-hidden=true><i class="material-icons align-middle">link</i></a></h4><p>There are already several models in the gallery that are available to install and get up and running with this backend, you can for example run flux by searching it in the Model gallery (<code>flux.1-dev-ggml</code>) or start LocalAI with <code>run</code>:</p><div class=prism-codeblock><pre id=3ce1f14 class=language-bash>
  <code>local-ai run flux.1-dev-ggml</code>
  </pre></div><p>To use a custom model, you can follow these steps:</p><ol><li>Create a model file <code>stablediffusion.yaml</code> in the models folder:</li></ol><div class=prism-codeblock><pre id=cbe4b6e class=language-yaml>
  <code>name: stablediffusion
backend: stablediffusion-ggml
parameters:
  model: gguf_model.gguf
step: 25
cfg_scale: 4.5
options:
- &#34;clip_l_path:clip_l.safetensors&#34;
- &#34;clip_g_path:clip_g.safetensors&#34;
- &#34;t5xxl_path:t5xxl-Q5_0.gguf&#34;
- &#34;sampler:euler&#34;</code>
  </pre></div><ol start=2><li>Download the required assets to the <code>models</code> repository</li><li>Start LocalAI</li></ol><h3 id=diffusers>Diffusers <a href="index.html#diffusers" class=anchor aria-hidden=true><i class="material-icons align-middle">link</i></a></h3><p><a href=https://huggingface.co/docs/diffusers/index rel=external target=_blank>Diffusers<svg width="16" height="16" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path fill="currentcolor" d="M14 5c-.552.0-1-.448-1-1s.448-1 1-1h6c.552.0 1 .448 1 1v6c0 .552-.448 1-1 1s-1-.448-1-1V6.414l-7.293 7.293c-.391.39-1.024.39-1.414.0-.391-.391-.391-1.024.0-1.414L17.586 5H14zM5 7c-.552.0-1 .448-1 1v11c0 .552.448 1 1 1h11c.552.0 1-.448 1-1v-4.563c0-.552.448-1 1-1s1 .448 1 1V19c0 1.657-1.343 3-3 3H5c-1.657.0-3-1.343-3-3V8c0-1.657 1.343-3 3-3h4.563c.552.0 1 .448 1 1s-.448 1-1 1H5z"/></svg></a> is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. LocalAI has a diffusers backend which allows image generation using the <code>diffusers</code> library.</p><p><img src="../../8aaca62a-e864-4011-98ae-dcc708103928_499489486369471042.png" alt=anime_girl width=1024 height=1024 loading=lazy>
(Generated with <a href=https://huggingface.co/Linaqruf/animagine-xl rel=external target=_blank>AnimagineXL<svg width="16" height="16" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path fill="currentcolor" d="M14 5c-.552.0-1-.448-1-1s.448-1 1-1h6c.552.0 1 .448 1 1v6c0 .552-.448 1-1 1s-1-.448-1-1V6.414l-7.293 7.293c-.391.39-1.024.39-1.414.0-.391-.391-.391-1.024.0-1.414L17.586 5H14zM5 7c-.552.0-1 .448-1 1v11c0 .552.448 1 1 1h11c.552.0 1-.448 1-1v-4.563c0-.552.448-1 1-1s1 .448 1 1V19c0 1.657-1.343 3-3 3H5c-1.657.0-3-1.343-3-3V8c0-1.657 1.343-3 3-3h4.563c.552.0 1 .448 1 1s-.448 1-1 1H5z"/></svg></a>)</p><h4 id=model-setup>Model setup <a href="index.html#model-setup" class=anchor aria-hidden=true><i class="material-icons align-middle">link</i></a></h4><p>The models will be downloaded the first time you use the backend from <code>huggingface</code> automatically.</p><p>Create a model configuration file in the <code>models</code> directory, for instance to use <code>Linaqruf/animagine-xl</code> with CPU:</p><div class=prism-codeblock><pre id=e86f440 class=language-yaml>
  <code>name: animagine-xl
parameters:
  model: Linaqruf/animagine-xl
backend: diffusers

# Force CPU usage - set to true for GPU
f16: false
diffusers:
  cuda: false # Enable for GPU usage (CUDA)
  scheduler_type: euler_a</code>
  </pre></div><h4 id=dependencies>Dependencies <a href="index.html#dependencies" class=anchor aria-hidden=true><i class="material-icons align-middle">link</i></a></h4><p>This is an extra backend - in the container is already available and there is nothing to do for the setup. Do not use <em>core</em> images (ending with <code>-core</code>). If you are building manually, see the <a href="../../basics/build/index.html">build instructions</a>.</p><h4 id=model-setup-1>Model setup <a href="index.html#model-setup-1" class=anchor aria-hidden=true><i class="material-icons align-middle">link</i></a></h4><p>The models will be downloaded the first time you use the backend from <code>huggingface</code> automatically.</p><p>Create a model configuration file in the <code>models</code> directory, for instance to use <code>Linaqruf/animagine-xl</code> with CPU:</p><div class=prism-codeblock><pre id=3e66b53 class=language-yaml>
  <code>name: animagine-xl
parameters:
  model: Linaqruf/animagine-xl
backend: diffusers
cuda: true
f16: true
diffusers:
  scheduler_type: euler_a</code>
  </pre></div><h4 id=local-models>Local models <a href="index.html#local-models" class=anchor aria-hidden=true><i class="material-icons align-middle">link</i></a></h4><p>You can also use local models, or modify some parameters like <code>clip_skip</code>, <code>scheduler_type</code>, for instance:</p><div class=prism-codeblock><pre id=77defa4 class=language-yaml>
  <code>name: stablediffusion
parameters:
  model: toonyou_beta6.safetensors
backend: diffusers
step: 30
f16: true
cuda: true
diffusers:
  pipeline_type: StableDiffusionPipeline
  enable_parameters: &#34;negative_prompt,num_inference_steps,clip_skip&#34;
  scheduler_type: &#34;k_dpmpp_sde&#34;
  clip_skip: 11

cfg_scale: 8</code>
  </pre></div><h4 id=configuration-parameters>Configuration parameters <a href="index.html#configuration-parameters" class=anchor aria-hidden=true><i class="material-icons align-middle">link</i></a></h4><p>The following parameters are available in the configuration file:</p><table><thead><tr><th>Parameter</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>f16</code></td><td>Force the usage of <code>float16</code> instead of <code>float32</code></td><td><code>false</code></td></tr><tr><td><code>step</code></td><td>Number of steps to run the model for</td><td><code>30</code></td></tr><tr><td><code>cuda</code></td><td>Enable CUDA acceleration</td><td><code>false</code></td></tr><tr><td><code>enable_parameters</code></td><td>Parameters to enable for the model</td><td><code>negative_prompt,num_inference_steps,clip_skip</code></td></tr><tr><td><code>scheduler_type</code></td><td>Scheduler type</td><td><code>k_dpp_sde</code></td></tr><tr><td><code>cfg_scale</code></td><td>Configuration scale</td><td><code>8</code></td></tr><tr><td><code>clip_skip</code></td><td>Clip skip</td><td>None</td></tr><tr><td><code>pipeline_type</code></td><td>Pipeline type</td><td><code>AutoPipelineForText2Image</code></td></tr><tr><td><code>lora_adapters</code></td><td>A list of lora adapters (file names relative to model directory) to apply</td><td>None</td></tr><tr><td><code>lora_scales</code></td><td>A list of lora scales (floats) to apply</td><td>None</td></tr></tbody></table><p>There are available several types of schedulers:</p><table><thead><tr><th>Scheduler</th><th>Description</th></tr></thead><tbody><tr><td><code>ddim</code></td><td>DDIM</td></tr><tr><td><code>pndm</code></td><td>PNDM</td></tr><tr><td><code>heun</code></td><td>Heun</td></tr><tr><td><code>unipc</code></td><td>UniPC</td></tr><tr><td><code>euler</code></td><td>Euler</td></tr><tr><td><code>euler_a</code></td><td>Euler a</td></tr><tr><td><code>lms</code></td><td>LMS</td></tr><tr><td><code>k_lms</code></td><td>LMS Karras</td></tr><tr><td><code>dpm_2</code></td><td>DPM2</td></tr><tr><td><code>k_dpm_2</code></td><td>DPM2 Karras</td></tr><tr><td><code>dpm_2_a</code></td><td>DPM2 a</td></tr><tr><td><code>k_dpm_2_a</code></td><td>DPM2 a Karras</td></tr><tr><td><code>dpmpp_2m</code></td><td>DPM++ 2M</td></tr><tr><td><code>k_dpmpp_2m</code></td><td>DPM++ 2M Karras</td></tr><tr><td><code>dpmpp_sde</code></td><td>DPM++ SDE</td></tr><tr><td><code>k_dpmpp_sde</code></td><td>DPM++ SDE Karras</td></tr><tr><td><code>dpmpp_2m_sde</code></td><td>DPM++ 2M SDE</td></tr><tr><td><code>k_dpmpp_2m_sde</code></td><td>DPM++ 2M SDE Karras</td></tr></tbody></table><p>Pipelines types available:</p><table><thead><tr><th>Pipeline type</th><th>Description</th></tr></thead><tbody><tr><td><code>StableDiffusionPipeline</code></td><td>Stable diffusion pipeline</td></tr><tr><td><code>StableDiffusionImg2ImgPipeline</code></td><td>Stable diffusion image to image pipeline</td></tr><tr><td><code>StableDiffusionDepth2ImgPipeline</code></td><td>Stable diffusion depth to image pipeline</td></tr><tr><td><code>DiffusionPipeline</code></td><td>Diffusion pipeline</td></tr><tr><td><code>StableDiffusionXLPipeline</code></td><td>Stable diffusion XL pipeline</td></tr><tr><td><code>StableVideoDiffusionPipeline</code></td><td>Stable video diffusion pipeline</td></tr><tr><td><code>AutoPipelineForText2Image</code></td><td>Automatic detection pipeline for text to image</td></tr><tr><td><code>VideoDiffusionPipeline</code></td><td>Video diffusion pipeline</td></tr><tr><td><code>StableDiffusion3Pipeline</code></td><td>Stable diffusion 3 pipeline</td></tr><tr><td><code>FluxPipeline</code></td><td>Flux pipeline</td></tr><tr><td><code>FluxTransformer2DModel</code></td><td>Flux transformer 2D model</td></tr><tr><td><code>SanaPipeline</code></td><td>Sana pipeline</td></tr></tbody></table><h5 id=advanced-additional-parameters>Advanced: Additional parameters <a href="index.html#advanced-additional-parameters" class=anchor aria-hidden=true><i class="material-icons align-middle">link</i></a></h5><p>Additional arbitrarly parameters can be specified in the option field in key/value separated by <code>:</code>:</p><div class=prism-codeblock><pre id=a098c11 class=language-yaml>
  <code>name: animagine-xl
# ...
options:
- &#34;cfg_scale:6&#34;</code>
  </pre></div><p><strong>Note</strong>: There is no complete parameter list. Any parameter can be passed arbitrarly and is passed to the model directly as argument to the pipeline. Different pipelines/implementations support different parameters.</p><p>The example above, will result in the following python code when generating images:</p><div class=prism-codeblock><pre id=d088a34 class=language-python>
  <code>pipe(
    prompt=&#34;A cute baby sea otter&#34;, # Options passed via API
    size=&#34;256x256&#34;, # Options passed via API
    cfg_scale=6 # Additional parameter passed via configuration file
)</code>
  </pre></div><h4 id=usage-1>Usage <a href="index.html#usage-1" class=anchor aria-hidden=true><i class="material-icons align-middle">link</i></a></h4><h4 id=text-to-image>Text to Image <a href="index.html#text-to-image" class=anchor aria-hidden=true><i class="material-icons align-middle">link</i></a></h4><p>Use the <code>image</code> generation endpoint with the <code>model</code> name from the configuration file:</p><div class=prism-codeblock><pre id=38b914d class=language-bash>
  <code>curl http://localhost:8080/v1/images/generations \
    -H &#34;Content-Type: application/json&#34; \
    -d &#39;{
      &#34;prompt&#34;: &#34;&lt;positive prompt&gt;|&lt;negative prompt&gt;&#34;, 
      &#34;model&#34;: &#34;animagine-xl&#34;, 
      &#34;step&#34;: 51,
      &#34;size&#34;: &#34;1024x1024&#34; 
    }&#39;</code>
  </pre></div><h4 id=image-to-image>Image to Image <a href="index.html#image-to-image" class=anchor aria-hidden=true><i class="material-icons align-middle">link</i></a></h4><p><a href=https://huggingface.co/docs/diffusers/using-diffusers/img2img rel=external target=_blank>https://huggingface.co/docs/diffusers/using-diffusers/img2img<svg width="16" height="16" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path fill="currentcolor" d="M14 5c-.552.0-1-.448-1-1s.448-1 1-1h6c.552.0 1 .448 1 1v6c0 .552-.448 1-1 1s-1-.448-1-1V6.414l-7.293 7.293c-.391.39-1.024.39-1.414.0-.391-.391-.391-1.024.0-1.414L17.586 5H14zM5 7c-.552.0-1 .448-1 1v11c0 .552.448 1 1 1h11c.552.0 1-.448 1-1v-4.563c0-.552.448-1 1-1s1 .448 1 1V19c0 1.657-1.343 3-3 3H5c-1.657.0-3-1.343-3-3V8c0-1.657 1.343-3 3-3h4.563c.552.0 1 .448 1 1s-.448 1-1 1H5z"/></svg></a></p><p>An example model (GPU):</p><div class=prism-codeblock><pre id=bb43ba0 class=language-yaml>
  <code>name: stablediffusion-edit
parameters:
  model: nitrosocke/Ghibli-Diffusion
backend: diffusers
step: 25
cuda: true
f16: true
diffusers:
  pipeline_type: StableDiffusionImg2ImgPipeline
  enable_parameters: &#34;negative_prompt,num_inference_steps,image&#34;</code>
  </pre></div><div class=prism-codeblock><pre id=dbd826c class=language-bash>
  <code>IMAGE_PATH=/path/to/your/image
(echo -n &#39;{&#34;file&#34;: &#34;&#39;; base64 $IMAGE_PATH; echo &#39;&#34;, &#34;prompt&#34;: &#34;a sky background&#34;,&#34;size&#34;: &#34;512x512&#34;,&#34;model&#34;:&#34;stablediffusion-edit&#34;}&#39;) |
curl -H &#34;Content-Type: application/json&#34; -d @-  http://localhost:8080/v1/images/generations</code>
  </pre></div><h4 id=depth-to-image>Depth to Image <a href="index.html#depth-to-image" class=anchor aria-hidden=true><i class="material-icons align-middle">link</i></a></h4><p><a href=https://huggingface.co/docs/diffusers/using-diffusers/depth2img rel=external target=_blank>https://huggingface.co/docs/diffusers/using-diffusers/depth2img<svg width="16" height="16" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path fill="currentcolor" d="M14 5c-.552.0-1-.448-1-1s.448-1 1-1h6c.552.0 1 .448 1 1v6c0 .552-.448 1-1 1s-1-.448-1-1V6.414l-7.293 7.293c-.391.39-1.024.39-1.414.0-.391-.391-.391-1.024.0-1.414L17.586 5H14zM5 7c-.552.0-1 .448-1 1v11c0 .552.448 1 1 1h11c.552.0 1-.448 1-1v-4.563c0-.552.448-1 1-1s1 .448 1 1V19c0 1.657-1.343 3-3 3H5c-1.657.0-3-1.343-3-3V8c0-1.657 1.343-3 3-3h4.563c.552.0 1 .448 1 1s-.448 1-1 1H5z"/></svg></a></p><div class=prism-codeblock><pre id=f4563b6 class=language-yaml>
  <code>name: stablediffusion-depth
parameters:
  model: stabilityai/stable-diffusion-2-depth
backend: diffusers
step: 50
# Force CPU usage
f16: true
cuda: true
diffusers:
  pipeline_type: StableDiffusionDepth2ImgPipeline
  enable_parameters: &#34;negative_prompt,num_inference_steps,image&#34;

cfg_scale: 6</code>
  </pre></div><div class=prism-codeblock><pre id=d96bb2b class=language-bash>
  <code>(echo -n &#39;{&#34;file&#34;: &#34;&#39;; base64 ~/path/to/image.jpeg; echo &#39;&#34;, &#34;prompt&#34;: &#34;a sky background&#34;,&#34;size&#34;: &#34;512x512&#34;,&#34;model&#34;:&#34;stablediffusion-depth&#34;}&#39;) |
curl -H &#34;Content-Type: application/json&#34; -d @-  http://localhost:8080/v1/images/generations</code>
  </pre></div><h4 id=img2vid>img2vid <a href="index.html#img2vid" class=anchor aria-hidden=true><i class="material-icons align-middle">link</i></a></h4><div class=prism-codeblock><pre id=dcd8283 class=language-yaml>
  <code>name: img2vid
parameters:
  model: stabilityai/stable-video-diffusion-img2vid
backend: diffusers
step: 25
# Force CPU usage
f16: true
cuda: true
diffusers:
  pipeline_type: StableVideoDiffusionPipeline</code>
  </pre></div><div class=prism-codeblock><pre id=318d416 class=language-bash>
  <code>(echo -n &#39;{&#34;file&#34;: &#34;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/svd/rocket.png?download=true&#34;,&#34;size&#34;: &#34;512x512&#34;,&#34;model&#34;:&#34;img2vid&#34;}&#39;) |
curl -H &#34;Content-Type: application/json&#34; -X POST -d @- http://localhost:8080/v1/images/generations</code>
  </pre></div><h4 id=txt2vid>txt2vid <a href="index.html#txt2vid" class=anchor aria-hidden=true><i class="material-icons align-middle">link</i></a></h4><div class=prism-codeblock><pre id=31bd565 class=language-yaml>
  <code>name: txt2vid
parameters:
  model: damo-vilab/text-to-video-ms-1.7b
backend: diffusers
step: 25
# Force CPU usage
f16: true
cuda: true
diffusers:
  pipeline_type: VideoDiffusionPipeline
  cuda: true</code>
  </pre></div><div class=prism-codeblock><pre id=1be991b class=language-bash>
  <code>(echo -n &#39;{&#34;prompt&#34;: &#34;spiderman surfing&#34;,&#34;size&#34;: &#34;512x512&#34;,&#34;model&#34;:&#34;txt2vid&#34;}&#39;) |
curl -H &#34;Content-Type: application/json&#34; -X POST -d @- http://localhost:8080/v1/images/generations</code>
  </pre></div></div><div class="gitinfo d-flex flex-wrap justify-content-between align-items-center opacity-85 pt-3"><div id=edit-this-page class=mt-1><a href=https://github.com/mudler/LocalAI/blob/master/docs/content/docs/features/image-generation.md alt="🎨 Image generation" rel="noopener noreferrer" target=_blank><span class="me-1 align-text-bottom"><svg width="20" height="20" viewBox="0 0 32 32" xmlns="http://www.w3.org/2000/svg" fill="currentcolor"><path d="M16 .396c-8.839.0-16 7.167-16 16 0 7.073 4.584 13.068 10.937 15.183.803.151 1.093-.344 1.093-.772.0-.38-.009-1.385-.015-2.719-4.453.964-5.391-2.151-5.391-2.151-.729-1.844-1.781-2.339-1.781-2.339-1.448-.989.115-.968.115-.968 1.604.109 2.448 1.645 2.448 1.645 1.427 2.448 3.744 1.74 4.661 1.328.14-1.031.557-1.74 1.011-2.135-3.552-.401-7.287-1.776-7.287-7.907.0-1.751.62-3.177 1.645-4.297-.177-.401-.719-2.031.141-4.235.0.0 1.339-.427 4.4 1.641 1.281-.355 2.641-.532 4-.541 1.36.009 2.719.187 4 .541 3.043-2.068 4.381-1.641 4.381-1.641.859 2.204.317 3.833.161 4.235 1.015 1.12 1.635 2.547 1.635 4.297.0 6.145-3.74 7.5-7.296 7.891.556.479 1.077 1.464 1.077 2.959.0 2.14-.02 3.864-.02 4.385.0.416.28.916 1.104.755 6.4-2.093 10.979-8.093 10.979-15.156.0-8.833-7.161-16-16-16z"/></svg></span>Edit this page</a></div><div id=last-modified class=mt-1><p class="mb-0 fw-semibold">Last updated <span id=relativetime data-authdate=2025-02-17T16:51:06+0100 title="17 Feb 2025, 16:51 +0100">17 Feb 2025, 16:51 +0100
</span>. <span class="material-icons size-20 align-text-bottom opacity-75">history</span></p></div></div></div><div><hr class=doc-hr><div id=doc-nav class=d-print-none><div class="row flex-xl-nowrap"><div class="col-sm-6 pt-2 doc-next"><a href="../text-to-audio/index.html"><div class="card h-100 my-1"><div class="card-body py-2"><p class="card-title fs-5 fw-semibold lh-base mb-0"><i class="material-icons align-middle">navigate_before</i> 🗣 Text to audio (TTS)</p><p class="card-text ms-2"></p></div></div></a></div><div class="col-sm-6 pt-2 doc-prev"><a class=ms-auto href="../embeddings/index.html"><div class="card h-100 my-1 text-end"><div class="card-body py-2"><p class="card-title fs-5 fw-semibold lh-base mb-0">🧠 Embeddings <i class="material-icons align-middle">navigate_next</i></p><p class="card-text me-2"></p></div></div></a></div></div></div></div></div></div></div></div><footer class="shadow py-3 d-print-none"><div class=container-fluid><div class="row align-items-center"><div class=col><div class="text-sm-start text-center mx-md-2"><p class=mb-0>© 2023-2025 <a href=https://mudler.pm target=_blank>Ettore Di Giacinto</a></p></div></div></div></div></footer></main></div></div><button onclick=topFunction() id=back-to-top aria-label="Back to Top Button" class="back-to-top fs-5"><svg width="24" height="24"><path d="M12 10.224l-6.3 6.3-1.38-1.372L12 7.472l7.68 7.68-1.38 1.376z" style="fill:#fff"/></svg></button>
<script>(()=>{var e=document.getElementById("mode");e!==null&&(window.matchMedia("(prefers-color-scheme: dark)").addEventListener("change",e=>{e.matches?(localStorage.setItem("theme","dark"),document.documentElement.setAttribute("data-dark-mode","")):(localStorage.setItem("theme","light"),document.documentElement.removeAttribute("data-dark-mode"))}),e.addEventListener("click",()=>{document.documentElement.toggleAttribute("data-dark-mode"),localStorage.setItem("theme",document.documentElement.hasAttribute("data-dark-mode")?"dark":"light")}),localStorage.getItem("theme")==="dark"?document.documentElement.setAttribute("data-dark-mode",""):document.documentElement.removeAttribute("data-dark-mode"))})()</script><script src="https://localai.io/docs/js/bootstrap.9bf1a2680076ecb3d5bf761d03e44316ea1546765dad06843315f94e5fd5d8e2257f2417a68c98561e887bfeac9556c9.js" integrity=sha384-m/GiaAB27LPVv3YdA+RDFuoVRnZdrQaEMxX5Tl/V2OIlfyQXpoyYVh6Ie/6slVbJ defer></script><script type=text/javascript src=https://localai.io/docs/js/bundle.min.9bae95431897c68aee2ce28f446b5c335db1d5ee19f5979fbbe677fef408caa167e1c766de7f791c59aab168be67ad6b.js integrity=sha384-m66VQxiXxoruLOKPRGtcM12x1e4Z9Zefu+Z3/vQIyqFn4cdm3n95HFmqsWi+Z61r crossorigin=anonymous defer></script><script type=module>
    var suggestions = document.getElementById('suggestions');
    var search = document.getElementById('flexsearch');

    const flexsearchContainer = document.getElementById('FlexSearchCollapse');

    const hideFlexsearchBtn = document.getElementById('hideFlexsearch');

    const configObject = { toggle: false }
    const flexsearchContainerCollapse = new Collapse(flexsearchContainer, configObject) 

    if (search !== null) {
        document.addEventListener('keydown', inputFocus);
        flexsearchContainer.addEventListener('shown.bs.collapse', function () {
            search.focus();
        });
        
        var topHeader = document.getElementById("top-header");
        document.addEventListener('click', function(elem) {
            if (!flexsearchContainer.contains(elem.target) && !topHeader.contains(elem.target))
                flexsearchContainerCollapse.hide();
        });
    }

    hideFlexsearchBtn.addEventListener('click', () =>{
        flexsearchContainerCollapse.hide()
    })

    function inputFocus(e) {
        if (e.ctrlKey && e.key === '/') {
            e.preventDefault();
            flexsearchContainerCollapse.toggle();
        }
        if (e.key === 'Escape' ) {
            search.blur();
            
            flexsearchContainerCollapse.hide();
        }
    };

    document.addEventListener('click', function(event) {

    var isClickInsideElement = suggestions.contains(event.target);

    if (!isClickInsideElement) {
        suggestions.classList.add('d-none');
    }

    });

    


    document.addEventListener('keydown',suggestionFocus);

    function suggestionFocus(e) {
    const suggestionsHidden = suggestions.classList.contains('d-none');
    if (suggestionsHidden) return;

    const focusableSuggestions= [...suggestions.querySelectorAll('a')];
    if (focusableSuggestions.length === 0) return;

    const index = focusableSuggestions.indexOf(document.activeElement);

    if (e.key === "ArrowUp") {
        e.preventDefault();
        const nextIndex = index > 0 ? index - 1 : 0;
        focusableSuggestions[nextIndex].focus();
    }
    else if (e.key === "ArrowDown") {
        e.preventDefault();
        const nextIndex= index + 1 < focusableSuggestions.length ? index + 1 : index;
        focusableSuggestions[nextIndex].focus();
    }

    }

    


    (function(){

    var index = new FlexSearch.Document({
        
        tokenize: "forward",
        minlength:  0 ,
        cache:  100 ,
        optimize:  true ,
        document: {
        id: 'id',
        store: [
            "href", "title", "description"
        ],
        index: ["title", "description", "content"]
        }
    });


    


    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    


    

    

    index.add(
            {
                id:  0 ,
                href: "\/",
                title: "Overview",
                description: "What is LocalAI?",
                content: " 💡 Get help - ❓FAQ 💭Discussions 💭Discord\n💻 Quickstart 🖼️ Models 🚀 Roadmap 🥽 Demo 🌍 Explorer 🛫 Examples\nLocalAI is the free, Open Source OpenAI alternative. LocalAI act as a drop-in replacement REST API that’s compatible with OpenAI API specifications for local inferencing. It allows you to run LLMs, generate images, audio (and not only) locally or on-prem with consumer grade hardware, supporting multiple model families and architectures. Does not require GPU. It is created and maintained by Ettore Di Giacinto.\nStart LocalAI linkStart the image with Docker to have a functional clone of OpenAI! 🚀:\ndocker run -p 8080:8080 --name local-ai -ti localai/localai:latest-aio-cpu # Do you have a Nvidia GPUs? Use this instead # CUDA 11 # docker run -p 8080:8080 --gpus all --name local-ai -ti localai/localai:latest-aio-gpu-nvidia-cuda-11 # CUDA 12 # docker run -p 8080:8080 --gpus all --name local-ai -ti localai/localai:latest-aio-gpu-nvidia-cuda-12 Or just use the bash installer:\ncurl https://localai.io/install.sh | sh See the 💻 Quickstart for all the options and way you can run LocalAI!\nWhat is LocalAI? linkIn a nutshell:\nLocal, OpenAI drop-in alternative REST API. You own your data. NO GPU required. NO Internet access is required either Optional, GPU Acceleration is available. See also the build section. Supports multiple models 🏃 Once loaded the first time, it keep models loaded in memory for faster inference ⚡ Doesn’t shell-out, but uses bindings for a faster inference and better performance. LocalAI is focused on making the AI accessible to anyone. Any contribution, feedback and PR is welcome!\nNote that this started just as a fun weekend project by mudler in order to try to create the necessary pieces for a full AI assistant like ChatGPT: the community is growing fast and we are working hard to make it better and more stable. If you want to help, please consider contributing (see below)!\n🚀 Features link 📖 Text generation with GPTs (llama.cpp, gpt4all.cpp, … 📖 and more) 🗣 Text to Audio 🔈 Audio to Text (Audio transcription with whisper.cpp) 🎨 Image generation with stable diffusion 🔥 OpenAI functions 🆕 🧠 Embeddings generation for vector databases ✍️ Constrained grammars 🖼️ Download Models directly from Huggingface 🥽 Vision API 💾 Stores 📈 Reranker 🆕🖧 P2P Inferencing Contribute and help linkTo help the project you can:\nIf you have technological skills and want to contribute to development, have a look at the open issues. If you are new you can have a look at the good-first-issue and help-wanted labels.\nIf you don’t have technological skills you can still help improving documentation or add examples or share your user-stories with our community, any help and contribution is welcome!\n🌟 Star history link star-history.comAprilJulyOctober2024AprilJulyOctober2025 5.0k10.0k15.0k20.0k25.0k30.0kmudler/LocalAIStar HistoryDateGitHub Stars ❤️ Sponsors link Do you find LocalAI useful?\nSupport the project by becoming a backer or sponsor. Your logo will show up here with a link to your website.\nA huge thank you to our generous sponsors who support this project covering CI expenses, and our Sponsor list:\n📖 License linkLocalAI is a community-driven project created by Ettore Di Giacinto.\nMIT - Author Ettore Di Giacinto\n🙇 Acknowledgements linkLocalAI couldn’t have been built without the help of great software already available from the community. Thank you!\nllama.cpp https://github.com/tatsu-lab/stanford_alpaca https://github.com/cornelk/llama-go for the initial ideas https://github.com/antimatter15/alpaca.cpp https://github.com/EdVince/Stable-Diffusion-NCNN https://github.com/ggerganov/whisper.cpp https://github.com/saharNooby/rwkv.cpp https://github.com/rhasspy/piper 🤗 Contributors linkThis is a community project, a special thanks to our contributors! 🤗 "
            }
        );
    index.add(
            {
                id:  1 ,
                href: "\/docs\/getting-started\/",
                title: "Getting started",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  2 ,
                href: "\/basics\/getting_started\/",
                title: "Quickstart",
                description: "LocalAI is a free, open-source alternative to OpenAI (Anthropic, etc.), functioning as a drop-in replacement REST API for local inferencing. It allows you to run LLMs, generate images, and produce audio, all locally or on-premises with consumer-grade hardware, supporting multiple model families and architectures.\n💡\nSecurity considerations\nIf you are exposing LocalAI remotely, make sure you protect the API endpoints adequately with a mechanism which allows to protect from the incoming traffic or alternatively, run LocalAI with API_KEY to gate the access with an API key.",
                content: "LocalAI is a free, open-source alternative to OpenAI (Anthropic, etc.), functioning as a drop-in replacement REST API for local inferencing. It allows you to run LLMs, generate images, and produce audio, all locally or on-premises with consumer-grade hardware, supporting multiple model families and architectures.\n💡\nSecurity considerations\nIf you are exposing LocalAI remotely, make sure you protect the API endpoints adequately with a mechanism which allows to protect from the incoming traffic or alternatively, run LocalAI with API_KEY to gate the access with an API key. The API key guarantees a total access to the features (there is no role separation), and it is to be considered as likely as an admin role.\nTo access the WebUI with an API_KEY, browser extensions such as Requestly can be used (see also https://github.com/mudler/LocalAI/issues/2227#issuecomment-2093333752). See also API flags for the flags / options available when starting LocalAI.\nUsing the Bash Installer linkInstall LocalAI easily using the bash installer with the following command:\ncurl https://localai.io/install.sh | sh For a full list of options, refer to the "
            }
        );
    index.add(
            {
                id:  3 ,
                href: "\/docs\/getting-started\/models\/",
                title: "Install and Run Models",
                description: "To install models with LocalAI, you can: Browse the Model Gallery from the Web Interface and install models with a couple of clicks. For more details, refer to the Gallery Documentation. Specify a model from the LocalAI gallery during startup, e.g., local-ai run . Use a URI to specify a model file (e.g., huggingface://..., oci://, or ollama://) when starting LocalAI, e.g., local-ai run huggingface://TheBloke/phi-2-GGUF/phi-2.Q8_0.gguf. Specify a URL to a model configuration file when starting LocalAI, e.",
                content: "To install models with LocalAI, you can:\nBrowse the Model Gallery from the Web Interface and install models with a couple of clicks. For more details, refer to the Gallery Documentation. Specify a model from the LocalAI gallery during startup, e.g., local-ai run . Use a URI to specify a model file (e.g., huggingface://..., oci://, or ollama://) when starting LocalAI, e.g., local-ai run huggingface://TheBloke/phi-2-GGUF/phi-2.Q8_0.gguf. Specify a URL to a model configuration file when starting LocalAI, e.g., local-ai run https://gist.githubusercontent.com/.../phi-2.yaml. Manually install the models by copying the files into the models directory (--models). Run and Install Models via the Gallery linkTo run models available in the LocalAI gallery, you can use the WebUI or specify the model name when starting LocalAI. Models can be found in the gallery via the Web interface, the model gallery, or the CLI with: local-ai models list.\nTo install a model from the gallery, use the model name as the URI. For example, to run LocalAI with the Hermes model, execute:\nlocal-ai run hermes-2-theta-llama-3-8b To install only the model, use:\nlocal-ai models install hermes-2-theta-llama-3-8b Note: The galleries available in LocalAI can be customized to point to a different URL or a local directory. For more information on how to setup your own gallery, see the Gallery Documentation.\nRun Models via URI linkTo run models via URI, specify a URI to a model file or a configuration file when starting LocalAI. Valid syntax includes:\nfile://path/to/model huggingface://repository_id/model_file (e.g., huggingface://TheBloke/phi-2-GGUF/phi-2.Q8_0.gguf) From OCIs: oci://container_image:tag, ollama://model_id:tag From configuration files: https://gist.githubusercontent.com/.../phi-2.yaml Configuration files can be used to customize the model defaults and settings. For advanced configurations, refer to the "
            }
        );
    index.add(
            {
                id:  4 ,
                href: "\/basics\/try\/",
                title: "Try it out",
                description: "Once LocalAI is installed, you can start it (either by using docker, or the cli, or the systemd service).\nBy default the LocalAI WebUI should be accessible from http://localhost:8080. You can also use 3rd party projects to interact with LocalAI as you would use OpenAI (see also Integrations ).\nAfter installation, install new models by navigating the model gallery, or by using the local-ai CLI.\n🚀\nTo install models with the WebUI, see the Models section.",
                content: "Once LocalAI is installed, you can start it (either by using docker, or the cli, or the systemd service).\nBy default the LocalAI WebUI should be accessible from http://localhost:8080. You can also use 3rd party projects to interact with LocalAI as you would use OpenAI (see also "
            }
        );
    index.add(
            {
                id:  5 ,
                href: "\/docs\/getting-started\/customize-model\/",
                title: "Customizing the Model",
                description: "To customize the prompt template or the default settings of the model, a configuration file is utilized. This file must adhere to the LocalAI YAML configuration standards. For comprehensive syntax details, refer to the advanced documentation. The configuration file can be located either remotely (such as in a Github Gist) or within the local filesystem or a remote URL.\nLocalAI can be initiated using either its container image or binary, with a command that includes URLs of model config files or utilizes a shorthand format (like huggingface:// or github://), which is then expanded into complete URLs.",
                content: "To customize the prompt template or the default settings of the model, a configuration file is utilized. This file must adhere to the LocalAI YAML configuration standards. For comprehensive syntax details, refer to the "
            }
        );
    index.add(
            {
                id:  6 ,
                href: "\/basics\/build\/",
                title: "Build LocalAI from source",
                description: "Build linkLocalAI can be built as a container image or as a single, portable binary. Note that the some model architectures might require Python libraries, which are not included in the binary. The binary contains only the core backends written in Go and C++.\nLocalAI’s extensible architecture allows you to add your own backends, which can be written in any language, and as such the container images contains also the Python dependencies to run all the available backends (for example, in order to run backends like Diffusers that allows to generate images and videos from text).",
                content: "Build linkLocalAI can be built as a container image or as a single, portable binary. Note that the some model architectures might require Python libraries, which are not included in the binary. The binary contains only the core backends written in Go and C++.\nLocalAI’s extensible architecture allows you to add your own backends, which can be written in any language, and as such the container images contains also the Python dependencies to run all the available backends (for example, in order to run backends like Diffusers that allows to generate images and videos from text).\nIn some cases you might want to re-build LocalAI from source (for instance to leverage Apple Silicon acceleration), or to build a custom container image with your own backends. This section contains instructions on how to build LocalAI from source.\nBuild LocalAI locally linkRequirements linkIn order to build LocalAI locally, you need the following requirements:\nGolang \u003e= 1.21 Cmake/make GCC GRPC To install the dependencies follow the instructions below:\nApple Debian From source Install xcode from the App Store\nbrew install abseil cmake go grpc protobuf protoc-gen-go protoc-gen-go-grpc python wget After installing the above dependencies, you need to install grpcio-tools from PyPI. You could do this via a pip –user install or a virtualenv.\npip install --user grpcio-tools apt install cmake golang libgrpc-dev make protobuf-compiler-grpc python3-grpc-tools After you have golang installed and working, you can install the required binaries for compiling the golang protobuf components via the following commands\ngo install google.golang.org/protobuf/cmd/protoc-gen-go@v1.34.2 go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@1958fcbe2ca8bd93af633f11e97d44e567e945af Specify BUILD_GRPC_FOR_BACKEND_LLAMA=true to build automatically the gRPC dependencies\nmake ... BUILD_GRPC_FOR_BACKEND_LLAMA=true build Build linkTo build LocalAI with make:\ngit clone https://github.com/go-skynet/LocalAI cd LocalAI make build This should produce the binary local-ai\nHere is the list of the variables available that can be used to customize the build:\nVariable Default Description BUILD_TYPE None Build type. Available: cublas, openblas, clblas, metal,hipblas, sycl_f16, sycl_f32 GO_TAGS tts stablediffusion Go tags. Available: stablediffusion, tts CLBLAST_DIR Specify a CLBlast directory CUDA_LIBPATH Specify a CUDA library path BUILD_API_ONLY false Set to true to build only the API (no backends will be built) notifications CPU flagset compatibility linkLocalAI uses different backends based on ggml and llama.cpp to run models. If your CPU doesn’t support common instruction sets, you can disable them during build:\nCMAKE_ARGS=\"-DGGML_F16C=OFF -DGGML_AVX512=OFF -DGGML_AVX2=OFF -DGGML_AVX=OFF -DGGML_FMA=OFF\" make build To have effect on the container image, you need to set REBUILD=true:\ndocker run quay.io/go-skynet/localai docker run --rm -ti -p 8080:8080 -e DEBUG=true -e MODELS_PATH=/models -e THREADS=1 -e REBUILD=true -e CMAKE_ARGS=\"-DGGML_F16C=OFF -DGGML_AVX512=OFF -DGGML_AVX2=OFF -DGGML_AVX=OFF -DGGML_FMA=OFF\" -v $PWD/models:/models quay.io/go-skynet/local-ai:latest Container image linkRequirements:\nDocker or podman, or a container engine In order to build the LocalAI container image locally you can use docker, for example:\n# build the image docker build -t localai . docker run localai There are some build arguments that can be used to customize the build:\nVariable Default Description IMAGE_TYPE extras Build type. Available: core, extras Example: Build on mac linkBuilding on Mac (M1, M2 or M3) works, but you may need to install some prerequisites using brew.\nThe below has been tested by one mac user and found to work. Note that this doesn’t use Docker to run the server:\nInstall xcode from the Apps Store (needed for metalkit)\n# install build dependencies brew install abseil cmake go grpc protobuf wget protoc-gen-go protoc-gen-go-grpc # clone the repo git clone https://github.com/go-skynet/LocalAI.git cd LocalAI # build the binary make build # Download phi-2 to models/ wget https://huggingface.co/TheBloke/phi-2-GGUF/resolve/main/phi-2.Q2_K.gguf -O models/phi-2.Q2_K # Use a template from the examples cp -rf prompt-templates/ggml-gpt4all-j.tmpl models/phi-2.Q2_K.tmpl # Run LocalAI ./local-ai --models-path=./models/ --debug=true # Now API is accessible at localhost:8080 curl http://localhost:8080/v1/models curl http://localhost:8080/v1/chat/completions -H \"Content-Type: application/json\" -d '{ \"model\": \"phi-2.Q2_K\", \"messages\": [{\"role\": \"user\", \"content\": \"How are you?\"}], \"temperature\": 0.9 }' Troubleshooting mac link If you encounter errors regarding a missing utility metal, install Xcode from the App Store.\nAfter the installation of Xcode, if you receive a xcrun error 'xcrun: error: unable to find utility \"metal\", not a developer tool or in PATH'. You might have installed the Xcode command line tools before installing Xcode, the former one is pointing to an incomplete SDK.\n# print /Library/Developer/CommandLineTools, if command line tools were installed in advance xcode-select --print-path # point to a complete SDK sudo xcode-select --switch /Applications/Xcode.app/Contents/Developer If completions are slow, ensure that gpu-layers in your model yaml matches the number of layers from the model in use (or simply use a high number such as 256).\nIf you a get a compile error: error: only virtual member functions can be marked 'final', reinstall all the necessary brew packages, clean the build, and try again.\n# reinstall build dependencies brew reinstall abseil cmake go grpc protobuf wget make clean make build Requirements: OpenCV, Gomp\nImage generation requires GO_TAGS=stablediffusion to be set during build:\nmake GO_TAGS=stablediffusion build Build with Text to audio support linkRequirements: piper-phonemize\nText to audio support is experimental and requires GO_TAGS=tts to be set during build:\nmake GO_TAGS=tts build Acceleration linkOpenBLAS linkSoftware acceleration.\nRequirements: OpenBLAS\nmake BUILD_TYPE=openblas build CuBLAS linkNvidia Acceleration.\nRequirement: Nvidia CUDA toolkit\nNote: CuBLAS support is experimental, and has not been tested on real HW. please report any issues you find!\nmake BUILD_TYPE=cublas build More informations available in the upstream PR: https://github.com/ggerganov/llama.cpp/pull/1412\nHipblas (AMD GPU with ROCm on Arch Linux) linkPackages:\npacman -S base-devel git rocm-hip-sdk rocm-opencl-sdk opencv clblast grpc Library links:\nexport CGO_CFLAGS=\"-I/usr/include/opencv4\" export CGO_CXXFLAGS=\"-I/usr/include/opencv4\" export CGO_LDFLAGS=\"-L/opt/rocm/hip/lib -lamdhip64 -L/opt/rocm/lib -lOpenCL -L/usr/lib -lclblast -lrocblas -lhipblas -lrocrand -lomp -O3 --rtlib=compiler-rt -unwindlib=libgcc -lhipblas -lrocblas --hip-link\" Build:\nmake BUILD_TYPE=hipblas GPU_TARGETS=gfx1030 ClBLAS linkAMD/Intel GPU acceleration.\nRequirement: OpenCL, CLBlast\nmake BUILD_TYPE=clblas build To specify a clblast dir set: CLBLAST_DIR\nIntel GPU acceleration linkIntel GPU acceleration is supported via SYCL.\nRequirements: Intel oneAPI Base Toolkit (see also llama.cpp setup installations instructions)\nmake BUILD_TYPE=sycl_f16 build # for float16 make BUILD_TYPE=sycl_f32 build # for float32 Metal (Apple Silicon) link make build # correct build type is automatically used on mac (BUILD_TYPE=metal) # Set `gpu_layers: 256` (or equal to the number of model layers) to your YAML model config file and `f16: true` Windows compatibility linkMake sure to give enough resources to the running container. See https://github.com/go-skynet/LocalAI/issues/2\nExamples linkMore advanced build options are available, for instance to build only a single backend.\nBuild only a single backend linkYou can control the backends that are built by setting the GRPC_BACKENDS environment variable. For instance, to build only the llama-cpp backend only:\nmake GRPC_BACKENDS=backend-assets/grpc/llama-cpp build By default, all the backends are built.\nSpecific llama.cpp version linkTo build with a specific version of llama.cpp, set CPPLLAMA_VERSION to the tag or wanted sha:\nCPPLLAMA_VERSION= make build "
            }
        );
    index.add(
            {
                id:  7 ,
                href: "\/basics\/container\/",
                title: "Run with container images",
                description: "LocalAI provides a variety of images to support different environments. These images are available on quay.io and Docker Hub.\nAll-in-One images comes with a pre-configured set of models and backends, standard images instead do not have any model pre-configured and installed.\nFor GPU Acceleration support for Nvidia video graphic cards, use the Nvidia/CUDA images, if you don’t have a GPU, use the CPU images. If you have AMD or Mac Silicon, see the build section.",
                content: "LocalAI provides a variety of images to support different environments. These images are available on quay.io and Docker Hub.\nAll-in-One images comes with a pre-configured set of models and backends, standard images instead do not have any model pre-configured and installed.\nFor GPU Acceleration support for Nvidia video graphic cards, use the Nvidia/CUDA images, if you don’t have a GPU, use the CPU images. If you have AMD or Mac Silicon, see the build section.\n💡\nAvailable Images Types:\nImages ending with -core are smaller images without predownload python dependencies. Use these images if you plan to use llama.cpp, stablediffusion-ncn or rwkv backends - if you are not sure which one to use, do not use these images.\nImages containing the aio tag are all-in-one images with all the features enabled, and come with an opinionated set of configuration.\nFFMpeg is not included in the default images due to its licensing. If you need FFMpeg, use the images ending with -ffmpeg. Note that ffmpeg is needed in case of using audio-to-text LocalAI’s features.\nIf using old and outdated CPUs and no GPUs you might need to set REBUILD to true as environment variable along with options to disable the flags which your CPU does not support, however note that inference will perform poorly and slow. See also flagset compatibility.\nPrerequisites linkBefore you begin, ensure you have a container engine installed if you are not using the binaries. Suitable options include Docker or Podman. For installation instructions, refer to the following guides:\nInstall Docker Desktop (Mac, Windows, Linux) Install Podman (Linux) Install Docker engine (Servers) 💡\nHardware Requirements: The hardware requirements for LocalAI vary based on the model size and quantization method used. For performance benchmarks with different backends, such as llama.cpp, visit this link. The rwkv backend is noted for its lower resource consumption.\nAll-in-one images linkAll-In-One images are images that come pre-configured with a set of models and backends to fully leverage almost all the LocalAI featureset. These images are available for both CPU and GPU environments. The AIO images are designed to be easy to use and requires no configuration. Models configuration can be found here separated by size.\nIn the AIO images there are models configured with the names of OpenAI models, however, they are really backed by Open Source models. You can find the table below\nCategory Model name Real model (CPU) Real model (GPU) Text Generation gpt-4 phi-2 hermes-2-pro-mistral Multimodal Vision gpt-4-vision-preview bakllava llava-1.6-mistral Image Generation stablediffusion stablediffusion dreamshaper-8 Speech to Text whisper-1 whisper with whisper-base model \u003c= same Text to Speech tts-1 en-us-amy-low.onnx from rhasspy/piper \u003c= same Embeddings text-embedding-ada-002 all-MiniLM-L6-v2 in Q4 all-MiniLM-L6-v2 Usage linkSelect the image (CPU or GPU) and start the container with Docker:\n# CPU example docker run -p 8080:8080 --name local-ai -ti localai/localai:latest-aio-cpu # For Nvidia GPUs: # docker run -p 8080:8080 --gpus all --name local-ai -ti localai/localai:latest-aio-gpu-nvidia-cuda-11 # docker run -p 8080:8080 --gpus all --name local-ai -ti localai/localai:latest-aio-gpu-nvidia-cuda-12 LocalAI will automatically download all the required models, and the API will be available at localhost:8080.\nOr with a docker-compose file:\nversion: \"3.9\" services: api: image: localai/localai:latest-aio-cpu # For a specific version: # image: localai/localai:v2.27.0-aio-cpu # For Nvidia GPUs decomment one of the following (cuda11 or cuda12): # image: localai/localai:v2.27.0-aio-gpu-nvidia-cuda-11 # image: localai/localai:v2.27.0-aio-gpu-nvidia-cuda-12 # image: localai/localai:latest-aio-gpu-nvidia-cuda-11 # image: localai/localai:latest-aio-gpu-nvidia-cuda-12 healthcheck: test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8080/readyz\"] interval: 1m timeout: 20m retries: 5 ports: - 8080:8080 environment: - DEBUG=true # ... volumes: - ./models:/build/models:cached # decomment the following piece if running with Nvidia GPUs # deploy: # resources: # reservations: # devices: # - driver: nvidia # count: 1 # capabilities: [gpu] 💡\nModels caching: The AIO image will download the needed models on the first run if not already present and store those in /build/models inside the container. The AIO models will be automatically updated with new versions of AIO images.\nYou can change the directory inside the container by specifying a MODELS_PATH environment variable (or --models-path).\nIf you want to use a named model or a local directory, you can mount it as a volume to /build/models:\ndocker run -p 8080:8080 --name local-ai -ti -v $PWD/models:/build/models localai/localai:latest-aio-cpu or associate a volume:\ndocker volume create localai-models docker run -p 8080:8080 --name local-ai -ti -v localai-models:/build/models localai/localai:latest-aio-cpu Available AIO images link Description Quay Docker Hub Latest images for CPU quay.io/go-skynet/local-ai:latest-aio-cpu localai/localai:latest-aio-cpu Versioned image (e.g. for CPU) quay.io/go-skynet/local-ai:v2.27.0-aio-cpu localai/localai:v2.27.0-aio-cpu Latest images for Nvidia GPU (CUDA11) quay.io/go-skynet/local-ai:latest-aio-gpu-nvidia-cuda-11 localai/localai:latest-aio-gpu-nvidia-cuda-11 Latest images for Nvidia GPU (CUDA12) quay.io/go-skynet/local-ai:latest-aio-gpu-nvidia-cuda-12 localai/localai:latest-aio-gpu-nvidia-cuda-12 Latest images for AMD GPU quay.io/go-skynet/local-ai:latest-aio-gpu-hipblas localai/localai:latest-aio-gpu-hipblas Latest images for Intel GPU (sycl f16) quay.io/go-skynet/local-ai:latest-aio-gpu-intel-f16 localai/localai:latest-aio-gpu-intel-f16 Latest images for Intel GPU (sycl f32) quay.io/go-skynet/local-ai:latest-aio-gpu-intel-f32 localai/localai:latest-aio-gpu-intel-f32 Available environment variables linkThe AIO Images are inheriting the same environment variables as the base images and the environment of LocalAI (that you can inspect by calling --help). However, it supports additional environment variables available only from the container image\nVariable Default Description PROFILE Auto-detected The size of the model to use. Available: cpu, gpu-8g MODELS Auto-detected A list of models YAML Configuration file URI/URL (see also "
            }
        );
    index.add(
            {
                id:  8 ,
                href: "\/basics\/kubernetes\/",
                title: "Run with Kubernetes",
                description: "For installing LocalAI in Kubernetes, the deployment file from the examples can be used and customized as prefered: kubectl apply -f https://raw.githubusercontent.com/mudler/LocalAI-examples/refs/heads/main/kubernetes/deployment.yaml For Nvidia GPUs: kubectl apply -f https://raw.githubusercontent.com/mudler/LocalAI-examples/refs/heads/main/kubernetes/deployment-nvidia.yaml Alternatively, the helm chart can be used as well: # Install the helm repository helm repo add go-skynet https://go-skynet.github.io/helm-charts/ # Update the repositories helm repo update # Get the values helm show values go-skynet/local-ai \u003e values.yaml # Edit the values value if needed # vim values.",
                content: "For installing LocalAI in Kubernetes, the deployment file from the examples can be used and customized as prefered:\nkubectl apply -f https://raw.githubusercontent.com/mudler/LocalAI-examples/refs/heads/main/kubernetes/deployment.yaml For Nvidia GPUs:\nkubectl apply -f https://raw.githubusercontent.com/mudler/LocalAI-examples/refs/heads/main/kubernetes/deployment-nvidia.yaml Alternatively, the helm chart can be used as well:\n# Install the helm repository helm repo add go-skynet https://go-skynet.github.io/helm-charts/ # Update the repositories helm repo update # Get the values helm show values go-skynet/local-ai \u003e values.yaml # Edit the values value if needed # vim values.yaml ... # Install the helm chart helm install local-ai go-skynet/local-ai -f values.yaml "
            }
        );
    index.add(
            {
                id:  9 ,
                href: "\/basics\/news\/",
                title: "News",
                description: "Release notes have been now moved completely over Github releases.\nYou can see the release notes here.\nOlder release notes link04-12-2023: v2.0.0 linkThis release brings a major overhaul in some backends.\nBreaking/important changes:\nBackend rename: llama-stable renamed to llama-ggml 1287 Prompt template changes: 1254 (extra space in roles) Apple metal bugfixes: 1365 New:\nAdded support for LLaVa and OpenAI Vision API support ( 1254 ) Python based backends are now using conda to track env dependencies ( 1144 ) Support for parallel requests ( 1290 ) Support for transformers-embeddings ( 1308 ) Watchdog for backends ( 1341 ).",
                content: "Release notes have been now moved completely over Github releases.\nYou can see the release notes here.\nOlder release notes link04-12-2023: v2.0.0 linkThis release brings a major overhaul in some backends.\nBreaking/important changes:\nBackend rename: llama-stable renamed to llama-ggml 1287 Prompt template changes: 1254 (extra space in roles) Apple metal bugfixes: 1365 New:\nAdded support for LLaVa and OpenAI Vision API support ( 1254 ) Python based backends are now using conda to track env dependencies ( 1144 ) Support for parallel requests ( 1290 ) Support for transformers-embeddings ( 1308 ) Watchdog for backends ( 1341 ). As https://github.com/ggerganov/llama.cpp/issues/3969 is hitting LocalAI’s llama-cpp implementation, we have now a watchdog that can be used to make sure backends are not stalling. This is a generic mechanism that can be enabled for all the backends now. Whisper.cpp updates ( 1302 ) Petals backend ( 1350 ) Full LLM fine-tuning example to use with LocalAI: https://localai.io/advanced/fine-tuning/ Due to the python dependencies size of images grew in size. If you still want to use smaller images without python dependencies, you can use the corresponding images tags ending with -core.\nFull changelog: https://github.com/mudler/LocalAI/releases/tag/v2.0.0\n30-10-2023: v1.40.0 linkThis release is a preparation before v2 - the efforts now will be to refactor, polish and add new backends. Follow up on: https://github.com/mudler/LocalAI/issues/1126\nHot topics linkThis release now brings the llama-cpp backend which is a c++ backend tied to llama.cpp. It follows more closely and tracks recent versions of llama.cpp. It is not feature compatible with the current llama backend but plans are to sunset the current llama backend in favor of this one. This one will be probably be the latest release containing the older llama backend written in go and c++. The major improvement with this change is that there are less layers that could be expose to potential bugs - and as well it ease out maintenance as well.\nSupport for ROCm/HIPBLAS linkThis release bring support for AMD thanks to @65a . See more details in 1100 More CLI commands linkThanks to @jespino now the local-ai binary has more subcommands allowing to manage the gallery or try out directly inferencing, check it out!\nRelease notes\n25-09-2023: v1.30.0 linkThis is an exciting LocalAI release! Besides bug-fixes and enhancements this release brings the new backend to a whole new level by extending support to vllm and vall-e-x for audio generation!\nCheck out the documentation for vllm here and Vall-E-X here\nRelease notes\n26-08-2023: v1.25.0 linkHey everyone, Ettore here, I’m so happy to share this release out - while this summer is hot apparently doesn’t stop LocalAI development :)\nThis release brings a lot of new features, bugfixes and updates! Also a big shout out to the community, this was a great release!\nAttention 🚨 linkFrom this release the llama backend supports only gguf files (see 943 ). LocalAI however still supports ggml files. We ship a version of llama.cpp before that change in a separate backend, named llama-stable to allow still loading ggml files. If you were specifying the llama backend manually to load ggml files from this release you should use llama-stable instead, or do not specify a backend at all (LocalAI will automatically handle this).\nImage generation enhancements linkThe Diffusers backend got now various enhancements, including support to generate images from images, longer prompts, and support for more kernels schedulers. See the Diffusers documentation for more information.\nLora adapters linkNow it’s possible to load lora adapters for llama.cpp. See 955 for more information.\nDevice management linkIt is now possible for single-devices with one GPU to specify --single-active-backend to allow only one backend active at the time 925 .\nCommunity spotlight linkResources management linkThanks to the continous community efforts (another cool contribution from dave-gray101 ) now it’s possible to shutdown a backend programmatically via the API. There is an ongoing effort in the community to better handling of resources. See also the 🔥Roadmap.\nNew how-to section linkThanks to the community efforts now we have a new how-to website with various examples on how to use LocalAI. This is a great starting point for new users! We are currently working on improving it, a huge shout out to lunamidori5 from the community for the impressive efforts on this!\n💡 More examples! link Open source autopilot? See the new addition by gruberdev in our examples on how to use Continue with LocalAI! Want to try LocalAI with Insomnia? Check out the new Insomnia example by dave-gray101 ! LocalAGI in discord! linkDid you know that we have now few cool bots in our Discord? come check them out! We also have an instance of LocalAGI ready to help you out!\nChangelog summary linkBreaking Changes 🛠 link feat: bump llama.cpp, add gguf support by mudler in 943 Exciting New Features 🎉 link feat(Makefile): allow to restrict backend builds by mudler in 890 feat(diffusers): various enhancements by mudler in 895 feat: make initializer accept gRPC delay times by mudler in 900 feat(diffusers): add DPMSolverMultistepScheduler++, DPMSolverMultistepSchedulerSDE++, guidance_scale by mudler in 903 feat(diffusers): overcome prompt limit by mudler in 904 feat(diffusers): add img2img and clip_skip, support more kernels schedulers by mudler in 906 Usage Features by dave-gray101 in 863 feat(diffusers): be consistent with pipelines, support also depthimg2img by mudler in 926 feat: add –single-active-backend to allow only one backend active at the time by mudler in 925 feat: add llama-stable backend by mudler in 932 feat: allow to customize rwkv tokenizer by dave-gray101 in 937 feat: backend monitor shutdown endpoint, process based by dave-gray101 in 938 feat: Allow to load lora adapters for llama.cpp by mudler in 955 Join our Discord community! our vibrant community is growing fast, and we are always happy to help! https://discord.gg/uJAeKSAGDy\nThe full changelog is available here.\n🔥🔥🔥🔥 12-08-2023: v1.24.0 🔥🔥🔥🔥 linkThis is release brings four(!) new additional backends to LocalAI: 🐶 Bark, 🦙 AutoGPTQ, 🧨 Diffusers, 🦙 exllama and a lot of improvements!\nMajor improvements: link feat: add bark and AutoGPTQ by mudler in 871 feat: Add Diffusers by mudler in 874 feat: add API_KEY list support by neboman11 and bnusunny in 877 feat: Add exllama by mudler in 881 feat: pre-configure LocalAI galleries by mudler in 886 🐶 Bark linkBark is a text-prompted generative audio model - it combines GPT techniques to generate Audio from text. It is a great addition to LocalAI, and it’s available in the container images by default.\nIt can also generate music, see the example: lion.webm\n🦙 AutoGPTQ linkAutoGPTQ is an easy-to-use LLMs quantization package with user-friendly apis, based on GPTQ algorithm.\nIt is targeted mainly for GPU usage only. Check out the documentation for usage.\n🦙 Exllama linkExllama is a “A more memory-efficient rewrite of the HF transformers implementation of Llama for use with quantized weights”. It is a faster alternative to run LLaMA models on GPU.Check out the Exllama documentation for usage.\n🧨 Diffusers linkDiffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Currently it is experimental, and supports generation only of images so you might encounter some issues on models which weren’t tested yet. Check out the Diffusers documentation for usage.\n🔑 API Keys linkThanks to the community contributions now it’s possible to specify a list of API keys that can be used to gate API requests.\nAPI Keys can be specified with the API_KEY environment variable as a comma-separated list of keys.\n🖼️ Galleries linkNow by default the model-gallery repositories are configured in the container images\n💡 New project linkLocalAGI is a simple agent that uses LocalAI functions to have a full locally runnable assistant (with no API keys needed).\nSee it here in action planning a trip for San Francisco!\nThe full changelog is available here.\n🔥🔥 29-07-2023: v1.23.0 🚀 linkThis release focuses mostly on bugfixing and updates, with just a couple of new features:\nfeat: add rope settings and negative prompt, drop grammar backend by mudler in 797 Added CPU information to entrypoint.sh by @finger42 in 794 feat: cancel stream generation if client disappears by @tmm1 in 792 Most notably, this release brings important fixes for CUDA (and not only):\nfix: add rope settings during model load, fix CUDA by mudler in 821 fix: select function calls if ’name’ is set in the request by mudler in 827 fix: symlink libphonemize in the container by mudler in 831 notifications From this release OpenAI functions are available in the llama backend. The llama-grammar has been deprecated. See also OpenAI functions.\nThe full changelog is available here\n🔥🔥🔥 23-07-2023: v1.22.0 🚀 link feat: add llama-master backend by mudler in 752 [build] pass build type to cmake on libtransformers.a build by @TonDar0n in 741 feat: resolve JSONSchema refs (planners) by mudler in 774 feat: backends improvements by mudler in 778 feat(llama2): add template for chat messages by dave-gray101 in 782 notifications From this release to use the OpenAI functions you need to use the llama-grammar backend. It has been added a llama backend for tracking llama.cpp master and llama-grammar for the grammar functionalities that have not been merged yet upstream. See also OpenAI functions. Until the feature is merged we will have two llama backends.\nHuggingface embeddings linkIn this release is now possible to specify to LocalAI external gRPC backends that can be used for inferencing 778 . It is now possible to write internal backends in any language, and a huggingface-embeddings backend is now available in the container image to be used with https://github.com/UKPLab/sentence-transformers. See also Embeddings.\nLLaMa 2 has been released! linkThanks to the community effort now LocalAI supports templating for LLaMa2! more at: 782 until we update the model gallery with LLaMa2 models!\nOfficial langchain integration linkProgress has been made to support LocalAI with langchain. See: https://github.com/langchain-ai/langchain/pull/8134\n🔥🔥🔥 17-07-2023: v1.21.0 🚀 link [whisper] Partial support for verbose_json format in transcribe endpoint by @ldotlopez in 721 LocalAI functions by @mudler in 726 gRPC-based backends by @mudler in 743 falcon support (7b and 40b) with ggllm.cpp by @mudler in 743 LocalAI functions linkThis allows to run OpenAI functions as described in the OpenAI blog post and documentation: https://openai.com/blog/function-calling-and-other-api-updates.\nThis is a video of running the same example, locally with LocalAI: And here when it actually picks to reply to the user instead of using functions! Note: functions are supported only with llama.cpp-compatible models.\nA full example is available here: https://github.com/go-skynet/LocalAI/tree/master/examples/functions\ngRPC backends linkThis is an internal refactor which is not user-facing, however, it allows to ease out maintenance and addition of new backends to LocalAI!\nfalcon support linkNow Falcon 7b and 40b models compatible with https://github.com/cmp-nct/ggllm.cpp are supported as well.\nThe former, ggml-based backend has been renamed to falcon-ggml.\nDefault pre-compiled binaries linkFrom this release the default behavior of images has changed. Compilation is not triggered on start automatically, to recompile local-ai from scratch on start and switch back to the old behavior, you can set REBUILD=true in the environment variables. Rebuilding can be necessary if your CPU and/or architecture is old and the pre-compiled binaries are not compatible with your platform. See the build section for more information.\nFull release changelog\n🔥🔥🔥 28-06-2023: v1.20.0 🚀 linkExciting New Features 🎉 link Add Text-to-Audio generation with go-piper by mudler in 649 See API endpoints in our documentation. Add gallery repository by mudler in 663 . See models for documentation. Container images link Standard (GPT + stablediffusion): quay.io/go-skynet/local-ai:v1.20.0 FFmpeg: quay.io/go-skynet/local-ai:v1.20.0-ffmpeg CUDA 11+FFmpeg: quay.io/go-skynet/local-ai:v1.20.0-cublas-cuda11-ffmpeg CUDA 12+FFmpeg: quay.io/go-skynet/local-ai:v1.20.0-cublas-cuda12-ffmpeg Updates linkUpdates to llama.cpp, go-transformers, gpt4all.cpp and rwkv.cpp.\nThe NUMA option was enabled by mudler in 684 , along with many new parameters (mmap,mmlock, ..). See "
            }
        );
    index.add(
            {
                id:  10 ,
                href: "\/features\/",
                title: "Features",
                description: "",
                content: ""
            }
        );
    index.add(
            {
                id:  11 ,
                href: "\/features\/gpu-acceleration\/",
                title: "⚡ GPU acceleration",
                description: "warning Section under construction\nThis section contains instruction on how to use LocalAI with GPU acceleration.\n⚡\nFor accelleration for AMD or Metal HW is still in development, for additional details see the build\nModel configuration linkDepending on the model architecture and backend used, there might be different ways to enable GPU acceleration. It is required to configure the model you intend to use with a YAML config file. For example, for llama.",
                content: " warning Section under construction\nThis section contains instruction on how to use LocalAI with GPU acceleration.\n⚡\nFor accelleration for AMD or Metal HW is still in development, for additional details see the build\nModel configuration linkDepending on the model architecture and backend used, there might be different ways to enable GPU acceleration. It is required to configure the model you intend to use with a YAML config file. For example, for llama.cpp workloads a configuration file might look like this (where gpu_layers is the number of layers to offload to the GPU):\nname: my-model-name # Default model parameters parameters: # Relative to the models path model: llama.cpp-model.ggmlv3.q5_K_M.bin context_size: 1024 threads: 1 f16: true # enable with GPU acceleration gpu_layers: 22 # GPU Layers (only used when built with cublas) For diffusers instead, it might look like this instead:\nname: stablediffusion parameters: model: toonyou_beta6.safetensors backend: diffusers step: 30 f16: true diffusers: pipeline_type: StableDiffusionPipeline cuda: true enable_parameters: \"negative_prompt,num_inference_steps,clip_skip\" scheduler_type: \"k_dpmpp_sde\" CUDA(NVIDIA) acceleration linkRequirements linkRequirement: nvidia-container-toolkit (installation instructions 1 2)\nTo check what CUDA version do you need, you can either run nvidia-smi or nvcc --version.\nAlternatively, you can also check nvidia-smi with docker:\ndocker run --runtime=nvidia --rm nvidia/cuda nvidia-smi To use CUDA, use the images with the cublas tag, for example.\nThe image list is on quay:\nCUDA 11 tags: master-cublas-cuda11, v1.40.0-cublas-cuda11, … CUDA 12 tags: master-cublas-cuda12, v1.40.0-cublas-cuda12, … CUDA 11 + FFmpeg tags: master-cublas-cuda11-ffmpeg, v1.40.0-cublas-cuda11-ffmpeg, … CUDA 12 + FFmpeg tags: master-cublas-cuda12-ffmpeg, v1.40.0-cublas-cuda12-ffmpeg, … In addition to the commands to run LocalAI normally, you need to specify --gpus all to docker, for example:\ndocker run --rm -ti --gpus all -p 8080:8080 -e DEBUG=true -e MODELS_PATH=/models -e THREADS=1 -v $PWD/models:/models quay.io/go-skynet/local-ai:v1.40.0-cublas-cuda12 If the GPU inferencing is working, you should be able to see something like:\n5:22PM DBG Loading model in memory from file: /models/open-llama-7b-q4_0.bin ggml_init_cublas: found 1 CUDA devices: Device 0: Tesla T4 llama.cpp: loading model from /models/open-llama-7b-q4_0.bin llama_model_load_internal: format = ggjt v3 (latest) llama_model_load_internal: n_vocab = 32000 llama_model_load_internal: n_ctx = 1024 llama_model_load_internal: n_embd = 4096 llama_model_load_internal: n_mult = 256 llama_model_load_internal: n_head = 32 llama_model_load_internal: n_layer = 32 llama_model_load_internal: n_rot = 128 llama_model_load_internal: ftype = 2 (mostly Q4_0) llama_model_load_internal: n_ff = 11008 llama_model_load_internal: n_parts = 1 llama_model_load_internal: model size = 7B llama_model_load_internal: ggml ctx size = 0.07 MB llama_model_load_internal: using CUDA for GPU acceleration llama_model_load_internal: mem required = 4321.77 MB (+ 1026.00 MB per state) llama_model_load_internal: allocating batch_size x 1 MB = 512 MB VRAM for the scratch buffer llama_model_load_internal: offloading 10 repeating layers to GPU llama_model_load_internal: offloaded 10/35 layers to GPU llama_model_load_internal: total VRAM used: 1598 MB ................................................................................................... llama_init_from_file: kv self size = 512.00 MB ROCM(AMD) acceleration linkThere are a limited number of tested configurations for ROCm systems however most newer deditated GPU consumer grade devices seem to be supported under the current ROCm6 implementation.\nDue to the nature of ROCm it is best to run all implementations in containers as this limits the number of packages required for installation on host system, compatability and package versions for dependencies across all variations of OS must be tested independently if disired, please refer to the build documentation.\nRequirements link ROCm 6.x.x compatible GPU/accelerator OS: Ubuntu (22.04, 20.04), RHEL (9.3, 9.2, 8.9, 8.8), SLES (15.5, 15.4) Installed to host: amdgpu-dkms and rocm \u003e=6.0.0 as per ROCm documentation. Recommendations link Do not use on a system running Wayland. If running with Xorg do not use GPU assigned for compute for desktop rendering. Ensure at least 100GB of free space on disk hosting container runtime and storing images prior to installation. Limitations linkOngoing verification testing of ROCm compatability with integrated backends. Please note the following list of verified backends and devices.\nLocalAI hipblas images are built against the following targets: gfx900,gfx906,gfx908,gfx940,gfx941,gfx942,gfx90a,gfx1030,gfx1031,gfx1100,gfx1101\nIf your device is not one of these you must specify the corresponding GPU_TARGETS and specify REBUILD=true. Otherwise you don’t need to specify these in the commands below.\nVerified linkThe devices in the following list have been tested with hipblas images running ROCm 6.0.0\nBackend Verified Devices llama.cpp yes Radeon VII (gfx906) diffusers yes Radeon VII (gfx906) piper yes Radeon VII (gfx906) whisper no none autogptq no none bark no none coqui no none transformers no none exllama no none exllama2 no none mamba no none sentencetransformers no none transformers-musicgen no none vall-e-x no none vllm no none You can help by expanding this list.\nSystem Prep link Check your GPU LLVM target is compatible with the version of ROCm. This can be found in the LLVM Docs. Check which ROCm version is compatible with your LLVM target and your chosen OS (pay special attention to supported kernel versions). See the following for compatability for (ROCm 6.0.0) or (ROCm 6.0.2) Install you chosen version of the dkms and rocm (it is recommended that the native package manager be used for this process for any OS as version changes are executed more easily via this method if updates are required). Take care to restart after installing amdgpu-dkms and before installing rocm, for details regarding this see the installation documentation for your chosen OS (6.0.2 or 6.0.0) Deploy. Yes it’s that easy. Setup Example (Docker/containerd) linkThe following are examples of the ROCm specific configuration elements required.\n# docker-compose.yaml # For full functionality select a non-'core' image, version locking the image is recommended for debug purposes. image: quay.io/go-skynet/local-ai:master-aio-gpu-hipblas environment: - DEBUG=true # If your gpu is not already included in the current list of default targets the following build details are required. - REBUILD=true - BUILD_TYPE=hipblas - GPU_TARGETS=gfx906 # Example for Radeon VII devices: # AMD GPU only require the following devices be passed through to the container for offloading to occur. - /dev/dri - /dev/kfd The same can also be executed as a run for your container runtime\ndocker run \\ -e DEBUG=true \\ -e REBUILD=true \\ -e BUILD_TYPE=hipblas \\ -e GPU_TARGETS=gfx906 \\ --device /dev/dri \\ --device /dev/kfd \\ quay.io/go-skynet/local-ai:master-aio-gpu-hipblas Please ensure to add all other required environment variables, port forwardings, etc to your compose file or run command.\nThe rebuild process will take some time to complete when deploying these containers and it is recommended that you pull the image prior to deployment as depending on the version these images may be ~20GB in size.\nExample (k8s) (Advanced Deployment/WIP) linkFor k8s deployments there is an additional step required before deployment, this is the deployment of the ROCm/k8s-device-plugin. For any k8s environment the documentation provided by AMD from the ROCm project should be successful. It is recommended that if you use rke2 or OpenShift that you deploy the SUSE or RedHat provided version of this resource to ensure compatability. After this has been completed the helm chart from go-skynet can be configured and deployed mostly un-edited.\nThe following are details of the changes that should be made to ensure proper function. While these details may be configurable in the values.yaml development of this Helm chart is ongoing and is subject to change.\nThe following details indicate the final state of the localai deployment relevant to GPU function.\napiVersion: apps/v1 kind: Deployment metadata: name: {NAME}-local-ai ... spec: ... template: ... spec: containers: - env: - name: HIP_VISIBLE_DEVICES value: '0' # This variable indicates the devices availible to container (0:device1 1:device2 2:device3) etc. # For multiple devices (say device 1 and 3) the value would be equivelant to HIP_VISIBLE_DEVICES=\"0,2\" # Please take note of this when an iGPU is present in host system as compatability is not assured. ... resources: limits: amd.com/gpu: '1' requests: amd.com/gpu: '1' This configuration has been tested on a ‘custom’ cluster managed by SUSE Rancher that was deployed on top of Ubuntu 22.04.4, certification of other configuration is ongoing and compatability is not gauranteed.\nNotes link When installing the ROCM kernel driver on your system ensure that you are installing an equal or newer version that that which is currently implemented in LocalAI (6.0.0 at time of writing). AMD documentation indicates that this will ensure functionality however your milage may vary depending on the GPU and distro you are using. If you encounter an Error 413 on attempting to upload an audio file or image for whisper or llava/bakllava on a k8s deployment, note that the ingress for your deployment may require the annontation nginx.ingress.kubernetes.io/proxy-body-size: \"25m\" to allow larger uploads. This may be included in future versions of the helm chart. Intel acceleration (sycl) linkRequirements linkIf building from source, you need to install Intel oneAPI Base Toolkit and have the Intel drivers available in the system.\nContainer images linkTo use SYCL, use the images with the sycl-f16 or sycl-f32 tag, for example v2.27.0-sycl-f32-core, v2.27.0-sycl-f16-ffmpeg-core, …\nThe image list is on quay.\nExample linkTo run LocalAI with Docker and sycl starting phi-2, you can use the following command as an example:\ndocker run -e DEBUG=true --privileged -ti -v $PWD/models:/build/models -p 8080:8080 -v /dev/dri:/dev/dri --rm quay.io/go-skynet/local-ai:master-sycl-f32-ffmpeg-core phi-2 Notes linkIn addition to the commands to run LocalAI normally, you need to specify --device /dev/dri to docker, for example:\ndocker run --rm -ti --device /dev/dri -p 8080:8080 -e DEBUG=true -e MODELS_PATH=/models -e THREADS=1 -v $PWD/models:/models quay.io/go-skynet/local-ai:v2.27.0-sycl-f16-ffmpeg-core Note also that sycl does have a known issue to hang with mmap: true. You have to disable it in the model configuration if explicitly enabled.\n"
            }
        );
    index.add(
            {
                id:  12 ,
                href: "\/features\/text-generation\/",
                title: "📖 Text generation (GPT)",
                description: "LocalAI supports generating text with GPT with llama.cpp and other backends (such as rwkv.cpp as ) see also the Model compatibility for an up-to-date list of the supported model families.\nNote:\nYou can also specify the model name as part of the OpenAI token. If only one model is available, the API will use it for all the requests. API Reference linkChat completions linkhttps://platform.openai.com/docs/api-reference/chat\nFor example, to generate a chat completion, you can send a POST request to the /v1/chat/completions endpoint with the instruction as the request body:",
                content: "LocalAI supports generating text with GPT with llama.cpp and other backends (such as rwkv.cpp as ) see also the Model compatibility for an up-to-date list of the supported model families.\nNote:\nYou can also specify the model name as part of the OpenAI token. If only one model is available, the API will use it for all the requests. API Reference linkChat completions linkhttps://platform.openai.com/docs/api-reference/chat\nFor example, to generate a chat completion, you can send a POST request to the /v1/chat/completions endpoint with the instruction as the request body:\ncurl http://localhost:8080/v1/chat/completions -H \"Content-Type: application/json\" -d '{ \"model\": \"ggml-koala-7b-model-q4_0-r2.bin\", \"messages\": [{\"role\": \"user\", \"content\": \"Say this is a test!\"}], \"temperature\": 0.7 }' Available additional parameters: top_p, top_k, max_tokens\nEdit completions linkhttps://platform.openai.com/docs/api-reference/edits\nTo generate an edit completion you can send a POST request to the /v1/edits endpoint with the instruction as the request body:\ncurl http://localhost:8080/v1/edits -H \"Content-Type: application/json\" -d '{ \"model\": \"ggml-koala-7b-model-q4_0-r2.bin\", \"instruction\": \"rephrase\", \"input\": \"Black cat jumped out of the window\", \"temperature\": 0.7 }' Available additional parameters: top_p, top_k, max_tokens.\nCompletions linkhttps://platform.openai.com/docs/api-reference/completions\nTo generate a completion, you can send a POST request to the /v1/completions endpoint with the instruction as per the request body:\ncurl http://localhost:8080/v1/completions -H \"Content-Type: application/json\" -d '{ \"model\": \"ggml-koala-7b-model-q4_0-r2.bin\", \"prompt\": \"A long time ago in a galaxy far, far away\", \"temperature\": 0.7 }' Available additional parameters: top_p, top_k, max_tokens\nList models linkYou can list all the models available with:\ncurl http://localhost:8080/v1/models Backends linkAutoGPTQ linkAutoGPTQ is an easy-to-use LLMs quantization package with user-friendly apis, based on GPTQ algorithm.\nPrerequisites linkThis is an extra backend - in the container images is already available and there is nothing to do for the setup.\nIf you are building LocalAI locally, you need to install AutoGPTQ manually.\nModel setup linkThe models are automatically downloaded from huggingface if not present the first time. It is possible to define models via YAML config file, or just by querying the endpoint with the huggingface repository model name. For example, create a YAML config file in models/:\nname: orca backend: autogptq model_base_name: \"orca_mini_v2_13b-GPTQ-4bit-128g.no-act.order\" parameters: model: \"TheBloke/orca_mini_v2_13b-GPTQ\" # ... Test with:\ncurl http://localhost:8080/v1/chat/completions -H \"Content-Type: application/json\" -d '{ \"model\": \"orca\", \"messages\": [{\"role\": \"user\", \"content\": \"How are you?\"}], \"temperature\": 0.1 }' RWKV linkA full example on how to run a rwkv model is in the examples.\nNote: rwkv models needs to specify the backend rwkv in the YAML config files and have an associated tokenizer along that needs to be provided with it:\n36464540 -rw-r--r-- 1 mudler mudler 1.2G May 3 10:51 rwkv_small 36464543 -rw-r--r-- 1 mudler mudler 2.4M May 3 10:51 rwkv_small.tokenizer.json llama.cpp linkllama.cpp is a popular port of Facebook’s LLaMA model in C/C++.\nnotifications The ggml file format has been deprecated. If you are using ggml models and you are configuring your model with a YAML file, specify, use a LocalAI version older than v2.25.0. For gguf models, use the llama backend. The go backend is deprecated as well but still available as go-llama.\nFeatures linkThe llama.cpp model supports the following features:\n📖 Text generation (GPT) 🧠 Embeddings 🔥 OpenAI functions ✍️ Constrained grammars Setup linkLocalAI supports llama.cpp models out of the box. You can use the llama.cpp model in the same way as any other model.\nManual setup linkIt is sufficient to copy the ggml or gguf model files in the models folder. You can refer to the model in the model parameter in the API calls.\n"
            }
        );
    index.add(
            {
                id:  13 ,
                href: "\/features\/reranker\/",
                title: "📈 Reranker",
                description: "A reranking model, often referred to as a cross-encoder, is a core component in the two-stage retrieval systems used in information retrieval and natural language processing tasks. Given a query and a set of documents, it will output similarity scores.\nWe can use then the score to reorder the documents by relevance in our RAG system to increase its overall accuracy and filter out non-relevant results.\nLocalAI supports reranker models, and you can use them by using the rerankers backend, which uses rerankers.",
                content: "A reranking model, often referred to as a cross-encoder, is a core component in the two-stage retrieval systems used in information retrieval and natural language processing tasks. Given a query and a set of documents, it will output similarity scores.\nWe can use then the score to reorder the documents by relevance in our RAG system to increase its overall accuracy and filter out non-relevant results.\nLocalAI supports reranker models, and you can use them by using the rerankers backend, which uses rerankers.\nUsage linkYou can test rerankers by using container images with python (this does NOT work with core images) and a model config file like this, or by installing cross-encoder from the gallery in the UI:\nname: jina-reranker-v1-base-en backend: rerankers parameters: model: cross-encoder # optionally: # type: flashrank # diffusers: # pipeline_type: en # to specify the english language and test it with:\ncurl http://localhost:8080/v1/rerank \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"jina-reranker-v1-base-en\", \"query\": \"Organic skincare products for sensitive skin\", \"documents\": [ \"Eco-friendly kitchenware for modern homes\", \"Biodegradable cleaning supplies for eco-conscious consumers\", \"Organic cotton baby clothes for sensitive skin\", \"Natural organic skincare range for sensitive skin\", \"Tech gadgets for smart homes: 2024 edition\", \"Sustainable gardening tools and compost solutions\", \"Sensitive skin-friendly facial cleansers and toners\", \"Organic food wraps and storage solutions\", \"All-natural pet food for dogs with allergies\", \"Yoga mats made from recycled materials\" ], \"top_n\": 3 }' "
            }
        );
    index.add(
            {
                id:  14 ,
                href: "\/features\/text-to-audio\/",
                title: "🗣 Text to audio (TTS)",
                description: "API Compatibility linkThe LocalAI TTS API is compatible with the OpenAI TTS API and the Elevenlabs API.\nLocalAI API linkThe /tts endpoint can also be used to generate speech from text.\nUsage linkInput: input, model\nFor example, to generate an audio file, you can send a POST request to the /tts endpoint with the instruction as the request body:\ncurl http://localhost:8080/tts -H \"Content-Type: application/json\" -d '{ \"input\": \"Hello world\", \"model\": \"tts\" }' Returns an audio/wav file.",
                content: "API Compatibility linkThe LocalAI TTS API is compatible with the OpenAI TTS API and the Elevenlabs API.\nLocalAI API linkThe /tts endpoint can also be used to generate speech from text.\nUsage linkInput: input, model\nFor example, to generate an audio file, you can send a POST request to the /tts endpoint with the instruction as the request body:\ncurl http://localhost:8080/tts -H \"Content-Type: application/json\" -d '{ \"input\": \"Hello world\", \"model\": \"tts\" }' Returns an audio/wav file.\nBackends link🐸 Coqui linkRequired: Don’t use LocalAI images ending with the -core tag,. Python dependencies are required in order to use this backend.\nCoqui works without any configuration, to test it, you can run the following curl command:\ncurl http://localhost:8080/tts -H \"Content-Type: application/json\" -d '{ \"backend\": \"coqui\", \"model\": \"tts_models/en/ljspeech/glow-tts\", \"input\":\"Hello, this is a test!\" }' You can use the env variable COQUI_LANGUAGE to set the language used by the coqui backend.\nYou can also use config files to configure tts models (see section below on how to use config files).\nBark linkBark allows to generate audio from text prompts.\nThis is an extra backend - in the container is already available and there is nothing to do for the setup.\nModel setup linkThere is nothing to be done for the model setup. You can already start to use bark. The models will be downloaded the first time you use the backend.\nUsage linkUse the tts endpoint by specifying the bark backend:\ncurl http://localhost:8080/tts -H \"Content-Type: application/json\" -d '{ \"backend\": \"bark\", \"input\":\"Hello!\" }' | aplay To specify a voice from https://github.com/suno-ai/bark#-voice-presets ( https://suno-ai.notion.site/8b8e8749ed514b0cbf3f699013548683?v=bc67cff786b04b50b3ceb756fd05f68c ), use the model parameter:\ncurl http://localhost:8080/tts -H \"Content-Type: application/json\" -d '{ \"backend\": \"bark\", \"input\":\"Hello!\", \"model\": \"v2/en_speaker_4\" }' | aplay Piper linkTo install the piper audio models manually:\nDownload Voices from https://github.com/rhasspy/piper/releases/tag/v0.0.2 Extract the .tar.tgz files (.onnx,.json) inside models Run the following command to test the model is working To use the tts endpoint, run the following command. You can specify a backend with the backend parameter. For example, to use the piper backend:\ncurl http://localhost:8080/tts -H \"Content-Type: application/json\" -d '{ \"model\":\"it-riccardo_fasol-x-low.onnx\", \"backend\": \"piper\", \"input\": \"Ciao, sono Ettore\" }' | aplay Note:\naplay is a Linux command. You can use other tools to play the audio file. The model name is the filename with the extension. The model name is case sensitive. LocalAI must be compiled with the GO_TAGS=tts flag. Transformers-musicgen linkLocalAI also has experimental support for transformers-musicgen for the generation of short musical compositions. Currently, this is implemented via the same requests used for text to speech:\ncurl --request POST \\ --url http://localhost:8080/tts \\ --header 'Content-Type: application/json' \\ --data '{ \"backend\": \"transformers-musicgen\", \"model\": \"facebook/musicgen-medium\", \"input\": \"Cello Rave\" }' | aplay Future versions of LocalAI will expose additional control over audio generation beyond the text prompt.\nVall-E-X linkVALL-E-X is an open source implementation of Microsoft’s VALL-E X zero-shot TTS model.\nSetup linkThe backend will automatically download the required files in order to run the model.\nThis is an extra backend - in the container is already available and there is nothing to do for the setup. If you are building manually, you need to install Vall-E-X manually first.\nUsage linkUse the tts endpoint by specifying the vall-e-x backend:\ncurl http://localhost:8080/tts -H \"Content-Type: application/json\" -d '{ \"backend\": \"vall-e-x\", \"input\":\"Hello!\" }' | aplay Voice cloning linkIn order to use voice cloning capabilities you must create a YAML configuration file to setup a model:\nname: cloned-voice backend: vall-e-x parameters: model: \"cloned-voice\" tts: vall-e: # The path to the audio file to be cloned # relative to the models directory # Max 15s audio_path: \"audio-sample.wav\" Then you can specify the model name in the requests:\ncurl http://localhost:8080/tts -H \"Content-Type: application/json\" -d '{ \"model\": \"cloned-voice\", \"input\":\"Hello!\" }' | aplay Parler-tts linkparler-tts. It is possible to install and configure the model directly from the gallery. https://github.com/huggingface/parler-tts\nUsing config files linkYou can also use a config-file to specify TTS models and their parameters.\nIn the following example we define a custom config to load the xtts_v2 model, and specify a voice and language.\nname: xtts_v2 backend: coqui parameters: language: fr model: tts_models/multilingual/multi-dataset/xtts_v2 tts: voice: Ana Florence With this config, you can now use the following curl command to generate a text-to-speech audio file:\ncurl -L http://localhost:8080/tts \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"xtts_v2\", \"input\": \"Bonjour, je suis Ana Florence. Comment puis-je vous aider?\" }' | aplay Response format linkTo provide some compatibility with OpenAI API regarding response_format, ffmpeg must be installed (or a docker image including ffmpeg used) to leverage converting the generated wav file before the api provide its response.\nWarning regarding a change in behaviour. Before this addition, the parameter was ignored and a wav file was always returned, with potential codec errors later in the integration (like trying to decode a mp3 file from a wav, which is the default format used by OpenAI)\nSupported format thanks to ffmpeg are wav, mp3, aac, flac, opus, defaulting to wav if an unknown or no format is provided.\ncurl http://localhost:8080/tts -H \"Content-Type: application/json\" -d '{ \"input\": \"Hello world\", \"model\": \"tts\", \"response_format\": \"mp3\" }' If a response_format is added in the query (other than wav) and ffmpeg is not available, the call will fail.\n"
            }
        );
    index.add(
            {
                id:  15 ,
                href: "\/features\/image-generation\/",
                title: "🎨 Image generation",
                description: "(Generated with AnimagineXL)\nLocalAI supports generating images with Stable diffusion, running on CPU using C++ and Python implementations.\nUsage linkOpenAI docs: https://platform.openai.com/docs/api-reference/images/create\nTo generate an image you can send a POST request to the /v1/images/generations endpoint with the instruction as the request body:\n# 512x512 is supported too curl http://localhost:8080/v1/images/generations -H \"Content-Type: application/json\" -d '{ \"prompt\": \"A cute baby sea otter\", \"size\": \"256x256\" }' Available additional parameters: mode, step.",
                content: " (Generated with AnimagineXL)\nLocalAI supports generating images with Stable diffusion, running on CPU using C++ and Python implementations.\nUsage linkOpenAI docs: https://platform.openai.com/docs/api-reference/images/create\nTo generate an image you can send a POST request to the /v1/images/generations endpoint with the instruction as the request body:\n# 512x512 is supported too curl http://localhost:8080/v1/images/generations -H \"Content-Type: application/json\" -d '{ \"prompt\": \"A cute baby sea otter\", \"size\": \"256x256\" }' Available additional parameters: mode, step.\nNote: To set a negative prompt, you can split the prompt with |, for instance: a cute baby sea otter|malformed.\ncurl http://localhost:8080/v1/images/generations -H \"Content-Type: application/json\" -d '{ \"prompt\": \"floating hair, portrait, ((loli)), ((one girl)), cute face, hidden hands, asymmetrical bangs, beautiful detailed eyes, eye shadow, hair ornament, ribbons, bowties, buttons, pleated skirt, (((masterpiece))), ((best quality)), colorful|((part of the head)), ((((mutated hands and fingers)))), deformed, blurry, bad anatomy, disfigured, poorly drawn face, mutation, mutated, extra limb, ugly, poorly drawn hands, missing limb, blurry, floating limbs, disconnected limbs, malformed hands, blur, out of focus, long neck, long body, Octane renderer, lowres, bad anatomy, bad hands, text\", \"size\": \"256x256\" }' Backends linkstablediffusion-ggml linkThis backend is based on stable-diffusion.cpp. Every model supported by that backend is suppoerted indeed with LocalAI.\nSetup linkThere are already several models in the gallery that are available to install and get up and running with this backend, you can for example run flux by searching it in the Model gallery (flux.1-dev-ggml) or start LocalAI with run:\nlocal-ai run flux.1-dev-ggml To use a custom model, you can follow these steps:\nCreate a model file stablediffusion.yaml in the models folder: name: stablediffusion backend: stablediffusion-ggml parameters: model: gguf_model.gguf step: 25 cfg_scale: 4.5 options: - \"clip_l_path:clip_l.safetensors\" - \"clip_g_path:clip_g.safetensors\" - \"t5xxl_path:t5xxl-Q5_0.gguf\" - \"sampler:euler\" Download the required assets to the models repository Start LocalAI Diffusers linkDiffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. LocalAI has a diffusers backend which allows image generation using the diffusers library.\n(Generated with AnimagineXL)\nModel setup linkThe models will be downloaded the first time you use the backend from huggingface automatically.\nCreate a model configuration file in the models directory, for instance to use Linaqruf/animagine-xl with CPU:\nname: animagine-xl parameters: model: Linaqruf/animagine-xl backend: diffusers # Force CPU usage - set to true for GPU f16: false diffusers: cuda: false # Enable for GPU usage (CUDA) scheduler_type: euler_a Dependencies linkThis is an extra backend - in the container is already available and there is nothing to do for the setup. Do not use core images (ending with -core). If you are building manually, see the build instructions.\nModel setup linkThe models will be downloaded the first time you use the backend from huggingface automatically.\nCreate a model configuration file in the models directory, for instance to use Linaqruf/animagine-xl with CPU:\nname: animagine-xl parameters: model: Linaqruf/animagine-xl backend: diffusers cuda: true f16: true diffusers: scheduler_type: euler_a Local models linkYou can also use local models, or modify some parameters like clip_skip, scheduler_type, for instance:\nname: stablediffusion parameters: model: toonyou_beta6.safetensors backend: diffusers step: 30 f16: true cuda: true diffusers: pipeline_type: StableDiffusionPipeline enable_parameters: \"negative_prompt,num_inference_steps,clip_skip\" scheduler_type: \"k_dpmpp_sde\" clip_skip: 11 cfg_scale: 8 Configuration parameters linkThe following parameters are available in the configuration file:\nParameter Description Default f16 Force the usage of float16 instead of float32 false step Number of steps to run the model for 30 cuda Enable CUDA acceleration false enable_parameters Parameters to enable for the model negative_prompt,num_inference_steps,clip_skip scheduler_type Scheduler type k_dpp_sde cfg_scale Configuration scale 8 clip_skip Clip skip None pipeline_type Pipeline type AutoPipelineForText2Image lora_adapters A list of lora adapters (file names relative to model directory) to apply None lora_scales A list of lora scales (floats) to apply None There are available several types of schedulers:\nScheduler Description ddim DDIM pndm PNDM heun Heun unipc UniPC euler Euler euler_a Euler a lms LMS k_lms LMS Karras dpm_2 DPM2 k_dpm_2 DPM2 Karras dpm_2_a DPM2 a k_dpm_2_a DPM2 a Karras dpmpp_2m DPM++ 2M k_dpmpp_2m DPM++ 2M Karras dpmpp_sde DPM++ SDE k_dpmpp_sde DPM++ SDE Karras dpmpp_2m_sde DPM++ 2M SDE k_dpmpp_2m_sde DPM++ 2M SDE Karras Pipelines types available:\nPipeline type Description StableDiffusionPipeline Stable diffusion pipeline StableDiffusionImg2ImgPipeline Stable diffusion image to image pipeline StableDiffusionDepth2ImgPipeline Stable diffusion depth to image pipeline DiffusionPipeline Diffusion pipeline StableDiffusionXLPipeline Stable diffusion XL pipeline StableVideoDiffusionPipeline Stable video diffusion pipeline AutoPipelineForText2Image Automatic detection pipeline for text to image VideoDiffusionPipeline Video diffusion pipeline StableDiffusion3Pipeline Stable diffusion 3 pipeline FluxPipeline Flux pipeline FluxTransformer2DModel Flux transformer 2D model SanaPipeline Sana pipeline Advanced: Additional parameters linkAdditional arbitrarly parameters can be specified in the option field in key/value separated by ::\nname: animagine-xl # ... options: - \"cfg_scale:6\" Note: There is no complete parameter list. Any parameter can be passed arbitrarly and is passed to the model directly as argument to the pipeline. Different pipelines/implementations support different parameters.\nThe example above, will result in the following python code when generating images:\npipe( prompt=\"A cute baby sea otter\", # Options passed via API size=\"256x256\", # Options passed via API cfg_scale=6 # Additional parameter passed via configuration file ) Usage linkText to Image linkUse the image generation endpoint with the model name from the configuration file:\ncurl http://localhost:8080/v1/images/generations \\ -H \"Content-Type: application/json\" \\ -d '{ \"prompt\": \"|\", \"model\": \"animagine-xl\", \"step\": 51, \"size\": \"1024x1024\" }' Image to Image linkhttps://huggingface.co/docs/diffusers/using-diffusers/img2img\nAn example model (GPU):\nname: stablediffusion-edit parameters: model: nitrosocke/Ghibli-Diffusion backend: diffusers step: 25 cuda: true f16: true diffusers: pipeline_type: StableDiffusionImg2ImgPipeline enable_parameters: \"negative_prompt,num_inference_steps,image\" IMAGE_PATH=/path/to/your/image (echo -n '{\"file\": \"'; base64 $IMAGE_PATH; echo '\", \"prompt\": \"a sky background\",\"size\": \"512x512\",\"model\":\"stablediffusion-edit\"}') | curl -H \"Content-Type: application/json\" -d @- http://localhost:8080/v1/images/generations Depth to Image linkhttps://huggingface.co/docs/diffusers/using-diffusers/depth2img\nname: stablediffusion-depth parameters: model: stabilityai/stable-diffusion-2-depth backend: diffusers step: 50 # Force CPU usage f16: true cuda: true diffusers: pipeline_type: StableDiffusionDepth2ImgPipeline enable_parameters: \"negative_prompt,num_inference_steps,image\" cfg_scale: 6 (echo -n '{\"file\": \"'; base64 ~/path/to/image.jpeg; echo '\", \"prompt\": \"a sky background\",\"size\": \"512x512\",\"model\":\"stablediffusion-depth\"}') | curl -H \"Content-Type: application/json\" -d @- http://localhost:8080/v1/images/generations img2vid link name: img2vid parameters: model: stabilityai/stable-video-diffusion-img2vid backend: diffusers step: 25 # Force CPU usage f16: true cuda: true diffusers: pipeline_type: StableVideoDiffusionPipeline (echo -n '{\"file\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/svd/rocket.png?download=true\",\"size\": \"512x512\",\"model\":\"img2vid\"}') | curl -H \"Content-Type: application/json\" -X POST -d @- http://localhost:8080/v1/images/generations txt2vid link name: txt2vid parameters: model: damo-vilab/text-to-video-ms-1.7b backend: diffusers step: 25 # Force CPU usage f16: true cuda: true diffusers: pipeline_type: VideoDiffusionPipeline cuda: true (echo -n '{\"prompt\": \"spiderman surfing\",\"size\": \"512x512\",\"model\":\"txt2vid\"}') | curl -H \"Content-Type: application/json\" -X POST -d @- http://localhost:8080/v1/images/generations "
            }
        );
    index.add(
            {
                id:  16 ,
                href: "\/features\/embeddings\/",
                title: "🧠 Embeddings",
                description: "LocalAI supports generating embeddings for text or list of tokens. For the API documentation you can refer to the OpenAI docs: https://platform.openai.com/docs/api-reference/embeddings Model compatibility linkThe embedding endpoint is compatible with llama.cpp models, bert.cpp models and sentence-transformers models available in huggingface. Manual Setup linkCreate a YAML config file in the models directory. Specify the backend and the model file. name: text-embedding-ada-002 # The model name used in the API parameters: model: backend: \"\" embeddings: true # .",
                content: "LocalAI supports generating embeddings for text or list of tokens.\nFor the API documentation you can refer to the OpenAI docs: https://platform.openai.com/docs/api-reference/embeddings\nModel compatibility linkThe embedding endpoint is compatible with llama.cpp models, bert.cpp models and sentence-transformers models available in huggingface.\nManual Setup linkCreate a YAML config file in the models directory. Specify the backend and the model file.\nname: text-embedding-ada-002 # The model name used in the API parameters: model: backend: \"\" embeddings: true # .. other parameters Huggingface embeddings linkTo use sentence-transformers and models in huggingface you can use the sentencetransformers embedding backend.\nname: text-embedding-ada-002 backend: sentencetransformers embeddings: true parameters: model: all-MiniLM-L6-v2 The sentencetransformers backend uses Python sentence-transformers. For a list of all pre-trained models available see here: https://github.com/UKPLab/sentence-transformers#pre-trained-models\nnotifications The sentencetransformers backend is an optional backend of LocalAI and uses Python. If you are running LocalAI from the containers you are good to go and should be already configured for use.\nIf you are running LocalAI manually you must install the python dependencies (make prepare-extra-conda-environments). This requires conda to be installed.\nFor local execution, you also have to specify the extra backend in the EXTERNAL_GRPC_BACKENDS environment variable.\nExample: EXTERNAL_GRPC_BACKENDS=\"sentencetransformers:/path/to/LocalAI/backend/python/sentencetransformers/sentencetransformers.py\" The sentencetransformers backend does support only embeddings of text, and not of tokens. If you need to embed tokens you can use the bert backend or llama.cpp.\nNo models are required to be downloaded before using the sentencetransformers backend. The models will be downloaded automatically the first time the API is used.\nLlama.cpp embeddings linkEmbeddings with llama.cpp are supported with the llama-cpp backend, it needs to be enabled with embeddings set to true.\nname: my-awesome-model backend: llama-cpp embeddings: true parameters: model: ggml-file.bin # ... Then you can use the API to generate embeddings:\ncurl http://localhost:8080/embeddings -X POST -H \"Content-Type: application/json\" -d '{ \"input\": \"My text\", \"model\": \"my-awesome-model\" }' | jq \".\" 💡 Examples link Example that uses LLamaIndex and LocalAI as embedding: here. "
            }
        );
    index.add(
            {
                id:  17 ,
                href: "\/features\/gpt-vision\/",
                title: "🥽 GPT Vision",
                description: "LocalAI supports understanding images by using LLaVA, and implements the GPT Vision API from OpenAI.\nUsage linkOpenAI docs: https://platform.openai.com/docs/guides/vision\nTo let LocalAI understand and reply with what sees in the image, use the /v1/chat/completions endpoint, for example with curl:\ncurl http://localhost:8080/v1/chat/completions -H \"Content-Type: application/json\" -d '{ \"model\": \"llava\", \"messages\": [{\"role\": \"user\", \"content\": [{\"type\":\"text\", \"text\": \"What is in the image?\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\" }}], \"temperature\": 0.9}]}' Grammars and function tools can be used as well in conjunction with vision APIs:",
                content: "LocalAI supports understanding images by using LLaVA, and implements the GPT Vision API from OpenAI.\nUsage linkOpenAI docs: https://platform.openai.com/docs/guides/vision\nTo let LocalAI understand and reply with what sees in the image, use the /v1/chat/completions endpoint, for example with curl:\ncurl http://localhost:8080/v1/chat/completions -H \"Content-Type: application/json\" -d '{ \"model\": \"llava\", \"messages\": [{\"role\": \"user\", \"content\": [{\"type\":\"text\", \"text\": \"What is in the image?\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\" }}], \"temperature\": 0.9}]}' Grammars and function tools can be used as well in conjunction with vision APIs:\ncurl http://localhost:8080/v1/chat/completions -H \"Content-Type: application/json\" -d '{ \"model\": \"llava\", \"grammar\": \"root ::= (\\\"yes\\\" | \\\"no\\\")\", \"messages\": [{\"role\": \"user\", \"content\": [{\"type\":\"text\", \"text\": \"Is there some grass in the image?\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\" }}], \"temperature\": 0.9}]}' Setup linkAll-in-One images have already shipped the llava model as gpt-4-vision-preview, so no setup is needed in this case.\nTo setup the LLaVa models, follow the full example in the configuration examples.\n"
            }
        );
    index.add(
            {
                id:  18 ,
                href: "\/features\/constrained_grammars\/",
                title: "✍️ Constrained Grammars",
                description: "Overview linkThe chat endpoint supports the grammar parameter, which allows users to specify a grammar in Backus-Naur Form (BNF). This feature enables the Large Language Model (LLM) to generate outputs adhering to a user-defined schema, such as JSON, YAML, or any other format that can be defined using BNF. For more details about BNF, see Backus-Naur Form on Wikipedia.\nnotifications Compatibility Notice: This feature is only supported by models that use the llama.",
                content: "Overview linkThe chat endpoint supports the grammar parameter, which allows users to specify a grammar in Backus-Naur Form (BNF). This feature enables the Large Language Model (LLM) to generate outputs adhering to a user-defined schema, such as JSON, YAML, or any other format that can be defined using BNF. For more details about BNF, see Backus-Naur Form on Wikipedia.\nnotifications Compatibility Notice: This feature is only supported by models that use the llama.cpp backend. For a complete list of compatible models, refer to the "
            }
        );
    index.add(
            {
                id:  19 ,
                href: "\/features\/distribute\/",
                title: "🆕🖧 Distributed Inference",
                description: "This functionality enables LocalAI to distribute inference requests across multiple worker nodes, improving efficiency and performance. Nodes are automatically discovered and connect via p2p by using a shared token which makes sure the communication is secure and private between the nodes of the network.\nLocalAI supports two modes of distributed inferencing via p2p:\nFederated Mode: Requests are shared between the cluster and routed to a single worker node in the network based on the load balancer’s decision.",
                content: "This functionality enables LocalAI to distribute inference requests across multiple worker nodes, improving efficiency and performance. Nodes are automatically discovered and connect via p2p by using a shared token which makes sure the communication is secure and private between the nodes of the network.\nLocalAI supports two modes of distributed inferencing via p2p:\nFederated Mode: Requests are shared between the cluster and routed to a single worker node in the network based on the load balancer’s decision. Worker Mode (aka “model sharding” or “splitting weights”): Requests are processed by all the workers which contributes to the final inference result (by sharing the model weights). Usage linkStarting LocalAI with --p2p generates a shared token for connecting multiple instances: and that’s all you need to create AI clusters, eliminating the need for intricate network setups.\nSimply navigate to the “Swarm” section in the WebUI and follow the on-screen instructions.\nFor fully shared instances, initiate LocalAI with –p2p –federated and adhere to the Swarm section’s guidance. This feature, while still experimental, offers a tech preview quality experience.\nFederated mode linkFederated mode allows to launch multiple LocalAI instances and connect them together in a federated network. This mode is useful when you want to distribute the load of the inference across multiple nodes, but you want to have a single point of entry for the API. In the Swarm section of the WebUI, you can see the instructions to connect multiple instances together.\nTo start a LocalAI server in federated mode, run:\nlocal-ai run --p2p --federated This will generate a token that you can use to connect other LocalAI instances to the network or others can use to join the network. If you already have a token, you can specify it using the TOKEN environment variable.\nTo start a load balanced server that routes the requests to the network, run with the TOKEN:\nlocal-ai federated To see all the available options, run local-ai federated --help.\nThe instructions are displayed in the “Swarm” section of the WebUI, guiding you through the process of connecting multiple instances.\nWorkers mode link notifications This feature is available exclusively with llama-cpp compatible models.\nThis feature was introduced in LocalAI pull request #2324 and is based on the upstream work in llama.cpp pull request #6829.\nTo connect multiple workers to a single LocalAI instance, start first a server in p2p mode:\nlocal-ai run --p2p And navigate the WebUI to the “Swarm” section to see the instructions to connect multiple workers to the network.\nWithout P2P linkTo start workers for distributing the computational load, run:\nlocal-ai worker llama-cpp-rpc --llama-cpp-args=\"-H -p -m \" And you can specify the address of the workers when starting LocalAI with the LLAMACPP_GRPC_SERVERS environment variable:\nLLAMACPP_GRPC_SERVERS=\"address1:port,address2:port\" local-ai run The workload on the LocalAI server will then be distributed across the specified nodes.\nAlternatively, you can build the RPC workers/server following the llama.cpp README, which is compatible with LocalAI.\nManual example (worker) linkUse the WebUI to guide you in the process of starting new workers. This example shows the manual steps to highlight the process.\nStart the server with --p2p: ./local-ai run --p2p # Get the token in the Swarm section of the WebUI Copy the token from the WebUI or via API call (e.g., curl http://localhost:8000/p2p/token) and save it for later use.\nTo reuse the same token later, restart the server with --p2ptoken or P2P_TOKEN.\nStart the workers. Copy the local-ai binary to other hosts and run as many workers as needed using the token: TOKEN=XXX ./local-ai worker p2p-llama-cpp-rpc --llama-cpp-args=\"-m \" # 1:06AM INF loading environment variables from file envFile=.env # 1:06AM INF Setting logging to info # {\"level\":\"INFO\",\"time\":\"2024-05-19T01:06:01.794+0200\",\"caller\":\"config/config.go:288\",\"message\":\"connmanager disabled\\n\"} # {\"level\":\"INFO\",\"time\":\"2024-05-19T01:06:01.794+0200\",\"caller\":\"config/config.go:295\",\"message\":\" go-libp2p resource manager protection enabled\"} # {\"level\":\"INFO\",\"time\":\"2024-05-19T01:06:01.794+0200\",\"caller\":\"config/config.go:409\",\"message\":\"max connections: 100\\n\"} # 1:06AM INF Starting llama-cpp-rpc-server on '127.0.0.1:34371' # {\"level\":\"INFO\",\"time\":\"2024-05-19T01:06:01.794+0200\",\"caller\":\"node/node.go:118\",\"message\":\" Starting EdgeVPN network\"} # create_backend: using CPU backend # Starting RPC server on 127.0.0.1:34371, backend memory: 31913 MB # 2024/05/19 01:06:01 failed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 2048 kiB, got: 416 kiB). # See https://github.com/quic-go/quic-go/wiki/UDP-Buffer-Sizes for details. # {\"level\":\"INFO\",\"time\":\"2024-05-19T01:06:01.805+0200\",\"caller\":\"node/node.go:172\",\"message\":\" Node ID: 12D3KooWJ7WQAbCWKfJgjw2oMMGGss9diw3Sov5hVWi8t4DMgx92\"} # {\"level\":\"INFO\",\"time\":\"2024-05-19T01:06:01.806+0200\",\"caller\":\"node/node.go:173\",\"message\":\" Node Addresses: [/ip4/127.0.0.1/tcp/44931 /ip4/127.0.0.1/udp/33251/quic-v1/webtransport/certhash/uEiAWAhZ-W9yx2ZHnKQm3BE_ft5jjoc468z5-Rgr9XdfjeQ/certhash/uEiB8Uwn0M2TQBELaV2m4lqypIAY2S-2ZMf7lt_N5LS6ojw /ip4/127.0.0.1/udp/35660/quic-v1 /ip4/192.168.68.110/tcp/44931 /ip4/192.168.68.110/udp/33251/quic-v1/webtransport/certhash/uEiAWAhZ-W9yx2ZHnKQm3BE_ft5jjoc468z5-Rgr9XdfjeQ/certhash/uEiB8Uwn0M2TQBELaV2m4lqypIAY2S-2ZMf7lt_N5LS6ojw /ip4/192.168.68.110/udp/35660/quic-v1 /ip6/::1/tcp/41289 /ip6/::1/udp/33160/quic-v1/webtransport/certhash/uEiAWAhZ-W9yx2ZHnKQm3BE_ft5jjoc468z5-Rgr9XdfjeQ/certhash/uEiB8Uwn0M2TQBELaV2m4lqypIAY2S-2ZMf7lt_N5LS6ojw /ip6/::1/udp/35701/quic-v1]\"} # {\"level\":\"INFO\",\"time\":\"2024-05-19T01:06:01.806+0200\",\"caller\":\"discovery/dht.go:104\",\"message\":\" Bootstrapping DHT\"} (Note: You can also supply the token via command-line arguments)\nThe server logs should indicate that new workers are being discovered.\nStart inference as usual on the server initiated in step 1. Environment Variables linkThere are options that can be tweaked or parameters that can be set using environment variables\nEnvironment Variable Description LOCALAI_P2P Set to “true” to enable p2p LOCALAI_FEDERATED Set to “true” to enable federated mode FEDERATED_SERVER Set to “true” to enable federated server LOCALAI_P2P_DISABLE_DHT Set to “true” to disable DHT and enable p2p layer to be local only (mDNS) LOCALAI_P2P_ENABLE_LIMITS Set to “true” to enable connection limits and resources management (useful when running with poor connectivity or want to limit resources consumption) LOCALAI_P2P_LISTEN_MADDRS Set to comma separated list of multiaddresses to override default libp2p 0.0.0.0 multiaddresses LOCALAI_P2P_DHT_ANNOUNCE_MADDRS Set to comma separated list of multiaddresses to override announcing of listen multiaddresses (useful when external address:port is remapped) LOCALAI_P2P_BOOTSTRAP_PEERS_MADDRS Set to comma separated list of multiaddresses to specify custom DHT bootstrap nodes LOCALAI_P2P_TOKEN Set the token for the p2p network LOCALAI_P2P_LOGLEVEL Set the loglevel for the LocalAI p2p stack (default: info) LOCALAI_P2P_LIB_LOGLEVEL Set the loglevel for the underlying libp2p stack (default: fatal) Architecture linkLocalAI uses https://github.com/libp2p/go-libp2p under the hood, the same project powering IPFS. Differently from other frameworks, LocalAI uses peer2peer without a single master server, but rather it uses sub/gossip and ledger functionalities to achieve consensus across different peers.\nEdgeVPN is used as a library to establish the network and expose the ledger functionality under a shared token to ease out automatic discovery and have separated, private peer2peer networks.\nThe weights are split proportional to the memory when running into worker mode, when in federation mode each request is split to every node which have to load the model fully.\nNotes link If running in p2p mode with container images, make sure you start the container with --net host or network_mode: host in the docker-compose file. Only a single model is supported currently. Ensure the server detects new workers before starting inference. Currently, additional workers cannot be added once inference has begun. For more details on the implementation, refer to LocalAI pull request #2343 "
            }
        );
    index.add(
            {
                id:  20 ,
                href: "\/features\/audio-to-text\/",
                title: "🔈 Audio to text",
                description: "Audio to text models are models that can generate text from an audio file. The transcription endpoint allows to convert audio files to text. The endpoint is based on whisper.cpp, a C++ library for audio transcription. The endpoint input supports all the audio formats supported by ffmpeg. Usage linkOnce LocalAI is started and whisper models are installed, you can use the /v1/audio/transcriptions API endpoint. For instance, with cURL: curl http://localhost:8080/v1/audio/transcriptions -H \"Content-Type: multipart/form-data\" -F file=\"@\" -F model=\"\" Example linkDownload one of the models from here in the models folder, and create a YAML file for your model:",
                content: "Audio to text models are models that can generate text from an audio file.\nThe transcription endpoint allows to convert audio files to text. The endpoint is based on whisper.cpp, a C++ library for audio transcription. The endpoint input supports all the audio formats supported by ffmpeg.\nUsage linkOnce LocalAI is started and whisper models are installed, you can use the /v1/audio/transcriptions API endpoint.\nFor instance, with cURL:\ncurl http://localhost:8080/v1/audio/transcriptions -H \"Content-Type: multipart/form-data\" -F file=\"@\" -F model=\"\" Example linkDownload one of the models from here in the models folder, and create a YAML file for your model:\nname: whisper-1 backend: whisper parameters: model: whisper-en The transcriptions endpoint then can be tested like so:\n## Get an example audio file wget --quiet --show-progress -O gb1.ogg https://upload.wikimedia.org/wikipedia/commons/1/1f/George_W_Bush_Columbia_FINAL.ogg ## Send the example audio file to the transcriptions endpoint curl http://localhost:8080/v1/audio/transcriptions -H \"Content-Type: multipart/form-data\" -F file=\"@$PWD/gb1.ogg\" -F model=\"whisper-1\" ## Result {\"text\":\"My fellow Americans, this day has brought terrible news and great sadness to our country.At nine o'clock this morning, Mission Control in Houston lost contact with our Space ShuttleColumbia.A short time later, debris was seen falling from the skies above Texas.The Columbia's lost.There are no survivors.One board was a crew of seven.Colonel Rick Husband, Lieutenant Colonel Michael Anderson, Commander Laurel Clark, Captain DavidBrown, Commander William McCool, Dr. Kultna Shavla, and Elon Ramon, a colonel in the IsraeliAir Force.These men and women assumed great risk in the service to all humanity.In an age when spaceflight has come to seem almost routine, it is easy to overlook thedangers of travel by rocket and the difficulties of navigating the fierce outer atmosphere ofthe Earth.These astronauts knew the dangers, and they faced them willingly, knowing they had a highand noble purpose in life.Because of their courage and daring and idealism, we will miss them all the more.All Americans today are thinking as well of the families of these men and women who havebeen given this sudden shock and grief.You're not alone.Our entire nation agrees with you, and those you loved will always have the respect andgratitude of this country.The cause in which they died will continue.Mankind has led into the darkness beyond our world by the inspiration of discovery andthe longing to understand.Our journey into space will go on.In the skies today, we saw destruction and tragedy.As farther than we can see, there is comfort and hope.In the words of the prophet Isaiah, \\\"Lift your eyes and look to the heavens who createdall these, he who brings out the starry hosts one by one and calls them each by name.\\\"Because of his great power and mighty strength, not one of them is missing.The same creator who names the stars also knows the names of the seven souls we mourntoday.The crew of the shuttle Columbia did not return safely to Earth yet we can pray that all aresafely home.May God bless the grieving families and may God continue to bless America.[BLANK_AUDIO]\"} "
            }
        );
    index.add(
            {
                id:  21 ,
                href: "\/features\/openai-functions\/",
                title: "🔥 OpenAI functions and tools",
                description: "LocalAI supports running OpenAI functions and tools API with llama.cpp compatible models.\nTo learn more about OpenAI functions, see also the OpenAI API blog post.\nLocalAI is also supporting JSON mode out of the box with llama.cpp-compatible models.\n💡 Check out also LocalAGI for an example on how to use LocalAI functions.\nSetup linkOpenAI functions are available only with ggml or gguf models compatible with llama.cpp.\nYou don’t need to do anything specific - just use ggml or gguf models.",
                content: "LocalAI supports running OpenAI functions and tools API with llama.cpp compatible models.\nTo learn more about OpenAI functions, see also the OpenAI API blog post.\nLocalAI is also supporting JSON mode out of the box with llama.cpp-compatible models.\n💡 Check out also LocalAGI for an example on how to use LocalAI functions.\nSetup linkOpenAI functions are available only with ggml or gguf models compatible with llama.cpp.\nYou don’t need to do anything specific - just use ggml or gguf models.\nUsage example linkYou can configure a model manually with a YAML config file in the models directory, for example:\nname: gpt-3.5-turbo parameters: # Model file name model: ggml-openllama.bin top_p: 80 top_k: 0.9 temperature: 0.1 To use the functions with the OpenAI client in python:\nfrom openai import OpenAI # ... # Send the conversation and available functions to GPT messages = [{\"role\": \"user\", \"content\": \"What is the weather like in Beijing now?\"}] tools = [ { \"type\": \"function\", \"function\": { \"name\": \"get_current_weather\", \"description\": \"Return the temperature of the specified region specified by the user\", \"parameters\": { \"type\": \"object\", \"properties\": { \"location\": { \"type\": \"string\", \"description\": \"User specified region\", }, \"unit\": { \"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"], \"description\": \"temperature unit\" }, }, \"required\": [\"location\"], }, }, } ] client = OpenAI( # This is the default and can be omitted api_key=\"test\", base_url=\"http://localhost:8080/v1/\" ) response =client.chat.completions.create( messages=messages, tools=tools, tool_choice =\"auto\", model=\"gpt-4\", ) #... For example, with curl:\ncurl http://localhost:8080/v1/chat/completions -H \"Content-Type: application/json\" -d '{ \"model\": \"gpt-4\", \"messages\": [{\"role\": \"user\", \"content\": \"What is the weather like in Beijing now?\"}], \"tools\": [ { \"type\": \"function\", \"function\": { \"name\": \"get_current_weather\", \"description\": \"Return the temperature of the specified region specified by the user\", \"parameters\": { \"type\": \"object\", \"properties\": { \"location\": { \"type\": \"string\", \"description\": \"User specified region\" }, \"unit\": { \"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"], \"description\": \"temperature unit\" } }, \"required\": [\"location\"] } } } ], \"tool_choice\":\"auto\" }' Return data：\n{ \"created\": 1724210813, \"object\": \"chat.completion\", \"id\": \"16b57014-477c-4e6b-8d25-aad028a5625e\", \"model\": \"gpt-4\", \"choices\": [ { \"index\": 0, \"finish_reason\": \"tool_calls\", \"message\": { \"role\": \"assistant\", \"content\": \"\", \"tool_calls\": [ { \"index\": 0, \"id\": \"16b57014-477c-4e6b-8d25-aad028a5625e\", \"type\": \"function\", \"function\": { \"name\": \"get_current_weather\", \"arguments\": \"{\\\"location\\\":\\\"Beijing\\\",\\\"unit\\\":\\\"celsius\\\"}\" } } ] } } ], \"usage\": { \"prompt_tokens\": 221, \"completion_tokens\": 26, \"total_tokens\": 247 } } Advanced linkUse functions without grammars linkThe functions calls maps automatically to grammars which are currently supported only by llama.cpp, however, it is possible to turn off the use of grammars, and extract tool arguments from the LLM responses, by specifying in the YAML file no_grammar and a regex to map the response from the LLM:\nname: model_name parameters: # Model file name model: model/name function: # set to true to not use grammars no_grammar: true # set one or more regexes used to extract the function tool arguments from the LLM response response_regex: - \"(?P\\w+)\\s*\\((?P.*)\\)\" The response regex have to be a regex with named parameters to allow to scan the function name and the arguments. For instance, consider:\n(?P\\w+)\\s*\\((?P.*)\\) will catch\nfunction_name({ \"foo\": \"bar\"}) Parallel tools calls linkThis feature is experimental and has to be configured in the YAML of the model by enabling function.parallel_calls:\nname: gpt-3.5-turbo parameters: # Model file name model: ggml-openllama.bin top_p: 80 top_k: 0.9 temperature: 0.1 function: # set to true to allow the model to call multiple functions in parallel parallel_calls: true Use functions with grammar linkIt is possible to also specify the full function signature (for debugging, or to use with other clients).\nThe chat endpoint accepts the grammar_json_functions additional parameter which takes a JSON schema object.\nFor example, with curl:\ncurl http://localhost:8080/v1/chat/completions -H \"Content-Type: application/json\" -d '{ \"model\": \"gpt-4\", \"messages\": [{\"role\": \"user\", \"content\": \"How are you?\"}], \"temperature\": 0.1, \"grammar_json_functions\": { \"oneOf\": [ { \"type\": \"object\", \"properties\": { \"function\": {\"const\": \"create_event\"}, \"arguments\": { \"type\": \"object\", \"properties\": { \"title\": {\"type\": \"string\"}, \"date\": {\"type\": \"string\"}, \"time\": {\"type\": \"string\"} } } } }, { \"type\": \"object\", \"properties\": { \"function\": {\"const\": \"search\"}, \"arguments\": { \"type\": \"object\", \"properties\": { \"query\": {\"type\": \"string\"} } } } } ] } }' Grammars and function tools can be used as well in conjunction with vision APIs:\ncurl http://localhost:8080/v1/chat/completions -H \"Content-Type: application/json\" -d '{ \"model\": \"llava\", \"grammar\": \"root ::= (\\\"yes\\\" | \\\"no\\\")\", \"messages\": [{\"role\": \"user\", \"content\": [{\"type\":\"text\", \"text\": \"Is there some grass in the image?\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\" }}], \"temperature\": 0.9}]}' 💡 Examples linkA full e2e example with docker-compose is available here.\n"
            }
        );
    index.add(
            {
                id:  22 ,
                href: "\/stores\/",
                title: "💾 Stores",
                description: "Stores are an experimental feature to help with querying data using similarity search. It is a low level API that consists of only get, set, delete and find.\nFor example if you have an embedding of some text and want to find text with similar embeddings. You can create embeddings for chunks of all your text then compare them against the embedding of the text you are searching on.\nAn embedding here meaning a vector of numbers that represent some information about the text.",
                content: "Stores are an experimental feature to help with querying data using similarity search. It is a low level API that consists of only get, set, delete and find.\nFor example if you have an embedding of some text and want to find text with similar embeddings. You can create embeddings for chunks of all your text then compare them against the embedding of the text you are searching on.\nAn embedding here meaning a vector of numbers that represent some information about the text. The embeddings are created from an A.I. model such as BERT or a more traditional method such as word frequency.\nPreviously you would have to integrate with an external vector database or library directly. With the stores feature you can now do it through the LocalAI API.\nNote however that doing a similarity search on embeddings is just one way to do retrieval. A higher level API can take this into account, so this may not be the best place to start.\nAPI overview linkThere is an internal gRPC API and an external facing HTTP JSON API. We’ll just discuss the external HTTP API, however the HTTP API mirrors the gRPC API. Consult pkg/store/client for internal usage.\nEverything is in columnar format meaning that instead of getting an array of objects with a key and a value each. You instead get two separate arrays of keys and values.\nKeys are arrays of floating point numbers with a maximum width of 32bits. Values are strings (in gRPC they are bytes).\nThe key vectors must all be the same length and it’s best for search performance if they are normalized. When addings keys it will be detected if they are not normalized and what length they are.\nAll endpoints accept a store field which specifies which store to operate on. Presently they are created on the fly and there is only one store backend so no configuration is required.\nSet linkTo set some keys you can do\ncurl -X POST http://localhost:8080/stores/set \\ -H \"Content-Type: application/json\" \\ -d '{\"keys\": [[0.1, 0.2], [0.3, 0.4]], \"values\": [\"foo\", \"bar\"]}' Setting the same keys again will update their values.\nOn success 200 OK is returned with no body.\nGet linkTo get some keys you can do\ncurl -X POST http://localhost:8080/stores/get \\ -H \"Content-Type: application/json\" \\ -d '{\"keys\": [[0.1, 0.2]]}' Both the keys and values are returned, e.g: {\"keys\":[[0.1,0.2]],\"values\":[\"foo\"]}\nThe order of the keys is not preserved! If a key does not exist then nothing is returned.\nDelete linkTo delete keys and values you can do\ncurl -X POST http://localhost:8080/stores/delete \\ -H \"Content-Type: application/json\" \\ -d '{\"keys\": [[0.1, 0.2]]}' If a key doesn’t exist then it is ignored.\nOn success 200 OK is returned with no body.\nFind linkTo do a similarity search you can do\ncurl -X POST http://localhost:8080/stores/find -H \"Content-Type: application/json\" \\ -d '{\"topk\": 2, \"key\": [0.2, 0.1]}' topk limits the number of results returned. The result value is the same as get, except that it also includes an array of similarities. Where 1.0 is the maximum similarity. They are returned in the order of most similar to least.\n"
            }
        );
    index.add(
            {
                id:  23 ,
                href: "\/models\/",
                title: "🖼️ Model gallery",
                description: "The model gallery is a curated collection of models configurations for LocalAI that enables one-click install of models directly from the LocalAI Web interface.\nA list of the models available can also be browsed at the Public LocalAI Gallery.\nLocalAI to ease out installations of models provide a way to preload models on start and downloading and installing them in runtime. You can install models manually by copying them over the models directory, or use the API or the Web interface to configure, download and verify the model assets for you.",
                content: "The model gallery is a curated collection of models configurations for LocalAI that enables one-click install of models directly from the LocalAI Web interface.\nA list of the models available can also be browsed at the Public LocalAI Gallery.\nLocalAI to ease out installations of models provide a way to preload models on start and downloading and installing them in runtime. You can install models manually by copying them over the models directory, or use the API or the Web interface to configure, download and verify the model assets for you.\nnotifications The models in this gallery are not directly maintained by LocalAI. If you find a model that is not working, please open an issue on the model gallery repository.\nnotifications GPT and text generation models might have a license which is not permissive for commercial use or might be questionable or without any license at all. Please check the model license before using it. The official gallery contains only open licensed models.\nUseful Links and resources link Open LLM Leaderboard - here you can find a list of the most performing models on the Open LLM benchmark. Keep in mind models compatible with LocalAI must be quantized in the gguf format. How it works linkNavigate the WebUI interface in the “Models” section from the navbar at the top. Here you can find a list of models that can be installed, and you can install them by clicking the “Install” button.\nAdd other galleries linkYou can add other galleries by setting the GALLERIES environment variable. The GALLERIES environment variable is a list of JSON objects, where each object has a name and a url field. The name field is the name of the gallery, and the url field is the URL of the gallery’s index file, for example:\nGALLERIES=[{\"name\":\"\", \"url\":\""
            }
        );
    index.add(
            {
                id:  24 ,
                href: "\/docs\/integrations\/",
                title: "Integrations",
                description: "Community integrations linkList of projects that are using directly LocalAI behind the scenes can be found here.\nThe list below is a list of software that integrates with LocalAI.\nAnythingLLM Logseq GPT3 OpenAI plugin allows to set a base URL, and works with LocalAI. https://plugins.jetbrains.com/plugin/21056-codegpt allows for custom OpenAI compatible endpoints since 2.4.0 Wave Terminal has native support for LocalAI! https://github.com/longy2k/obsidian-bmo-chatbot https://github.com/FlowiseAI/Flowise https://github.com/k8sgpt-ai/k8sgpt https://github.com/kairos-io/kairos https://github.com/langchain4j/langchain4j https://github.com/henomis/lingoose https://github.com/trypromptly/LLMStack https://github.com/mattermost/openops https://github.com/charmbracelet/mods https://github.com/cedriking/spark Big AGI is a powerful web interface entirely running in the browser, supporting LocalAI Midori AI Subsystem Manager is a powerful docker subsystem for running all types of AI programs LLPhant is a PHP library for interacting with LLMs and Vector Databases GPTLocalhost (Word Add-in) - run LocalAI in Microsoft Word locally use LocalAI from Nextcloud with the integration plugin and AI assistant Feel free to open up a Pull request (by clicking at the “Edit page” below) to get a page for your project made or if you see a error on one of the pages!",
                content: "Community integrations linkList of projects that are using directly LocalAI behind the scenes can be found here.\nThe list below is a list of software that integrates with LocalAI.\nAnythingLLM Logseq GPT3 OpenAI plugin allows to set a base URL, and works with LocalAI. https://plugins.jetbrains.com/plugin/21056-codegpt allows for custom OpenAI compatible endpoints since 2.4.0 Wave Terminal has native support for LocalAI! https://github.com/longy2k/obsidian-bmo-chatbot https://github.com/FlowiseAI/Flowise https://github.com/k8sgpt-ai/k8sgpt https://github.com/kairos-io/kairos https://github.com/langchain4j/langchain4j https://github.com/henomis/lingoose https://github.com/trypromptly/LLMStack https://github.com/mattermost/openops https://github.com/charmbracelet/mods https://github.com/cedriking/spark Big AGI is a powerful web interface entirely running in the browser, supporting LocalAI Midori AI Subsystem Manager is a powerful docker subsystem for running all types of AI programs LLPhant is a PHP library for interacting with LLMs and Vector Databases GPTLocalhost (Word Add-in) - run LocalAI in Microsoft Word locally use LocalAI from Nextcloud with the integration plugin and AI assistant Feel free to open up a Pull request (by clicking at the “Edit page” below) to get a page for your project made or if you see a error on one of the pages!\n"
            }
        );
    index.add(
            {
                id:  25 ,
                href: "\/docs\/advanced\/",
                title: "Advanced",
                description: "Advanced usage",
                content: ""
            }
        );
    index.add(
            {
                id:  26 ,
                href: "\/advanced\/",
                title: "Advanced usage",
                description: "Advanced configuration with YAML files linkIn order to define default prompts, model parameters (such as custom default top_p or top_k), LocalAI can be configured to serve user-defined models with a set of default parameters and templates.\nIn order to configure a model, you can create multiple yaml files in the models path or either specify a single YAML configuration file. Consider the following models folder in the example/chatbot-ui:\nbase ❯ ls -liah examples/chatbot-ui/models 36487587 drwxr-xr-x 2 mudler mudler 4.",
                content: "Advanced configuration with YAML files linkIn order to define default prompts, model parameters (such as custom default top_p or top_k), LocalAI can be configured to serve user-defined models with a set of default parameters and templates.\nIn order to configure a model, you can create multiple yaml files in the models path or either specify a single YAML configuration file. Consider the following models folder in the example/chatbot-ui:\nbase ❯ ls -liah examples/chatbot-ui/models 36487587 drwxr-xr-x 2 mudler mudler 4.0K May 3 12:27 . 36487586 drwxr-xr-x 3 mudler mudler 4.0K May 3 10:42 .. 36465214 -rw-r--r-- 1 mudler mudler 10 Apr 27 07:46 completion.tmpl 36464855 -rw-r--r-- 1 mudler mudler ?G Apr 27 00:08 luna-ai-llama2-uncensored.ggmlv3.q5_K_M.bin 36464537 -rw-r--r-- 1 mudler mudler 245 May 3 10:42 gpt-3.5-turbo.yaml 36467388 -rw-r--r-- 1 mudler mudler 180 Apr 27 07:46 chat.tmpl In the gpt-3.5-turbo.yaml file it is defined the gpt-3.5-turbo model which is an alias to use luna-ai-llama2 with pre-defined options.\nFor instance, consider the following that declares gpt-3.5-turbo backed by the luna-ai-llama2 model:\nname: gpt-3.5-turbo # Default model parameters parameters: # Relative to the models path model: luna-ai-llama2-uncensored.ggmlv3.q5_K_M.bin # temperature temperature: 0.3 # all the OpenAI request options here.. # Default context size context_size: 512 threads: 10 # Define a backend (optional). By default it will try to guess the backend the first time the model is interacted with. backend: llama-stable # available: llama, stablelm, gpt2, gptj rwkv # Enable prompt caching prompt_cache_path: \"alpaca-cache\" prompt_cache_all: true # stopwords (if supported by the backend) stopwords: - \"HUMAN:\" - \"### Response:\" # define chat roles roles: assistant: '### Response:' system: '### System Instruction:' user: '### Instruction:' template: # template file \".tmpl\" with the prompt template to use by default on the endpoint call. Note there is no extension in the files completion: completion chat: chat Specifying a config-file via CLI allows to declare models in a single file as a list, for instance:\n- name: list1 parameters: model: testmodel context_size: 512 threads: 10 stopwords: - \"HUMAN:\" - \"### Response:\" roles: user: \"HUMAN:\" system: \"GPT:\" template: completion: completion chat: chat - name: list2 parameters: model: testmodel context_size: 512 threads: 10 stopwords: - \"HUMAN:\" - \"### Response:\" roles: user: \"HUMAN:\" system: \"GPT:\" template: completion: completion chat: chat See also chatbot-ui as an example on how to use config files.\nIt is possible to specify a full URL or a short-hand URL to a YAML model configuration file and use it on start with local-ai, for example to use phi-2:\nlocal-ai github://mudler/LocalAI/examples/configurations/phi-2.yaml@master Full config model file reference link # Main configuration of the model, template, and system features. name: \"\" # Model name, used to identify the model in API calls. # Precision settings for the model, reducing precision can enhance performance on some hardware. f16: null # Whether to use 16-bit floating-point precision. embeddings: true # Enable embeddings for the model. # Concurrency settings for the application. threads: null # Number of threads to use for processing. # Roles define how different entities interact in a conversational model. # It can be used to map roles to specific parts of the conversation. roles: {} # Roles for entities like user, system, assistant, etc. # Backend to use for computation (like llama-cpp, diffusers, whisper). backend: \"\" # Backend for AI computations. # Templates for various types of model interactions. template: chat: \"\" # Template for chat interactions. Uses golang templates with Sprig functions. chat_message: \"\" # Template for individual chat messages. Uses golang templates with Sprig functions. completion: \"\" # Template for generating text completions. Uses golang templates with Sprig functions. edit: \"\" # Template for edit operations. Uses golang templates with Sprig functions. function: \"\" # Template for function calls. Uses golang templates with Sprig functions. use_tokenizer_template: false # Whether to use a specific tokenizer template. (vLLM) join_chat_messages_by_character: null # Character to join chat messages, if applicable. Defaults to newline. # Function-related settings to control behavior of specific function calls. function: disable_no_action: false # Whether to disable the no-action behavior. grammar: parallel_calls: false # Allow to return parallel tools disable_parallel_new_lines: false # Disable parallel processing for new lines in grammar checks. mixed_mode: false # Allow mixed-mode grammar enforcing no_mixed_free_string: false # Disallow free strings in mixed mode. disable: false # Completely disable grammar enforcing functionality. prefix: \"\" # Prefix to add before grammars rules. expect_strings_after_json: false # Expect string after JSON data. no_action_function_name: \"\" # Function name to call when no action is determined. no_action_description_name: \"\" # Description name for no-action functions. response_regex: [] # Regular expressions to match response from argument_regex: [] # Named regular to extract function arguments from the response. argument_regex_key_name: \"key\" # Name of the named regex capture to capture the key of the function arguments argument_regex_value_name: \"value\" # Name of the named regex capture to capture the value of the function arguments json_regex_match: [] # Regular expressions to match JSON data when in tool mode replace_function_results: [] # Placeholder to replace function call results with arbitrary strings or patterns. replace_llm_results: [] # Replace language model results with arbitrary strings or patterns. capture_llm_results: [] # Capture language model results as text result, among JSON, in function calls. For instance, if a model returns a block for \"thinking\" and a block for \"response\", this will allow you to capture the thinking block. function_name_key: \"name\" function_arguments_key: \"arguments\" # Feature gating flags to enable experimental or optional features. feature_flags: {} # System prompt to use by default. system_prompt: \"\" # Configuration for splitting tensors across GPUs. tensor_split: \"\" # Identifier for the main GPU used in multi-GPU setups. main_gpu: \"\" # Small value added to the denominator in RMS normalization to prevent division by zero. rms_norm_eps: 0 # Natural question generation model parameter. ngqa: 0 # Path where prompt cache is stored. prompt_cache_path: \"\" # Whether to cache all prompts. prompt_cache_all: false # Whether the prompt cache is read-only. prompt_cache_ro: false # Mirostat sampling settings. mirostat_eta: null mirostat_tau: null mirostat: null # GPU-specific layers configuration. gpu_layers: null # Memory mapping for efficient I/O operations. mmap: null # Memory locking to ensure data remains in RAM. mmlock: null # Mode to use minimal VRAM for GPU operations. low_vram: null # Words or phrases that halts processing. stopwords: [] # Strings to cut from responses to maintain context or relevance. cutstrings: [] # Strings to trim from responses for cleaner outputs. trimspace: [] trimsuffix: [] # Default context size for the model's understanding of the conversation or text. context_size: null # Non-uniform memory access settings, useful for systems with multiple CPUs. numa: false # Configuration for LoRA lora_adapter: \"\" lora_base: \"\" lora_scale: 0 # Disable matrix multiplication queuing in GPU operations. no_mulmatq: false # Model for generating draft responses. draft_model: \"\" n_draft: 0 # Quantization settings for the model, impacting memory and processing speed. quantization: \"\" # Utilization percentage of GPU memory to allocate for the model. (vLLM) gpu_memory_utilization: 0 # Whether to trust and execute remote code. trust_remote_code: false # Force eager execution of TensorFlow operations if applicable. (vLLM) enforce_eager: false # Space allocated for swapping data in and out of memory. (vLLM) swap_space: 0 # Maximum model length, possibly referring to the number of tokens or parameters. (vLLM) max_model_len: 0 # Size of the tensor parallelism in distributed computing environments. (vLLM) tensor_parallel_size: 0 # vision model to use for multimodal mmproj: \"\" # Disables offloading of key/value pairs in transformer models to save memory. no_kv_offloading: false # Scaling factor for the rope penalty. rope_scaling: \"\" # Type of configuration, often related to the type of task or model architecture. type: \"\" # YARN settings yarn_ext_factor: 0 yarn_attn_factor: 0 yarn_beta_fast: 0 yarn_beta_slow: 0 # AutoGPT-Q settings, for configurations specific to GPT models. autogptq: model_base_name: \"\" # Base name of the model. device: \"\" # Device to run the model on. triton: false # Whether to use Triton Inference Server. use_fast_tokenizer: false # Whether to use a fast tokenizer for quicker processing. # configuration for diffusers model diffusers: cuda: false # Whether to use CUDA pipeline_type: \"\" # Type of pipeline to use. scheduler_type: \"\" # Type of scheduler for controlling operations. enable_parameters: \"\" # Parameters to enable in the diffuser. cfg_scale: 0 # Scale for CFG in the diffuser setup. img2img: false # Whether image-to-image transformation is supported. clip_skip: 0 # Number of steps to skip in CLIP operations. clip_model: \"\" # Model to use for CLIP operations. clip_subfolder: \"\" # Subfolder for storing CLIP-related data. control_net: \"\" # Control net to use # Step count, usually for image processing models step: 0 # Configuration for gRPC communication. grpc: attempts: 0 # Number of retry attempts for gRPC calls. attempts_sleep_time: 0 # Sleep time between retries. # Text-to-Speech (TTS) configuration. tts: voice: \"\" # Voice setting for TTS. vall-e: audio_path: \"\" # Path to audio files for Vall-E. # Whether to use CUDA for GPU-based operations. cuda: false # List of files to download as part of the setup or operations. download_files: [] Prompt templates linkThe API doesn’t inject a default prompt for talking to the model. You have to use a prompt similar to what’s described in the standford-alpaca docs: https://github.com/tatsu-lab/stanford_alpaca#data-release.\nYou can use a default template for every model present in your model path, by creating a corresponding file with the `.tmpl` suffix next to your model. For instance, if the model is called `foo.bin`, you can create a sibling file, `foo.bin.tmpl` which will be used as a default prompt and can be used with alpaca: The below instruction describes a task. Write a response that appropriately completes the request. ### Instruction: {{.Input}} ### Response: See the prompt-templates directory in this repository for templates for some of the most popular models.\nFor the edit endpoint, an example template for alpaca-based models can be:\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. ### Instruction: {{.Instruction}} ### Input: {{.Input}} ### Response: Install models using the API linkInstead of installing models manually, you can use the LocalAI API endpoints and a model definition to install programmatically via API models in runtime.\nA curated collection of model files is in the model-gallery (work in progress!). The files of the model gallery are different from the model files used to configure LocalAI models. The model gallery files contains information about the model setup, and the files necessary to run the model locally.\nTo install for example lunademo, you can send a POST call to the /models/apply endpoint with the model definition url (url) and the name of the model should have in LocalAI (name, optional):\ncurl --location 'http://localhost:8080/models/apply' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"id\": \"TheBloke/Luna-AI-Llama2-Uncensored-GGML/luna-ai-llama2-uncensored.ggmlv3.q5_K_M.bin\", \"name\": \"lunademo\" }' Preloading models during startup linkIn order to allow the API to start-up with all the needed model on the first-start, the model gallery files can be used during startup.\nPRELOAD_MODELS='[{\"url\": \"https://raw.githubusercontent.com/go-skynet/model-gallery/main/gpt4all-j.yaml\",\"name\": \"gpt4all-j\"}]' local-ai PRELOAD_MODELS (or --preload-models) takes a list in JSON with the same parameter of the API calls of the /models/apply endpoint.\nSimilarly it can be specified a path to a YAML configuration file containing a list of models with PRELOAD_MODELS_CONFIG ( or --preload-models-config ):\n- url: https://raw.githubusercontent.com/go-skynet/model-gallery/main/gpt4all-j.yaml name: gpt4all-j # ... Automatic prompt caching linkLocalAI can automatically cache prompts for faster loading of the prompt. This can be useful if your model need a prompt template with prefixed text in the prompt before the input.\nTo enable prompt caching, you can control the settings in the model config YAML file:\n# Enable prompt caching prompt_cache_path: \"cache\" prompt_cache_all: true prompt_cache_path is relative to the models folder. you can enter here a name for the file that will be automatically create during the first load if prompt_cache_all is set to true.\nConfiguring a specific backend for the model linkBy default LocalAI will try to autoload the model by trying all the backends. This might work for most of models, but some of the backends are NOT configured to autoload.\nThe available backends are listed in the model compatibility table.\nIn order to specify a backend for your models, create a model config file in your models directory specifying the backend:\nname: gpt-3.5-turbo # Default model parameters parameters: # Relative to the models path model: ... backend: llama-stable # ... Connect external backends linkLocalAI backends are internally implemented using gRPC services. This also allows LocalAI to connect to external gRPC services on start and extend LocalAI functionalities via third-party binaries.\nThe --external-grpc-backends parameter in the CLI can be used either to specify a local backend (a file) or a remote URL. The syntax is :. Once LocalAI is started with it, the new backend name will be available for all the API endpoints.\nSo for instance, to register a new backend which is a local file:\n./local-ai --debug --external-grpc-backends \"my-awesome-backend:/path/to/my/backend.py\" Or a remote URI:\n./local-ai --debug --external-grpc-backends \"my-awesome-backend:host:port\" For example, to start vllm manually after compiling LocalAI (also assuming running the command from the root of the repository):\n./local-ai --external-grpc-backends \"vllm:$PWD/backend/python/vllm/run.sh\" Note that first is is necessary to create the environment with:\nmake -C backend/python/vllm Environment variables linkWhen LocalAI runs in a container, there are additional environment variables available that modify the behavior of LocalAI on startup:\nEnvironment variable Default Description REBUILD false Rebuild LocalAI on startup BUILD_TYPE Build type. Available: cublas, openblas, clblas GO_TAGS Go tags. Available: stablediffusion HUGGINGFACEHUB_API_TOKEN Special token for interacting with HuggingFace Inference API, required only when using the langchain-huggingface backend EXTRA_BACKENDS A space separated list of backends to prepare. For example EXTRA_BACKENDS=\"backend/python/diffusers backend/python/transformers\" prepares the python environment on start DISABLE_AUTODETECT false Disable autodetect of CPU flagset on start LLAMACPP_GRPC_SERVERS A list of llama.cpp workers to distribute the workload. For example LLAMACPP_GRPC_SERVERS=\"address1:port,address2:port\" Here is how to configure these variables:\n# Option 1: command line docker run --env REBUILD=true localai # Option 2: set within an env file docker run --env-file .env localai CLI parameters linkYou can control LocalAI with command line arguments, to specify a binding address, or the number of threads. Any command line parameter can be specified via an environment variable.\nIn the help text below, BASEPATH is the location that local-ai is being executed from\nGlobal Flags link Parameter Default Description Environment Variable -h, –help Show context-sensitive help. –log-level info Set the level of logs to output [error,warn,info,debug] $LOCALAI_LOG_LEVEL Storage Flags link Parameter Default Description Environment Variable –models-path BASEPATH/models Path containing models used for inferencing $LOCALAI_MODELS_PATH –backend-assets-path /tmp/localai/backend_data Path used to extract libraries that are required by some of the backends in runtime $LOCALAI_BACKEND_ASSETS_PATH –image-path /tmp/generated/images Location for images generated by backends (e.g. stablediffusion) $LOCALAI_IMAGE_PATH –audio-path /tmp/generated/audio Location for audio generated by backends (e.g. piper) $LOCALAI_AUDIO_PATH –upload-path /tmp/localai/upload Path to store uploads from files api $LOCALAI_UPLOAD_PATH –config-path /tmp/localai/config $LOCALAI_CONFIG_PATH –localai-config-dir BASEPATH/configuration Directory for dynamic loading of certain configuration files (currently api_keys.json and external_backends.json) $LOCALAI_CONFIG_DIR –localai-config-dir-poll-interval Typically the config path picks up changes automatically, but if your system has broken fsnotify events, set this to a time duration to poll the LocalAI Config Dir (example: 1m) $LOCALAI_CONFIG_DIR_POLL_INTERVAL –models-config-file STRING YAML file containing a list of model backend configs $LOCALAI_MODELS_CONFIG_FILE Models Flags link Parameter Default Description Environment Variable –galleries STRING JSON list of galleries $LOCALAI_GALLERIES –autoload-galleries $LOCALAI_AUTOLOAD_GALLERIES –remote-library “https://raw.githubusercontent.com/mudler/LocalAI/master/embedded/model_library.yaml\" A LocalAI remote library URL $LOCALAI_REMOTE_LIBRARY –preload-models STRING A List of models to apply in JSON at start $LOCALAI_PRELOAD_MODELS –models MODELS,… A List of model configuration URLs to load $LOCALAI_MODELS –preload-models-config STRING A List of models to apply at startup. Path to a YAML config file $LOCALAI_PRELOAD_MODELS_CONFIG Performance Flags link Parameter Default Description Environment Variable –f16 Enable GPU acceleration $LOCALAI_F16 -t, –threads 4 Number of threads used for parallel computation. Usage of the number of physical cores in the system is suggested $LOCALAI_THREADS –context-size 512 Default context size for models $LOCALAI_CONTEXT_SIZE API Flags link Parameter Default Description Environment Variable –address “:8080” Bind address for the API server $LOCALAI_ADDRESS –cors $LOCALAI_CORS –cors-allow-origins $LOCALAI_CORS_ALLOW_ORIGINS –upload-limit 15 Default upload-limit in MB $LOCALAI_UPLOAD_LIMIT –api-keys API-KEYS,… List of API Keys to enable API authentication. When this is set, all the requests must be authenticated with one of these API keys $LOCALAI_API_KEY –disable-welcome Disable welcome pages $LOCALAI_DISABLE_WELCOME –machine-tag If not empty - put that string to Machine-Tag header in each response. Useful to track response from different machines using multiple P2P federated nodes $LOCALAI_MACHINE_TAG Backend Flags link Parameter Default Description Environment Variable –parallel-requests Enable backends to handle multiple requests in parallel if they support it (e.g.: llama.cpp or vllm) $LOCALAI_PARALLEL_REQUESTS –single-active-backend Allow only one backend to be run at a time $LOCALAI_SINGLE_ACTIVE_BACKEND –preload-backend-only Do not launch the API services, only the preloaded models / backends are started (useful for multi-node setups) $LOCALAI_PRELOAD_BACKEND_ONLY –external-grpc-backends EXTERNAL-GRPC-BACKENDS,… A list of external grpc backends $LOCALAI_EXTERNAL_GRPC_BACKENDS –enable-watchdog-idle Enable watchdog for stopping backends that are idle longer than the watchdog-idle-timeout $LOCALAI_WATCHDOG_IDLE –watchdog-idle-timeout 15m Threshold beyond which an idle backend should be stopped $LOCALAI_WATCHDOG_IDLE_TIMEOUT, $WATCHDOG_IDLE_TIMEOUT –enable-watchdog-busy Enable watchdog for stopping backends that are busy longer than the watchdog-busy-timeout $LOCALAI_WATCHDOG_BUSY –watchdog-busy-timeout 5m Threshold beyond which a busy backend should be stopped $LOCALAI_WATCHDOG_BUSY_TIMEOUT .env files linkAny settings being provided by an Environment Variable can also be provided from within .env files. There are several locations that will be checked for relevant .env files. In order of precedence they are:\n.env within the current directory localai.env within the current directory localai.env within the home directory .config/localai.env within the home directory /etc/localai.env Environment variables within files earlier in the list will take precedence over environment variables defined in files later in the list.\nAn example .env file is:\nLOCALAI_THREADS=10 LOCALAI_MODELS_PATH=/mnt/storage/localai/models LOCALAI_F16=true Request headers linkYou can use ‘Extra-Usage’ request header key presence (‘Extra-Usage: true’) to receive inference timings in milliseconds extending default OpenAI response model in the usage field:\n... { \"id\": \"...\", \"created\": ..., \"model\": \"...\", \"choices\": [ { ... }, ... ], \"object\": \"...\", \"usage\": { \"prompt_tokens\": ..., \"completion_tokens\": ..., \"total_tokens\": ..., // Extra-Usage header key will include these two float fields: \"timing_prompt_processing: ..., \"timing_token_generation\": ..., }, } ... Extra backends linkLocalAI can be extended with extra backends. The backends are implemented as gRPC services and can be written in any language. The container images that are built and published on quay.io contain a set of images split in core and extra. By default Images bring all the dependencies and backends supported by LocalAI (we call those extra images). The -core images instead bring only the strictly necessary dependencies to run LocalAI without only a core set of backends.\nIf you wish to build a custom container image with extra backends, you can use the core images and build only the backends you are interested into or prepare the environment on startup by using the EXTRA_BACKENDS environment variable. For instance, to use the diffusers backend:\nFROM quay.io/go-skynet/local-ai:master-ffmpeg-core RUN make -C backend/python/diffusers Remember also to set the EXTERNAL_GRPC_BACKENDS environment variable (or --external-grpc-backends as CLI flag) to point to the backends you are using (EXTERNAL_GRPC_BACKENDS=\"backend_name:/path/to/backend\"), for example with diffusers:\nFROM quay.io/go-skynet/local-ai:master-ffmpeg-core RUN make -C backend/python/diffusers ENV EXTERNAL_GRPC_BACKENDS=\"diffusers:/build/backend/python/diffusers/run.sh\" notifications You can specify remote external backends or path to local files. The syntax is backend-name:/path/to/backend or backend-name:host:port.\nIn runtime linkWhen using the -core container image it is possible to prepare the python backends you are interested into by using the EXTRA_BACKENDS variable, for instance:\ndocker run --env EXTRA_BACKENDS=\"backend/python/diffusers\" quay.io/go-skynet/local-ai:master-ffmpeg-core Concurrent requests linkLocalAI supports parallel requests for the backends that supports it. For instance, vLLM and llama.cpp supports parallel requests, and thus LocalAI allows to run multiple requests in parallel.\nIn order to enable parallel requests, you have to pass --parallel-requests or set the PARALLEL_REQUEST to true as environment variable.\nA list of the environment variable that tweaks parallelism is the following:\n### Python backends GRPC max workers ### Default number of workers for GRPC Python backends. ### This actually controls wether a backend can process multiple requests or not. # PYTHON_GRPC_MAX_WORKERS=1 ### Define the number of parallel LLAMA.cpp workers (Defaults to 1) # LLAMACPP_PARALLEL=1 ### Enable to run parallel requests # LOCALAI_PARALLEL_REQUESTS=true Note that, for llama.cpp you need to set accordingly LLAMACPP_PARALLEL to the number of parallel processes your GPU/CPU can handle. For python-based backends (like vLLM) you can set PYTHON_GRPC_MAX_WORKERS to the number of parallel requests.\nDisable CPU flagset auto detection in llama.cpp linkLocalAI will automatically discover the CPU flagset available in your host and will use the most optimized version of the backends.\nIf you want to disable this behavior, you can set DISABLE_AUTODETECT to true in the environment variables.\n"
            }
        );
    index.add(
            {
                id:  27 ,
                href: "\/docs\/advanced\/fine-tuning\/",
                title: "Fine-tuning LLMs for text generation",
                description: "notifications Section under construction\nThis section covers how to fine-tune a language model for text generation and consume it in LocalAI.\nOpen in ColabOpen in Colab Requirements linkFor this example you will need at least a 12GB VRAM of GPU and a Linux box.\nFine-tuning linkFine-tuning a language model is a process that requires a lot of computational power and time.\nCurrently LocalAI doesn’t support the fine-tuning endpoint as LocalAI but there are are plans to support that.",
                content: " notifications Section under construction\nThis section covers how to fine-tune a language model for text generation and consume it in LocalAI.\nOpen in ColabOpen in Colab Requirements linkFor this example you will need at least a 12GB VRAM of GPU and a Linux box.\nFine-tuning linkFine-tuning a language model is a process that requires a lot of computational power and time.\nCurrently LocalAI doesn’t support the fine-tuning endpoint as LocalAI but there are are plans to support that. For the time being a guide is proposed here to give a simple starting point on how to fine-tune a model and use it with LocalAI (but also with llama.cpp).\nThere is an e2e example of fine-tuning a LLM model to use with LocalAI written by @mudler available here.\nThe steps involved are:\nPreparing a dataset Prepare the environment and install dependencies Fine-tune the model Merge the Lora base with the model Convert the model to gguf Use the model with LocalAI Dataset preparation linkWe are going to need a dataset or a set of datasets.\nAxolotl supports a variety of formats, in the notebook and in this example we are aiming for a very simple dataset and build that manually, so we are going to use the completion format which requires the full text to be used for fine-tuning.\nA dataset for an instructor model (like Alpaca) can look like the following:\n[ { \"text\": \"As an AI language model you are trained to reply to an instruction. Try to be as much polite as possible\\n\\n## Instruction\\n\\nWrite a poem about a tree.\\n\\n## Response\\n\\nTrees are beautiful, ...\", }, { \"text\": \"As an AI language model you are trained to reply to an instruction. Try to be as much polite as possible\\n\\n## Instruction\\n\\nWrite a poem about a tree.\\n\\n## Response\\n\\nTrees are beautiful, ...\", } ] Every block in the text is the whole text that is used to fine-tune. For example, for an instructor model it follows the following format (more or less):\n## Instruction ## Response The instruction format works such as when we are going to inference with the model, we are going to feed it only the first part up to the ## Instruction block, and the model is going to complete the text with the ## Response block.\nPrepare a dataset, and upload it to your Google Drive in case you are using the Google colab. Otherwise place it next the axolotl.yaml file as dataset.json.\nInstall dependencies link # Install axolotl and dependencies git clone https://github.com/OpenAccess-AI-Collective/axolotl \u0026\u0026 pushd axolotl \u0026\u0026 git checkout 797f3dd1de8fd8c0eafbd1c9fdb172abd9ff840a \u0026\u0026 popd #0.3.0 pip install packaging pushd axolotl \u0026\u0026 pip install -e '.[flash-attn,deepspeed]' \u0026\u0026 popd # https://github.com/oobabooga/text-generation-webui/issues/4238 pip install https://github.com/Dao-AILab/flash-attention/releases/download/v2.3.0/flash_attn-2.3.0+cu117torch2.0cxx11abiFALSE-cp310-cp310-linux_x86_64.whl Configure accelerate:\naccelerate config default Fine-tuning linkWe will need to configure axolotl. In this example is provided a file to use axolotl.yaml that uses openllama-3b for fine-tuning. Copy the axolotl.yaml file and edit it to your needs. The dataset needs to be next to it as dataset.json. You can find the axolotl.yaml file here.\nIf you have a big dataset, you can pre-tokenize it to speedup the fine-tuning process:\n# Optional pre-tokenize (run only if big dataset) python -m axolotl.cli.preprocess axolotl.yaml Now we are ready to start the fine-tuning process:\n# Fine-tune accelerate launch -m axolotl.cli.train axolotl.yaml After we have finished the fine-tuning, we merge the Lora base with the model:\n# Merge lora python3 -m axolotl.cli.merge_lora axolotl.yaml --lora_model_dir=\"./qlora-out\" --load_in_8bit=False --load_in_4bit=False And we convert it to the gguf format that LocalAI can consume:\n# Convert to gguf git clone https://github.com/ggerganov/llama.cpp.git pushd llama.cpp \u0026\u0026 make GGML_CUDA=1 \u0026\u0026 popd # We need to convert the pytorch model into ggml for quantization # It crates 'ggml-model-f16.bin' in the 'merged' directory. pushd llama.cpp \u0026\u0026 python convert.py --outtype f16 \\ ../qlora-out/merged/pytorch_model-00001-of-00002.bin \u0026\u0026 popd # Start off by making a basic q4_0 4-bit quantization. # It's important to have 'ggml' in the name of the quant for some # software to recognize it's file format. pushd llama.cpp \u0026\u0026 ./quantize ../qlora-out/merged/ggml-model-f16.gguf \\ ../custom-model-q4_0.bin q4_0 Now you should have ended up with a custom-model-q4_0.bin file that you can copy in the LocalAI models directory and use it with LocalAI.\n"
            }
        );
    index.add(
            {
                id:  28 ,
                href: "\/docs\/reference\/",
                title: "References",
                description: "Reference",
                content: ""
            }
        );
    index.add(
            {
                id:  29 ,
                href: "\/faq\/",
                title: "FAQ",
                description: "Frequently asked questions linkHere are answers to some of the most common questions.\nHow do I get models? linkMost gguf-based models should work, but newer models may require additions to the API. If a model doesn’t work, please feel free to open up issues. However, be cautious about downloading models from the internet and directly onto your machine, as there may be security vulnerabilities in lama.cpp or ggml that could be maliciously exploited.",
                content: "Frequently asked questions linkHere are answers to some of the most common questions.\nHow do I get models? linkMost gguf-based models should work, but newer models may require additions to the API. If a model doesn’t work, please feel free to open up issues. However, be cautious about downloading models from the internet and directly onto your machine, as there may be security vulnerabilities in lama.cpp or ggml that could be maliciously exploited. Some models can be found on Hugging Face: https://huggingface.co/models?search=gguf, or models from gpt4all are compatible too: https://github.com/nomic-ai/gpt4all.\nBenchmarking LocalAI and llama.cpp shows different results! linkLocalAI applies a set of defaults when loading models with the llama.cpp backend, one of these is mirostat sampling - while it achieves better results, it slows down the inference. You can disable this by setting mirostat: 0 in the model config file. See also the advanced section (/advanced/) for more information and this issue.\nWhat’s the difference with Serge, or XXX? linkLocalAI is a multi-model solution that doesn’t focus on a specific model type (e.g., llama.cpp or alpaca.cpp), and it handles all of these internally for faster inference, easy to set up locally and deploy to Kubernetes.\nEverything is slow, how is it possible? linkThere are few situation why this could occur. Some tips are:\nDon’t use HDD to store your models. Prefer SSD over HDD. In case you are stuck with HDD, disable mmap in the model config file so it loads everything in memory. Watch out CPU overbooking. Ideally the --threads should match the number of physical cores. For instance if your CPU has 4 cores, you would ideally allocate \u003c= 4 threads to a model. Run LocalAI with DEBUG=true. This gives more information, including stats on the token inference speed. Check that you are actually getting an output: run a simple curl request with \"stream\": true to see how fast the model is responding. Can I use it with a Discord bot, or XXX? linkYes! If the client uses OpenAI and supports setting a different base URL to send requests to, you can use the LocalAI endpoint. This allows to use this with every application that was supposed to work with OpenAI, but without changing the application!\nCan this leverage GPUs? linkThere is GPU support, see /features/gpu-acceleration/.\nWhere is the webUI? linkThere is the availability of localai-webui and chatbot-ui in the examples section and can be setup as per the instructions. However as LocalAI is an API you can already plug it into existing projects that provides are UI interfaces to OpenAI’s APIs. There are several already on Github, and should be compatible with LocalAI already (as it mimics the OpenAI API)\nDoes it work with AutoGPT? linkYes, see the examples!\nHow can I troubleshoot when something is wrong? linkEnable the debug mode by setting DEBUG=true in the environment variables. This will give you more information on what’s going on. You can also specify --debug in the command line.\nI’m getting ‘invalid pitch’ error when running with CUDA, what’s wrong? linkThis typically happens when your prompt exceeds the context size. Try to reduce the prompt size, or increase the context size.\nI’m getting a ‘SIGILL’ error, what’s wrong? linkYour CPU probably does not have support for certain instructions that are compiled by default in the pre-built binaries. If you are running in a container, try setting REBUILD=true and disable the CPU instructions that are not compatible with your CPU. For instance: CMAKE_ARGS=\"-DGGML_F16C=OFF -DGGML_AVX512=OFF -DGGML_AVX2=OFF -DGGML_FMA=OFF\" make build\n"
            }
        );
    index.add(
            {
                id:  30 ,
                href: "\/docs\/advanced\/installer\/",
                title: "Installer options",
                description: "An installation script is available for quick and hassle-free installations, streamlining the setup process for new users.\nCan be used with the following command:\ncurl https://localai.io/install.sh | sh Installation can be configured with Environment variables, for example:\ncurl https://localai.io/install.sh | VAR=value sh List of the Environment Variables:\nEnvironment Variable Description DOCKER_INSTALL Set to “true” to enable the installation of Docker images. USE_AIO Set to “true” to use the all-in-one LocalAI Docker image.",
                content: "An installation script is available for quick and hassle-free installations, streamlining the setup process for new users.\nCan be used with the following command:\ncurl https://localai.io/install.sh | sh Installation can be configured with Environment variables, for example:\ncurl https://localai.io/install.sh | VAR=value sh List of the Environment Variables:\nEnvironment Variable Description DOCKER_INSTALL Set to “true” to enable the installation of Docker images. USE_AIO Set to “true” to use the all-in-one LocalAI Docker image. API_KEY Specify an API key for accessing LocalAI, if required. CORE_IMAGES Set to “true” to download core LocalAI images. PORT Specifies the port on which LocalAI will run (default is 8080). THREADS Number of processor threads the application should use. Defaults to the number of logical cores minus one. VERSION Specifies the version of LocalAI to install. Defaults to the latest available version. MODELS_PATH Directory path where LocalAI models are stored (default is /usr/share/local-ai/models). P2P_TOKEN Token to use for the federation or for starting workers see documentation WORKER Set to “true” to make the instance a worker (p2p token is required see documentation) FEDERATED Set to “true” to share the instance with the federation (p2p token is required see documentation) FEDERATED_SERVER Set to “true” to run the instance as a federation server which forwards requests to the federation (p2p token is required see documentation) We are looking into improving the installer, and as this is a first iteration any feedback is welcome! Open up an issue if something doesn’t work for you!\n"
            }
        );
    index.add(
            {
                id:  31 ,
                href: "\/model-compatibility\/",
                title: "Model compatibility table",
                description: "Besides llama based models, LocalAI is compatible also with other architectures. The table below lists all the backends, compatible models families and the associated repository.\nnotifications LocalAI will attempt to automatically load models which are not explicitly configured for a specific backend. You can specify the backend to use by configuring a model with a YAML file. See the advanced section for more details.\nBackend and Bindings Compatible models Completion/Chat endpoint Capability Embeddings support Token stream support Acceleration llama.",
                content: "Besides llama based models, LocalAI is compatible also with other architectures. The table below lists all the backends, compatible models families and the associated repository.\nnotifications LocalAI will attempt to automatically load models which are not explicitly configured for a specific backend. You can specify the backend to use by configuring a model with a YAML file. See "
            }
        );
    index.add(
            {
                id:  32 ,
                href: "\/docs\/reference\/architecture\/",
                title: "Architecture",
                description: "LocalAI is an API written in Go that serves as an OpenAI shim, enabling software already developed with OpenAI SDKs to seamlessly integrate with LocalAI. It can be effortlessly implemented as a substitute, even on consumer-grade hardware. This capability is achieved by employing various C++ backends, including ggml, to perform inference on LLMs using both CPU and, if desired, GPU. Internally LocalAI backends are just gRPC server, indeed you can specify and build your own gRPC server and extend LocalAI in runtime as well.",
                content: "LocalAI is an API written in Go that serves as an OpenAI shim, enabling software already developed with OpenAI SDKs to seamlessly integrate with LocalAI. It can be effortlessly implemented as a substitute, even on consumer-grade hardware. This capability is achieved by employing various C++ backends, including ggml, to perform inference on LLMs using both CPU and, if desired, GPU. Internally LocalAI backends are just gRPC server, indeed you can specify and build your own gRPC server and extend LocalAI in runtime as well. It is possible to specify external gRPC server and/or binaries that LocalAI will manage internally.\nLocalAI uses a mixture of backends written in various languages (C++, Golang, Python, …). You can check the model compatibility table to learn about all the components of LocalAI.\nBackstory linkAs much as typical open source projects starts, I, mudler, was fiddling around with llama.cpp over my long nights and wanted to have a way to call it from go, as I am a Golang developer and use it extensively. So I’ve created LocalAI (or what was initially known as llama-cli) and added an API to it.\nBut guess what? The more I dived into this rabbit hole, the more I realized that I had stumbled upon something big. With all the fantastic C++ projects floating around the community, it dawned on me that I could piece them together to create a full-fledged OpenAI replacement. So, ta-da! LocalAI was born, and it quickly overshadowed its humble origins.\nNow, why did I choose to go with C++ bindings, you ask? Well, I wanted to keep LocalAI snappy and lightweight, allowing it to run like a champ on any system and avoid any Golang penalties of the GC, and, most importantly built on shoulders of giants like llama.cpp. Go is good at backends and API and is easy to maintain. And hey, don’t forget that I’m all about sharing the love. That’s why I made LocalAI MIT licensed, so everyone can hop on board and benefit from it.\nAs if that wasn’t exciting enough, as the project gained traction, mkellerman and Aisuko jumped in to lend a hand. mkellerman helped set up some killer examples, while Aisuko is becoming our community maestro. The community now is growing even more with new contributors and users, and I couldn’t be happier about it!\nOh, and let’s not forget the real MVP here—llama.cpp. Without this extraordinary piece of software, LocalAI wouldn’t even exist. So, a big shoutout to the community for making this magic happen!\n"
            }
        );
    index.add(
            {
                id:  33 ,
                href: "\/docs\/reference\/binaries\/",
                title: "LocalAI binaries",
                description: "LocalAI binaries are available for both Linux and MacOS platforms and can be executed directly from your command line. These binaries are continuously updated and hosted on our GitHub Releases page. This method also supports Windows users via the Windows Subsystem for Linux (WSL).\nUse the following one-liner command in your terminal to download and run LocalAI on Linux or MacOS:\ncurl -Lo local-ai \"https://github.com/mudler/LocalAI/releases/download/v2.27.0/local-ai-$(uname -s)-$(uname -m)\" \u0026\u0026 chmod +x local-ai \u0026\u0026 .",
                content: "LocalAI binaries are available for both Linux and MacOS platforms and can be executed directly from your command line. These binaries are continuously updated and hosted on our GitHub Releases page. This method also supports Windows users via the Windows Subsystem for Linux (WSL).\nUse the following one-liner command in your terminal to download and run LocalAI on Linux or MacOS:\ncurl -Lo local-ai \"https://github.com/mudler/LocalAI/releases/download/v2.27.0/local-ai-$(uname -s)-$(uname -m)\" \u0026\u0026 chmod +x local-ai \u0026\u0026 ./local-ai Otherwise, here are the links to the binaries:\nOS Link Linux (amd64) Download Linux (arm64) Download MacOS (arm64) Download ⚡\nBinaries do have limited support compared to container images:\nPython-based backends are not shipped with binaries (e.g. bark, diffusers or transformers)\nMacOS binaries and Linux-arm64 do not ship TTS nor stablediffusion-cpp backends\nLinux binaries do not ship stablediffusion-cpp backend\n"
            }
        );
    index.add(
            {
                id:  34 ,
                href: "\/docs\/reference\/nvidia-l4t\/",
                title: "Running on Nvidia ARM64",
                description: "LocalAI can be run on Nvidia ARM64 devices, such as the Jetson Nano, Jetson Xavier NX, and Jetson AGX Xavier. The following instructions will guide you through building the LocalAI container for Nvidia ARM64 devices.\nPrerequisites link Docker engine installed (https://docs.docker.com/engine/install/ubuntu/) Nvidia container toolkit installed (https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installing-with-ap) Build the container linkBuild the LocalAI container for Nvidia ARM64 devices using the following command:\ngit clone https://github.com/mudler/LocalAI cd LocalAI docker build --build-arg SKIP_DRIVERS=true --build-arg BUILD_TYPE=cublas --build-arg BASE_IMAGE=nvcr.",
                content: "LocalAI can be run on Nvidia ARM64 devices, such as the Jetson Nano, Jetson Xavier NX, and Jetson AGX Xavier. The following instructions will guide you through building the LocalAI container for Nvidia ARM64 devices.\nPrerequisites link Docker engine installed (https://docs.docker.com/engine/install/ubuntu/) Nvidia container toolkit installed (https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installing-with-ap) Build the container linkBuild the LocalAI container for Nvidia ARM64 devices using the following command:\ngit clone https://github.com/mudler/LocalAI cd LocalAI docker build --build-arg SKIP_DRIVERS=true --build-arg BUILD_TYPE=cublas --build-arg BASE_IMAGE=nvcr.io/nvidia/l4t-jetpack:r36.4.0 --build-arg IMAGE_TYPE=core -t quay.io/go-skynet/local-ai:master-nvidia-l4t-arm64-core . Otherwise images are available on quay.io and dockerhub:\ndocker pull quay.io/go-skynet/local-ai:master-nvidia-l4t-arm64-core Usage linkRun the LocalAI container on Nvidia ARM64 devices using the following command, where /data/models is the directory containing the models:\ndocker run -e DEBUG=true -p 8080:8080 -v /data/models:/build/models -ti --restart=always --name local-ai --runtime nvidia --gpus all quay.io/go-skynet/local-ai:master-nvidia-l4t-arm64-core Note: /data/models is the directory containing the models. You can replace it with the directory containing your models.\n"
            }
        );
    index.add(
            {
                id:  35 ,
                href: "\/docs\/",
                title: "Docs",
                description: "",
                content: ""
            }
        );
    search.addEventListener('input', show_results, true);

    function show_results(){
        const maxResult =  5 ;
        const minlength =  0 ;
        var searchQuery = sanitizeHTML(this.value);
        var results = index.search(searchQuery, {limit: maxResult, enrich: true});

        
        const flatResults = new Map(); 
        for (const result of results.flatMap(r => r.result)) {
        if (flatResults.has(result.doc.href)) continue;
        flatResults.set(result.doc.href, result.doc);
        }

        suggestions.innerHTML = "";
        suggestions.classList.remove('d-none');

        
        if (searchQuery.length < minlength) {
            const minCharMessage = document.createElement('div')
            minCharMessage.innerHTML = `Please type at least <strong>${minlength}</strong> characters`
            minCharMessage.classList.add("suggestion__no-results");
            suggestions.appendChild(minCharMessage);
            return;
        } else {
            
            if (flatResults.size === 0 && searchQuery) {
                const noResultsMessage = document.createElement('div')
                noResultsMessage.innerHTML = "No results for" + ` "<strong>${searchQuery}</strong>"`
                noResultsMessage.classList.add("suggestion__no-results");
                suggestions.appendChild(noResultsMessage);
                return;
            }
        }

        
        for(const [href, doc] of flatResults) {
            const entry = document.createElement('div');
            suggestions.appendChild(entry);

            const a = document.createElement('a');
            a.href = href;
            entry.appendChild(a);

            const title = document.createElement('span');
            title.textContent = doc.title;
            title.classList.add("suggestion__title");
            a.appendChild(title);

            const description = document.createElement('span');
            description.textContent = doc.description;
            description.classList.add("suggestion__description");
            a.appendChild(description);

            suggestions.appendChild(entry);

            if(suggestions.childElementCount == maxResult) break;
        }
    }
    }());
</script></body></html>